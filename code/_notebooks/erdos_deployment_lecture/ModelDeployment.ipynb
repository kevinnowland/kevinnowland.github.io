{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: code-post\n",
    "title: Basic model deployment\n",
    "tags: [misc]\n",
    "---\n",
    "\n",
    "While a data scientist might rely heavily on Jupyter notebooks to run local experiments\n",
    "and choose a model, the story does not end here. Ultimately the goal is to get a model\n",
    "into _production_, i.e., make it available for use by the data scientist's clients, whether\n",
    "they be internal or external clients. Some of the components that go into this\n",
    "include\n",
    "\n",
    "1. Exposing the model via an API\n",
    "2. Standardizing the build and production environments\n",
    "3. Monitoring model performance\n",
    "4. Logging diagnostic events\n",
    "\n",
    "We will focus on the first two points above.\n",
    "\n",
    "## Exposing a model via API\n",
    "\n",
    "API stands for _Application Programming Interface_ and refers to how your\n",
    "code is exposed to others. For example, people will talk about the fact\n",
    "that `matplotlib` as two APIs: the one based on repeatedly plotting and \n",
    "the one based on using `axes` objects. People also talking about accessing\n",
    "Twitter's API to get the content of tweets.\n",
    "\n",
    "For our case, we will mean the ability to pass input data to the model via\n",
    "a web interface.\n",
    "\n",
    "### Building a model\n",
    "\n",
    "Before jumping into exposing a model, let's build one first. Since the\n",
    "modelling isn't the point here, we'll just build a simple random forest\n",
    "predictor usingthe classicial \n",
    "[Iris](https://archive.ics.uci.edu/ml/datasets/iris) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "25                 5.0               3.0                1.6               0.2   \n",
       "30                 4.8               3.1                1.6               0.2   \n",
       "74                 6.4               2.9                4.3               1.3   \n",
       "29                 4.7               3.2                1.6               0.2   \n",
       "135                7.7               3.0                6.1               2.3   \n",
       "\n",
       "     target  \n",
       "25        0  \n",
       "30        0  \n",
       "74        1  \n",
       "29        0  \n",
       "135       2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# load dataset\n",
    "iris = datasets.load_iris()\n",
    "df_iris = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\n",
    "df_iris['target'] = iris['target']\n",
    "\n",
    "# create train test split\n",
    "df_train, df_test = train_test_split(df_iris,\n",
    "                                     test_size=0.3,\n",
    "                                     random_state=47,\n",
    "                                     stratify=df_iris['target'])\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of fit model: 0.933\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# put together pipeline\n",
    "model = Pipeline([\n",
    "    ('scale', StandardScaler())\n",
    "    ,('gbm', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "# make model\n",
    "model.fit(df_train[iris['feature_names']], df_train['target'])\n",
    "\n",
    "# see how accurate it is on test data\n",
    "test_predictions = model.predict(df_test[iris['feature_names']])\n",
    "test_accuracy = accuracy_score(df_test['target'], test_predictions)\n",
    "print('Test accuracy of fit model: {:.3f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic way to deliver this model would be to actually deliver this\n",
    "notebook to a user, but that requires that the user be able to run a Jupyter\n",
    "notebook and know how to call the model. This might be a valid way to deliver\n",
    "a model to an internal user who is technically proficient such as a data\n",
    "analyst or another data scientist.\n",
    "But instead of delivering a notebook, we can binarize the model object itself\n",
    "using the `pickle` module which is built into base python. So let's save\n",
    "the model out to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can then be reloaded from disk and called as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model prediction: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('model.pickle', 'rb') as f:\n",
    "    model_ = pickle.load(f)\n",
    "\n",
    "print('model prediction:', model_.predict(np.random.normal(size=(1, 4)))[0])\n",
    "\n",
    "del model_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flask\n",
    "\n",
    "Now let's move on to exposing the model via an API. We will be using\n",
    "[Flask](https://flask.palletsprojects.com/en/1.1.x/), which\n",
    "is one of several web micro-fameworks for python. A _web framework_ is\n",
    "a set of tools for a language that helps programmers deal with common tasks\n",
    "necessary to set up web applications. This can include standardizing names\n",
    "and interactions with databases, serving static content such as images from \n",
    "specific file directory locations, and allowing language specific code to be\n",
    "used in setting up the HTML files that are ultimately displayed in a public\n",
    "facing website. Examples of framworks include \n",
    "[django](https://www.djangoproject.com/) and\n",
    "[cherryPy](https://cherrypy.org/) for python and \n",
    "[rails](https://rubyonrails.org/) for ruby. Flask is fairly lightweight and \n",
    "easy to get started with, so we will use this.\n",
    "\n",
    "A simple application might look like the following file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting application.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile application.py\n",
    "\n",
    "# imports related to the model we have built\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# imports related to flask and loading the model\n",
    "from flask import Flask, request, jsonify\n",
    "import pickle\n",
    "\n",
    "json_header = {'content-type': 'application/json; charset=UTF-8'}\n",
    "model = pickle.load(open('model.pickle', 'rb'))\n",
    "app = Flask(__name__)    \n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def get_prediction():\n",
    "    \n",
    "    args = request.args\n",
    "\n",
    "    # check that all args are present\n",
    "    desired_args = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "    missing_args = [a for a in desired_args if args.get(a) is None]\n",
    "    \n",
    "    if len(missing_args) > 0:\n",
    "        error_msg = 'argument(s) missing: {}'.format(missing_args)\n",
    "        return (jsonify(error_msg), 422, json_header)\n",
    "                \n",
    "    # check that all args are floats\n",
    "    def arg_is_float(arg):\n",
    "        is_float = False\n",
    "        \n",
    "        try:\n",
    "            x = float(arg)\n",
    "            is_float = True\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        return is_float\n",
    "    \n",
    "    nonfloat_args = [a for a in desired_args if not arg_is_float(args.get(a))]\n",
    "    \n",
    "    if len(nonfloat_args) > 0:\n",
    "        error_msg = 'argument(s) not float: {}'.format(nonfloat_args)\n",
    "        return (jsonify(error_msg), 422, json_header)\n",
    "    \n",
    "    # make predictions\n",
    "    X = [[float(args.get(a)) for a in desired_args]]\n",
    "    prediction = str(model.predict(X)[0])\n",
    "    \n",
    "    return (jsonify({'prediction': prediction}), 200, json_header)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "25                 5.0               3.0                1.6               0.2   \n",
       "30                 4.8               3.1                1.6               0.2   \n",
       "74                 6.4               2.9                4.3               1.3   \n",
       "29                 4.7               3.2                1.6               0.2   \n",
       "135                7.7               3.0                6.1               2.3   \n",
       "\n",
       "     target  \n",
       "25        0  \n",
       "30        0  \n",
       "74        1  \n",
       "29        0  \n",
       "135       2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now written out to the file `application.py`. Without getting very technical, \n",
    "we'll point out some of the important features.\n",
    "\n",
    "At the top of the file we have\n",
    "```python\n",
    "json_header = {'content-type': 'application/json; charset=UTF-8'}\n",
    "model = pickle.load('model.pickle')\n",
    "app = Flask(__name__)    \n",
    "```\n",
    "Here we have a JSON header that is a `dict`, we load the model from a \n",
    "pickle file, and we create a `Flask` object using the `__name__` variable.\n",
    "The JSON header is used to tell the user calling our application what\n",
    "sort of data to expect to receive in response to the request they make.\n",
    "\n",
    "Note that at the bottom of the file we have\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0')\n",
    "```\n",
    "This causes the application to run with host `0.0.0.0`, aka the `localhost` aka\n",
    "our local machine will run the application.\n",
    "\n",
    "Between the initial creation of the application and\n",
    "running it if appropriate we have the following lines.\n",
    "```python\n",
    "@app.route('/', methods=['GET'])\n",
    "def get_prediction():\n",
    "    \n",
    "    args = request.args\n",
    "```\n",
    "Here we have a function named `get_prediction()` that is decorated with \n",
    "`@app.route('/', methods=['GET'])`. This tells Flask that to look for an HTTP `GET` request\n",
    "at the location `/`. If the application receives a request at this location, it\n",
    "then tries to run this function.  The first thing that happens in this function\n",
    "is to store `request.args` as `args`. Note that `request` is not mentioned anywhere\n",
    "in the module as either a local or global variable. This means that `request` must\n",
    "be a parameter that is inherited from the `@app.route` decoration. Indeed it is, and it\n",
    "contains all the information the application receives from the user at this route.\n",
    "The function then processes `args`, which is a `dict`. In a minute we will see some of\n",
    "what gets put into this variable.\n",
    "\n",
    "Following this is code that attempts to validates the contents of `args`. We\n",
    "first check that all of our desired arguments are included among the keys of the\n",
    "incoming arguments.\n",
    "```python\n",
    "    desired_args = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "    missing_args = [a for a in desired_args if args.get(a) is None]\n",
    "    \n",
    "    if len(missing_args) > 0:\n",
    "        error_msg = 'argument(s) missing: {}'.format(missing_args)\n",
    "        return (jsonify.dumps(error_msg), 422, JSON_HEADER)\n",
    "```\n",
    "Here we see that if some of the desired arguments are missing we exit\n",
    "the function early. We dump an error message as a JSON object\n",
    "along with the [_HTTP response status code_](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) \n",
    "422, which indicates that the application was not able to process the request properly.\n",
    "We also pass in our json header object. Similarly, we then check that the values of the\n",
    "arguments can be cast as floats.\n",
    "\n",
    "The function ends by making a prediction writing out the result with response status\n",
    "code 200, the everything was fine on our end signal.\n",
    "```python\n",
    "    X = [[float(args.get(a)) for a in desired_args]]\n",
    "    prediction = model.predict(X)[0]\n",
    "    \n",
    "    return (jsonify({'prediction': prediction}), 200, JSON_HEADER)\n",
    "```\n",
    "\n",
    "Then, in your terminal, you can start the application locally by running\n",
    "```bash\n",
    "> python application.py\n",
    " * Serving Flask app \"application\" (lazy loading)\n",
    " * Environment: production\n",
    "   WARNING: This is a development server. Do not use it in a production deployment.\n",
    "   Use a production WSGI server instead.\n",
    " * Debug mode: off\n",
    " * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n",
    "```\n",
    "The last line of the output tells you that this is running at port 5000.\n",
    "In a separate terminal you can test the API by running\n",
    "```bash\n",
    "> curl 'localhost:5000/?sepal_length=7.7&sepal_width=3.0&petal_length=6.1&petal_width=2.3'\n",
    "{\"prediction\":\"2\"}\n",
    "```\n",
    "(You can also replace `localhost` with `0.0.0.0`.)\n",
    "\n",
    "And voilà! Your model is now available via an API... even if it's only locally.\n",
    "This delivery mechanism could be achieved by sending the files `application.py`\n",
    "and `model.pickle` to whoever wants to run this application, assuming they have\n",
    "the ability to receive incoming requests and the proper versions of python\n",
    "and all its packages installed... We'll get to the environment problem soon, but\n",
    "first, let's speed things up.\n",
    "\n",
    "### Gunicorn\n",
    "\n",
    "When we ran our Flask application, we helpfully received the message\n",
    "```bash\n",
    "WARNING: This is a development server. Do not use it in a production deployment.\n",
    "```\n",
    "The reason for this warning is that Flask is slow. The web server that Flask\n",
    "creates works by sitting there waiting\n",
    "for requests which is tries to execute one at a time. This can cause requests to back\n",
    "up and might cause errors. It's fine for development, but is not sufficient for \n",
    "production.\n",
    "\n",
    "A quick way to speed things up is to pair Flask with the python package\n",
    "[`gunicorn`](https://gunicorn.org/) aka Green Unicorn. (Unicorn is a web server \n",
    "for ruby applications, pythons are green I suppose so... green unicorn.) Gunicorn will\n",
    "sit in front of the Flask web server and create multiple workers. When a request\n",
    "comes it, it will be routed to one fo the workers that are hopefully idle.\n",
    "\n",
    "In order to use gunicorn, we will add the following file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wsgi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wsgi.py\n",
    "\n",
    "from application import app\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(use_reloader=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The acronym WSGI stands for [_Web Server Gateway Interface_](https://wsgi.readthedocs.io/en/latest/)\n",
    "which is a Python standard specification for how web servers and web applications communicate.\n",
    "Gunicorn is a WSGI server and will use this file to load `app` from the `application` module\n",
    "that we previously wrote. With just this bit of code, we can now start a webserver with\n",
    "gunicorn with the command\n",
    "```bash\n",
    "> gunicorn -w 3 -b :5001 -t 360 --reload wsgi:app\n",
    "[2020-06-28 15:40:12 -0400] [86149] [INFO] Starting gunicorn 20.0.4\n",
    "[2020-06-28 15:40:12 -0400] [86149] [INFO] Listening at: http://0.0.0.0:5001 (86149)\n",
    "[2020-06-28 15:40:12 -0400] [86149] [INFO] Using worker: sync\n",
    "[2020-06-28 15:40:12 -0400] [86152] [INFO] Booting worker with pid: 86152\n",
    "[2020-06-28 15:40:12 -0400] [86153] [INFO] Booting worker with pid: 86153\n",
    "[2020-06-28 15:40:12 -0400] [86154] [INFO] Booting worker with pid: 86154\n",
    "```\n",
    "You can tell that we have started three workers and we are listening at\n",
    "port 5001 of `localhost`. We can again cURL to get a response:\n",
    "```bash\n",
    "> curl 'localhost:5001/?sepal_length=7.7&sepal_width=3.0&petal_length=6.1&petal_width=2.3'\n",
    "{\"prediction\":\"2\"}\n",
    "```\n",
    "Note that we did not have to curl a specific worker, we just had to \n",
    "curl to the port and gunicorn took care of delgating the task for us.\n",
    "\n",
    "### Nginx?\n",
    "\n",
    "The gunicorn documentation will also tell you not to deploy an application just\n",
    "with gunicorn and will point you to nginx. We won't use nginx webserver here\n",
    "(apache is another popular one), but we'll remark that nginx sits in front of\n",
    "gunicorn in a similar way to how gunicorn sits in front of flask. Nginx\n",
    "should be used to actually accept traffic from the web and will direct\n",
    "requests to appropriate places and serve files directly to users if needed. Nginx\n",
    "is an all purpose web server, while gunicorn just serves python applications.\n",
    "\n",
    "\n",
    "\n",
    "## Standardizing environments\n",
    "\n",
    "\n",
    "\n",
    "A problem which has long plagued software engineers and now also data scientists\n",
    "is the fact that environments are not standardized. One data scientist will not\n",
    "necessarily have the same exact version of `pandas` installed. Or even the same\n",
    "verison of `python`. Or even the same operating system. How do we solve for this?\n",
    "\n",
    "### Local environments\n",
    "\n",
    "One way to make sure that everyone is using the same packages and python\n",
    "versions is to use the `venv` module. Going this route will create directories\n",
    "in the directory where venv is called and install python packages will live there.\n",
    "At least among data scientists, this has fallen out of fashion. More information\n",
    "can be found [here](https://docs.python.org/3/library/venv.html).\n",
    "\n",
    "Since most python based data scientists use Anaconda, it is convenient to rely\n",
    "on the environment abilities of the `conda` tool, which is able to create\n",
    "environments from environment files that can be checked into `git` repositories.\n",
    "For example, here is the contents of the `environment.yml` file that I am using:\n",
    "```yaml\n",
    "name: erdos\n",
    "dependencies:\n",
    "    - python=3.8\n",
    "    - pandas=1.0.*\n",
    "    - numpy=1.18.*\n",
    "    - ipykernel=5.1.*\n",
    "    - jupyterlab=1.2.*\n",
    "    - scikit-learn=0.22*\n",
    "    - nb_conda_kernels=2.2.*\n",
    "    - flask\n",
    "    - gunicorn\n",
    "    - pip\n",
    "```\n",
    "This is a YAML file, where YAML which stands for YAML Ain't Markup Language. YAML\n",
    "files are just text files with either the `.yml` or `.yaml` endings which are\n",
    "increasingly common in software engineering in various \"__ as code\" situations.\n",
    "You can create a new conda environment and make it available to jupyter\n",
    "by running\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "python -m ipykernel install --user --name erdos --display-name \"Python (erdos)\"\n",
    "```\n",
    "Updates are handled via `conda env update -f environment.yml`.\n",
    "\n",
    "\n",
    "### Production environments\n",
    "\n",
    "\n",
    "While conda is fine for maintaing local environments among teammates working\n",
    "on a project, production servers will not have anaconda installed and may not\n",
    "even have the same operating system as personal development laptops. I.e.,\n",
    "you might have a mac or an ubuntu machine, but the production servers might\n",
    "run CentOS.\n",
    "\n",
    "A decade ago the answer to the operating system issue would be virtual machines. \n",
    "A virtual machine allows you to run a full copy of one operating system on another.\n",
    "The production cluster where applications are deployedmight contain servers running \n",
    "CentOS or Red Had Linux or whatever operating system,\n",
    "but these would just be hosts that run virtual machines that can then be any\n",
    "other operating system. If your application required a flavor of linux you \n",
    "would deploy to the appropriate virtual machine.\n",
    "\n",
    "This still does not solve the package and versioning issues. The technology \n",
    "that developed to solve both issues at once is called _Docker_. Docker\n",
    "bundles together basic operating system information application information into\n",
    "an _image_ which is then deployed as a _container_. Without getting into \n",
    "technical details, it suffices to know that docker images are much more lightweight\n",
    "than full virtual machines even though they naively seem very similar. Docker\n",
    "images run what are called _microservices_; where a virtual machine can run a host\n",
    "of applications at the same time, a docker container should only handle small\n",
    "tasks and will not have many applicaitons running at once.\n",
    "\n",
    "With that introduction out of the way, let's get to an example of how to build\n",
    "a docker container and package up our gunicorn + flask based API.\n",
    "\n",
    "This is a sample _Dockerfile_, which is the basic unit that defines a docker image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "FROM python:3.8\n",
    "\n",
    "# system updates\n",
    "RUN apt-get update\n",
    "\n",
    "# create and switch to non-root user\n",
    "RUN groupadd appgroup\n",
    "RUN useradd -g appgroup -m appuser\n",
    "USER appuser\n",
    "\n",
    "# copy files into image\n",
    "COPY requirements.txt .\n",
    "COPY data.cvs # <- would have to connect to database\n",
    "COPY build_model.py\n",
    "COPY application.py .\n",
    "COPY wsgi.py .\n",
    "\n",
    "# install requirements\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --no-cache-dir -r requirements.txt --user\n",
    "\n",
    "# build the model\n",
    "RUN python build_model.py\n",
    "\n",
    "# start it up\n",
    "CMD gunicorn -w 3 -b :5000 -t 360 --reload wsgi:app\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post was created for a lecture for the [Erdos institute](erdosinstitute.org)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (erdos)",
   "language": "python",
   "name": "erdos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
