{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: code-post\n",
    "title: Recurrent neural networks\n",
    "tags: [neural nets]\n",
    "---\n",
    "\n",
    "By now a classicial and outdated technique, I am going to try to implement a\n",
    "recurrent neural network and see how well it fits some dummy data.\n",
    "A good resource for an overview of RNNs is [this cheat sheet](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)\n",
    "from a Stanford CS course.\n",
    "\n",
    "Outline:\n",
    "1. Architecture\n",
    "2. Backpropagation through time (BTT)\n",
    "3. Dummy training examples\n",
    "4. LSTMs and other cell types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architecture\n",
    "\n",
    "RNNs are based on _cells_ and act on sequences of inputs. A single (simple) cell will take\n",
    "two inputs: one element of the input sequence and either the output or the\n",
    "_hidden state_ of the previous cell in the sequence. One can also take the two\n",
    "inputs to be the output of the previous cell along with the hidden state of the \n",
    "previous cell if there is no longer an input sequence to rely upon. The many\n",
    "variations can be found in the cheat sheet referenced above. A cell can also\n",
    "take more than two inputs, as in the context of Long Short-Term Memory cells\n",
    "there is an additional _context_ that exists in addition to the hidden state\n",
    "that is passed between cells.\n",
    "\n",
    "Let $x\\_t$ be the $t$th element of the input sequence and $h\\_{t-1}$ the\n",
    "hidden state from the $(t-1)$st cell. Then the new hidden state is given\n",
    "by\n",
    "$$\n",
    "    h\\_t = \\sigma(W\\_{xh}x\\_t + W\\_{hh}h\\_{t-1} + b\\_h)\n",
    "$$\n",
    "where $\\sigma$ is the activation function (we will take this to be ReLU).\n",
    "If inpute sequence element $x\\_t$ has dimension $n\\_x$ and the hidden\n",
    "state has size $n\\_h$, then $W\\_{xh}$ has size $n\\_h\\times n\\_x$\n",
    "while $W\\_{hh}$ is a square matrix of size $n\\_h\\times n\\_h$. This is equivalent\n",
    "to\n",
    "$$\n",
    "    h\\_t = \\sigma([W\\_{xh} | W\\_{hh}](x\\_t\\oplus h\\_{t-1} + b\\_h)\n",
    "$$\n",
    "where$\\oplus$ indicates vector concatenation and $[W\\_{xh} | W\\_{hh}]$ is a matrix\n",
    "of size $n\\_h \\times(n\\_x + n\\_h)$. Many of the descriptions of RNNs, and in particular\n",
    "when cells become more complicated as in _Long Short-Term Memory_ cells, formulate\n",
    "things in terms of concatenation, so I just wanted to quickly show that these were\n",
    "equivalent.\n",
    "\n",
    "Moving forward, I will say that the hidden state is given by\n",
    "$$\n",
    "    h\\_t = \\sigma(W\\_h(x\\_t\\oplus h\\_{t-1}) + b\\_h)\n",
    "$$\n",
    "where $W\\_h$ has size $n\\_h\\times(n\\_x + n\\_h)$. Without working it out, my\n",
    "intuition -- which could be wrong! -- suggests that this means we can keep\n",
    "formulating the initialization in terms of the input and output vector sizes.\n",
    "\n",
    "The output is then given by\n",
    "$$\n",
    "    y\\_t = \\sigma(W\\_yh\\_t + b\\_y).\n",
    "$$\n",
    "\n",
    "All of these words are why you will usually just see diagrams when discussing\n",
    "varieties of LSTMs.\n",
    "\n",
    "The simplified version of an RNN would actually just take the hidden state and\n",
    "the output to be the same, with no extra layer between the hidden\n",
    "state and the output.\n",
    "\n",
    "In the following code we implement a `Cell` class that initializes based on\n",
    "input size, hidden state size, and output size. It has methods to take an\n",
    "input and update the hidden state and update both the hidden state and its\n",
    "output, which it stores. The updates can also return the inputs to the hidden\n",
    "state neuron layer and the output neuron layers that are fed into the\n",
    "ReLU and sigmoid activations, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x * (x>0) + 0 * (x<=0)\n",
    "\n",
    "def relu_prime(x):\n",
    "    return 1 * (x>0) + 0 * (x<=0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "class Cell():\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, random_seed=None):                 \n",
    "        \"\"\" give the size of the input vectors, hidden state\n",
    "        vectors, and output sizes. \n",
    "        \n",
    "        If using the cells in such a way that the output is\n",
    "        used in place of the hidden state, then hidden size and\n",
    "        output size must be the same.\n",
    "        \n",
    "        If using in such a way that the output is used in place\n",
    "        of the input at some point, then the output and input\n",
    "        sizes must be the same.\n",
    "        \"\"\"\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "        self._input_size = input_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._output_size = output_size\n",
    "        \n",
    "        self._hidden_state = np.zeros((self._hidden_size, 1))\n",
    "        self._y = np.zeros((self._output_size, 1))\n",
    "        \n",
    "        def initialize(nrows, ncols, init_type=None):\n",
    "            # Kaiming He initialiation for weights\n",
    "            # or just some arbitrary initialization for biases\n",
    "            if ncols > 1:\n",
    "                if init_type=='He':\n",
    "                    std = np.sqrt(2.0 / ncols)\n",
    "                elif init_type=='Xavier':\n",
    "                    std = np.sqrt(2.0 / (ncols + nrows))\n",
    "                else:\n",
    "                    msg = 'init_type must be He or Xavier if initializing weights'\n",
    "                    raise Exception(msg)\n",
    "            else:\n",
    "                # this is for biases\n",
    "                std = np.sqrt(2.0 / nrows)\n",
    "            \n",
    "            try_again = True\n",
    "            while try_again:\n",
    "                w = np.random.normal(0, std, size=(nrows, ncols))\n",
    "                try_again = abs(np.std(w)-std) > .05 or abs(np.mean(w)) > .05\n",
    "                \n",
    "            return w\n",
    "        \n",
    "        self._wh = initialize(self._hidden_size, self._input_size + self._hidden_size, 'He')\n",
    "        self._bh = initialize(self._hidden_size, 1)\n",
    "        self._wy = initialize(self._output_size, self._hidden_size, 'Xavier')\n",
    "        self._by = initialize(self._output_size, 1)\n",
    "        \n",
    "    @property\n",
    "    def input_size(self):\n",
    "        return self._input_size\n",
    "    \n",
    "    @property\n",
    "    def hidden_size(self):\n",
    "        return self._hidden_size\n",
    "    \n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._output_size\n",
    "    \n",
    "    @property\n",
    "    def hidden_state(self):\n",
    "        return self._hidden_state\n",
    "    \n",
    "    def _shape_assert(self, oldval, newval):\n",
    "        assert oldval.shape == newval.shape, \\\n",
    "            \"new value must have shape {}\".format(oldval.shape)\n",
    "    \n",
    "    @hidden_state.setter\n",
    "    def hidden_state(self, val):\n",
    "        self._shape_assert(self.hidden_state, val)\n",
    "        self._hidden_state = val\n",
    "        \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self._y\n",
    "    \n",
    "    @y.setter\n",
    "    def y(self, val):\n",
    "        self._shape_assert(self.y, val)\n",
    "        self._y = val\n",
    "        \n",
    "    @property\n",
    "    def wh(self):\n",
    "        return self._wh\n",
    "    \n",
    "    @wh.setter\n",
    "    def wh(self, val):\n",
    "        self._shape_assert(self.wh, val)\n",
    "        self._wh = val\n",
    "        \n",
    "    @property\n",
    "    def bh(self):\n",
    "        return self._bh\n",
    "    \n",
    "    @bh.setter\n",
    "    def bh(self, val):\n",
    "        self._shape_assert(self.bh, val)\n",
    "        self._bh = val\n",
    "        \n",
    "    @property\n",
    "    def wy(self):\n",
    "        return self._wy\n",
    "    \n",
    "    @wy.setter\n",
    "    def wy(self, val):\n",
    "        self._shape_assert(self.wy, val)\n",
    "        self._wy = val\n",
    "        \n",
    "    @property\n",
    "    def by(self):\n",
    "        return self._by\n",
    "    \n",
    "    @by.setter\n",
    "    def by(self, val):\n",
    "        self._shape_assert(self.by, val)\n",
    "        self._by = val\n",
    "        \n",
    "    def update_hidden_state(self, x, return_activation_input=False):\n",
    "        \"\"\" take a numpy vector of size (input_size, 1) and\n",
    "        calculate the hidden state of size (hidden_size, 1).\n",
    "        \n",
    "        This returns nothing, updating in place. \"\"\"\n",
    "        \n",
    "        assert type(x) == np.ndarray, \"x must by numpy.ndarray\"\n",
    "        assert x.shape[0] == self.input_size, \\\n",
    "            \"x must have first dim of length {}\".format(self.input_size)\n",
    "        assert x.shape[1] == 1, \"x must have second dim of length 1\"\n",
    "        \n",
    "        # concatenate\n",
    "        xh = np.concatenate([x, self.hidden_state])\n",
    "        \n",
    "        # get output\n",
    "        activation_input = np.matmul(self.wh, xh) + self.bh\n",
    "        self.hidden_state = relu(activation_input)\n",
    "        \n",
    "        if return_activation_input:\n",
    "            return activation_input\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def update_y(self, return_activation_input=False):\n",
    "        \"\"\" takes the hidden state and runs through a neuron\n",
    "        layer with sigmoid activation function to get the output y \"\"\"\n",
    "        activation_input = np.matmul(self.wy, self.hidden_state) + self.by\n",
    "        self.y = sigmoid(activation_input)\n",
    "        \n",
    "        if return_activation_input:\n",
    "            return activation_input\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def update(self, x, return_activation_inputs=False):\n",
    "        hidden_layer_input = self.update_hidden_state(x, return_activation_inputs)\n",
    "        output_layer_input = self.update_y(return_activation_inputs)\n",
    "        \n",
    "        if return_activation_inputs:\n",
    "            return hidden_layer_input, output_layer_input\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking about unfolding the RNN in time, we have a single layer of cells.\n",
    "It also possible to have multiple cells at a single timestep. At timestep\n",
    "$t$ the first cell takes the intput $x\\_t$ and the hidden state $h\\_{t-1, 1}$\n",
    "from the previous timestep of the same cells. The second cell would take\n",
    "the output $y\\_{t, 1}$ and its previous hidden state $h\\_{t-1, 2}$ as its\n",
    "input and put out the output $y\\_t$. Unrolling this structure in time would\n",
    "lead to multiple layers of cells. It seems that only one or two layers of cells\n",
    "are typical, as training is very expensive for RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gitpage] *",
   "language": "python",
   "name": "conda-env-gitpage-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
