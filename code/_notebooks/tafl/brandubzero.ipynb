{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: code-post\n",
    "title: AlphaZero for Brandub\n",
    "description: We provide a from scratch implementation of AlphaZero to learn how to play brandub.\n",
    "tags: [tafl, neural nets]\n",
    "---\n",
    "\n",
    "In this post I'm going to explore how to implement [AlphaZero](https://arxiv.org/pdf/1712.01815.pdf)\n",
    "to learn how to play Brandub (a tafl variant). I previously put together a [Brandub package](https://github.com/kevinnowland/brandub)\n",
    "which I won't use directly, but whose code I will modify as needed. You can find the rules of the\n",
    "game there.\n",
    "\n",
    "The main purpose of this is to prove that I understand the algorithm well enough to implement\n",
    "a very crude version of it. How crude? I doubt the best trained player after even a few days of training\n",
    "will be able to play very well. My big question is whether the trained model can beat me, who has\n",
    "never played a full game before despite all this coding that I've done.\n",
    "\n",
    "\n",
    "## Algorithm description\n",
    "\n",
    "### Game play technology\n",
    "\n",
    "AlphaZero plays games using two main pieces of technology. The first is a neural network $f\\_\\theta(s)$ with\n",
    "parameters $\\theta$ which takes the current game state $s$ and outputs $(p, z)$ where where $p$ is a probability\n",
    "vector of all possible moves and $z \\in [-1, 1]$ is an estimated game score for the current player with -1 being a\n",
    "loss, 0 a draw, and 1 a win. Only the neural network parameters $\\theta$ will be learned.\n",
    "\n",
    "One could play \n",
    "just using $f\\_\\theta$ by choosing moves according to the output move probabilities $p$, \n",
    "but instead of doing this, we rely on\n",
    "the second piece of technology, a Monte Carlo Tree Search (MCTS) algorithm. The tree search\n",
    "uses the neural network to explore the space of moves and choose the best one taking into account\n",
    "how the game might proceed. The neural network output, as we will\n",
    "see later, is extremely raw and can suggest moves which are illegal (moving an opponent's piece) or \n",
    "even impossible (moving off the board). In addition to suggestiongm oves which will likely lead to victory\n",
    "for the curernt player, the tree search algorithm helps encode the rules of the game\n",
    "by by only exploring using legal moves. The tree\n",
    "search is not learned directly, but does take as inputs the neural network $f\\_\\theta$ and current game state. \n",
    "The output is a policy $\\pi$, a probability vector that is used to select the next move.\n",
    "\n",
    "### Learning\n",
    "\n",
    "Give the neural network and MCTS algorithm, play proceeds until a game ends at step $T$.\n",
    "At each step $t$ of the game we have neural network output $(p\\_t, v\\_t)$ as well as \n",
    "the policy $\\pi\\_t$ governing move selection. The final game result is $s\\_T \\in \\{-1, 0, 1\\}$. \n",
    "The loss $\\ell\\_t$ for a step is\n",
    "$$\n",
    "  \\ell\\_t = (v\\_t - s\\_T)^2 - \\pi\\_t^t\\log p\\_t + c \\|\\theta\\|^2,\n",
    "$$\n",
    "where $c$ is an $\\ell^2$-regularization parameter. Thus we are looking at mean squared\n",
    "error for the \n",
    "predicted output $v$ with cross-entropy forcing predicted moved probabilities $p$ to look like \n",
    "the MCTS policy $\\pi$ with a regularization term.\n",
    "The overall loss function is the average of this over the $T$ moves making up the game.\n",
    "\n",
    "After each game, we backpropagate from the loss function through the nerual network to\n",
    "change the neural network parameters $\\theta$. Note that while policies $\\pi\\_t$ do depend on \n",
    "$\\theta$ since the MCTS takes the nerual network as input, we will pretend it does not and thus \n",
    "does not affect backpropogation.\n",
    "\n",
    "\n",
    "## Implementation\n",
    "\n",
    "### Game State\n",
    "\n",
    "The state of a game is encoded similarly but not identically to what was done in the brandub\n",
    "package. The state tensor has is a stack of 7x7 binary tensors and some constant tensors.\n",
    "The first layer encodes the current player's pawns and the second layer is the current player's\n",
    "monarch. The current player might be the attacker and not have a monarch, but that is fine and\n",
    "the input plane will be all zero. These layers repeat with the opponent's positions. \n",
    "AlphaZero maintains 8 timesteps of history, but we will try to keep only 3, if that. While the connections\n",
    "from these planes into the neural net\n",
    "will be learned and truly will be input, I suspect their main use will be checking to see if a \n",
    "position has repeated leading to a draw. AlphaZero for Chess contains two more constant planes, \n",
    "either 1 or 0, depending on if the current position has repeated once or twice before.\n",
    "Probably the order matters, although I am not sure. Since brandub ends in a draw with only a \n",
    "single repeated position, we do not need to encode this information for the player.\n",
    "\n",
    "In addition we will include a constant plane that is not repeated to indicate which player is\n",
    "playing, either 0 for defense or 1 for attack.\n",
    "\n",
    "Overall, this leads to a game state tensor of size $7\\times7\\times(4\\times3 + 1)$, i.e.,\n",
    "we have 637 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_initial_game_state():\n",
    "    \"\"\"returns pytorch tensor encoding initial brandub game state\"\"\"\n",
    "    \n",
    "    game_state = torch.zeros([7, 7, 13], dtype=torch.float64)\n",
    "    \n",
    "    # attacking pawns\n",
    "    game_state[3, 0, 0] = 1\n",
    "    game_state[3, 1, 0] = 1\n",
    "    game_state[3, 5, 0] = 1\n",
    "    game_state[3, 6, 0] = 1\n",
    "    game_state[0, 3, 0] = 1\n",
    "    game_state[1, 3, 0] = 1\n",
    "    game_state[5, 3, 0] = 1\n",
    "    game_state[6, 3, 0] = 1\n",
    "    \n",
    "    # defensive pawns\n",
    "    game_state[3, 2, 2] = 1\n",
    "    game_state[3, 4, 2] = 1\n",
    "    game_state[2, 3, 2] = 1\n",
    "    game_state[4, 3, 2] = 1\n",
    "    \n",
    "    # defensive monarch\n",
    "    game_state[3, 3, 3] = 1\n",
    "    \n",
    "    # attack's turn\n",
    "    game_state[:, :, 12] = 1\n",
    "    \n",
    "    return game_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write some functions to check for end game conditions. While I won't be optimizing this\n",
    "code too much, I won't bother to check types and board legality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monarch_index(game_state):\n",
    "    \"\"\" return which game state layer encodes the defensive monarch \n",
    "    for the current board \"\"\"\n",
    "    if game_state[0, 0, 12] == 1:\n",
    "        return 3\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "    \n",
    "def attack_victory(board, monarch_index):\n",
    "    \"\"\" attack wins if the monarch is not on the current board\"\"\"\n",
    "    \n",
    "    return board[:, :, monarch_index].max() == 0\n",
    "\n",
    "\n",
    "def defense_victory(board, monarch_index):\n",
    "    \"\"\" defense wins if the monarch is in the forest \"\"\"\n",
    "    \n",
    "    return board[0, 0, monarch_index] == 1 or \\\n",
    "        board[0, 6, monarch_index] == 1 or \\\n",
    "        board[6, 0, monarch_index] == 1 or \\\n",
    "        board[6, 6, monarch_index] == 1\n",
    "\n",
    "\n",
    "def board_shadow(board, is_attack_turn):\n",
    "    \"\"\" return a 7x7 tensor encoding current state with\n",
    "    -1 being an attacking pawn, 1 a defensive pawn,\n",
    "    2 the monarch.\n",
    "    \n",
    "    board is a 7x7x4 tensor\n",
    "    is_attack_turn is a boolean\n",
    "    \n",
    "    This is used to look for draws with game states\"\"\"\n",
    "    \n",
    "    raw_shadow = board[:, :, 0] + 2 * board[:, :, 1] - \\\n",
    "        board[:, :, 2] - 2 * board[:, :, 3]\n",
    "    \n",
    "    if is_attack_turn:\n",
    "        return -1 * raw_shadow\n",
    "    else:\n",
    "        return raw_shadow\n",
    "    \n",
    "    \n",
    "def game_shadow(game_state):\n",
    "    \"\"\" returns the board shadow for the current board\n",
    "    and player. \"\"\"\n",
    "    \n",
    "    return board_shadow(game_state[:, :, :4], game_state[0, 0, 12] == 1)\n",
    "        \n",
    "\n",
    "def drawn_game(board, game_state):\n",
    "    \"\"\" take a new board and compare to the boards in the game state\n",
    "    to see if it would result in a draw.\n",
    "    \n",
    "    NOTE: This assumes that the current player for board is DIFFERENT\n",
    "    than the current player in the game_state. \n",
    "    \n",
    "    NOTE: Do not bother checking most recent game state, as we\n",
    "    assume the presented board is after a move. \"\"\"\n",
    "    \n",
    "    is_attack_turn = game_state[0, 0, 12] == 0  # note: changing game state\n",
    "    new_board_shadow = board_shadow(board, is_attack_turn)\n",
    "    \n",
    "    # check two moves ago\n",
    "    board_shadow_1 = board_shadow(game_state[:, :, 4:8], is_attack_turn)\n",
    "    \n",
    "    if torch.all(torch.eq(new_board_shadow, board_shadow_1)):\n",
    "        return True\n",
    "    else:\n",
    "        board_shadow_2 = board_shadow(game_state[:, :, 8:12],\n",
    "                                     not is_attack_turn)\n",
    "        return torch.all(torch.eq(new_board_shadow, board_shadow_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0., -0., -0., -1., -0., -0., -0.],\n",
      "        [-0., -0., -0., -1., -0., -0., -0.],\n",
      "        [-0., -0., -0.,  1., -0., -0., -0.],\n",
      "        [-1., -1.,  1.,  2.,  1., -1., -1.],\n",
      "        [-0., -0., -0.,  1., -0., -0., -0.],\n",
      "        [-0., -0., -0., -1., -0., -0., -0.],\n",
      "        [-0., -0., -0., -1., -0., -0., -0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "initial_game_state = get_initial_game_state()\n",
    "print(game_shadow(initial_game_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movement\n",
    "\n",
    "Movement will be encoded as a $7\\times7\\times24$ tensor. The first two coordinates\n",
    "indicate the position on the board where a piece should be found and the\n",
    "final coordinate indicates the direction. The value modulo 6 then adding 1 (not\n",
    "modulo 6) is the amount of spaces to move, raw values between 0 and 5 indicate\n",
    "moving down, 6 to 11 moving up, 12 to 17 moving right, 18 to 23 moving\n",
    "left.\n",
    "\n",
    "This will be the shape of the probability vector put out by the neural net and\n",
    "will include invalid moves. The first step of the MCTS will be to reduce this\n",
    "only to valid moves.\n",
    "\n",
    "A singular move will be an index tensor that we can samples using the `Categorical`\n",
    "object in `torch.distributions.categorical`.\n",
    "\n",
    "We will need auxiliary functions to determine legal moves based on the\n",
    "current board, to map from a raw probability vector from the network to probabilities\n",
    "of only legal moves, and then a function which takes the game state and a legal\n",
    "move and returns a new game state. We start with the functions which find all\n",
    "valid moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_forest(position_2d):\n",
    "    \"\"\" returns whether the first two coords of\n",
    "    position_2d is a forest\"\"\"\n",
    "    return position_2d[0] in [0, 6] and position_2d[1] in [0, 6]\n",
    "\n",
    "\n",
    "def is_castle(position_2d):\n",
    "    \"\"\" returns whether the first two coords of position_2d\n",
    "    is a castle\"\"\"\n",
    "    return position_2d[0] == 3 and position_2d[1] == 3\n",
    "\n",
    "\n",
    "def valid_moves(board, position):\n",
    "    \"\"\" find valid movement indices 0-24 for a piece\n",
    "    that is (assumed to be) at position, which is a list like with length 2\"\"\"\n",
    "    \n",
    "    is_pawn = board[position[0], position[1], 1] == 0\n",
    "    \n",
    "    shadow = board[:, :, 0] + board[:, :, 1] + board[:, :, 2] + board[:, :, 3]\n",
    "    \n",
    "    def check_direction(direction_vector):\n",
    "        \"\"\" get valid moves in the given direction\n",
    "        direction must be [+/-1, 0] or [0, +/-1] torch tensors\n",
    "        \"\"\"\n",
    "\n",
    "        valid_moves = []\n",
    "\n",
    "        coord = 0 if direction_vector[0] != 0 else 1\n",
    "        positive_direction = direction_vector[coord] == 1\n",
    "        end_value = 6 if positive_direction else 0\n",
    "        \n",
    "        if coord == 0:\n",
    "            if positive_direction:\n",
    "                base = 0\n",
    "            else:\n",
    "                base = 6\n",
    "        else:\n",
    "            if positive_direction:\n",
    "                base = 12\n",
    "            else:\n",
    "                base = 18\n",
    "\n",
    "        keep_going = position[coord] != end_value\n",
    "        i = 0\n",
    "        while keep_going:\n",
    "            i += 1\n",
    "\n",
    "            new_pos = position + i * direction_vector\n",
    "\n",
    "            # stop if run into a piece\n",
    "            if shadow[tuple(new_pos)] == 1:\n",
    "                break\n",
    "\n",
    "            # ignore the castle\n",
    "            if is_castle(new_pos):\n",
    "                continue\n",
    "\n",
    "            keep_going = new_pos[coord] != end_value\n",
    "\n",
    "            # if pawn and at the wall, see if its a forest but don't add\n",
    "            if not keep_going and is_pawn and is_forest(new_pos):\n",
    "                break\n",
    "\n",
    "            valid_moves.append(base + i - 1)\n",
    "\n",
    "        return valid_moves\n",
    "    \n",
    "    direction_vectors = (\n",
    "        torch.tensor([1, 0]),\n",
    "        torch.tensor([-1, 0]),\n",
    "        torch.tensor([0, 1]),\n",
    "        torch.tensor([0, -1])\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        direction\n",
    "        for dvec in direction_vectors\n",
    "        for direction in check_direction(dvec)\n",
    "    ]\n",
    "    \n",
    "    \n",
    "def all_valid_moves(board):\n",
    "    \"\"\" get all indices  of valid moves. Only pieces on the first\n",
    "    two planes can move. \n",
    "    \n",
    "    Returns 7x7x24 binary tensor \"\"\"\n",
    "    \n",
    "    # get all pieces that can move\n",
    "    positions = [pos[:2] for pos in board[:, :, :2].nonzero()]\n",
    "    pos_moves = [\n",
    "        (pos[0], pos[1], move_index)\n",
    "        for pos in positions\n",
    "        for move_index in valid_moves(board, pos)\n",
    "    ]\n",
    "    \n",
    "    moves = torch.zeros(7, 7, 24)\n",
    "    for pos_move in pos_moves:\n",
    "        moves[pos_move] = 1\n",
    "        \n",
    "    return moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write the functions that allow us to move pieces and then remove any pieces that have\n",
    "been captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_pieces(board):\n",
    "    \"\"\" remove any pieces on the board which are captured \"\"\"\n",
    "    return board\n",
    "\n",
    "\n",
    "def move_piece(board, move):\n",
    "    \"\"\" take the board and do the move and return a new board\n",
    "    after that move. Move is a tensor of size 3.\n",
    "    \n",
    "    NOTE: this will perform invalid moves, although it\n",
    "    does guarantee that a piece was there.\n",
    "    \n",
    "    This will flip the attacker and defender.\"\"\"\n",
    "    \n",
    "    # piece position\n",
    "    i, j = move[0], move[1]\n",
    "    \n",
    "    # whether to move vertically or horizontally\n",
    "    move_vertical = move[2] < 12\n",
    "    \n",
    "    # how much to move\n",
    "    direction = 1 if move[2] % 12 < 6 else -1\n",
    "    move_val = direction * (move[2] % 6 + 1)\n",
    "    \n",
    "    # find where the piece is\n",
    "    plane = None\n",
    "    k = 0\n",
    "    while plane is None and k < 4:\n",
    "        if board[i, j, k] == 1:\n",
    "            plane = k\n",
    "        k += 1\n",
    "    \n",
    "    assert plane is not None, \"No piece found\"\n",
    "    \n",
    "    # move the piece\n",
    "    board_1 = torch.zeros(7, 7, 4, dtype=torch.float64)\n",
    "    board_1[:, :, :] = board\n",
    "    \n",
    "    board_1[i, j, plane] = 0\n",
    "    \n",
    "    if move_vertical:\n",
    "        board_1[i + move_val, j, plane] = 1\n",
    "    else:\n",
    "        board_1[i, j + move_val, plane] = 1\n",
    "        \n",
    "    \n",
    "    # capture pieces\n",
    "    board_2 = capture_pieces(board_1)\n",
    "    \n",
    "    \n",
    "    # flip players\n",
    "    new_board = torch.zeros(7, 7, 4, dtype=torch.float64)\n",
    "    new_board[:, :, :2] = board_2[:, :, 2:]\n",
    "    new_board[:, :, 2:] = board_2[:, :, :2]\n",
    "    \n",
    "    return new_board\n",
    "\n",
    "\n",
    "def advance_game_state(board, game_state):\n",
    "    \"\"\" return a new game state by adding the given\n",
    "    board.\n",
    "    \n",
    "    The game may or may not be over at this point. \"\"\"\n",
    "    \n",
    "    new_game_state = tensor.zeros(7, 7, 13, dtype=torch.float64)\n",
    "    \n",
    "    # update boards\n",
    "    new_game_state[:, :, :4] = board\n",
    "    new_game_state[:, :, 4:8] = game_state[:, :, :4]\n",
    "    new_game_state[:, :, 8:12] = game_state[:, 4:8]\n",
    "    \n",
    "    # update player\n",
    "    new_game_state[:, :, 12] = 1 - game_state[:, :, 12]\n",
    "    \n",
    "    return new_game_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gitpage",
   "language": "python",
   "name": "gitpage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
