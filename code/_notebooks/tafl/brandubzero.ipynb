{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: code-post\n",
    "title: AlphaZero for Brandub\n",
    "description: We provide a from scratch implementation of AlphaZero to learn how to play brandub.\n",
    "tags: [tafl, neural nets]\n",
    "---\n",
    "\n",
    "In this post I'm going to explore how to implement [AlphaZero](https://arxiv.org/pdf/1712.01815.pdf)\n",
    "to learn how to play Brandub (a tafl variant). I previously put together a [Brandub package](https://github.com/kevinnowland/brandub)\n",
    "which I won't use directly, but whose code I will modify as needed. You can find the rules of the\n",
    "game there.\n",
    "\n",
    "The main purpose of this is to prove that I understand the algorithm well enough to implement\n",
    "a very crude version of it. How crude? I doubt the best trained player after even a few days of training\n",
    "will be able to play very well. My big question is whether the trained model can beat me, who has\n",
    "never played a full game before despite all this coding that I've done.\n",
    "\n",
    "\n",
    "## Algorithm description\n",
    "\n",
    "### Game play technology\n",
    "\n",
    "AlphaZero plays games using two main pieces of technology. The first is a neural network $f\\_\\theta(s)$ with\n",
    "parameters $\\theta$ which takes the current game state $s$ and outputs $(p, z)$ where where $p$ is a probability\n",
    "vector of all possible moves and $z \\in [-1, 1]$ is an estimated game score for the current player with -1 being a\n",
    "loss, 0 a draw, and 1 a win. Only the neural network parameters $\\theta$ will be learned.\n",
    "\n",
    "One could play \n",
    "just using $f\\_\\theta$ by choosing moves according to the output move probabilities $p$, \n",
    "but instead of doing this, we rely on\n",
    "the second piece of technology, a Monte Carlo Tree Search (MCTS) algorithm. The tree search\n",
    "uses the neural network to explore the space of moves and choose the best one taking into account\n",
    "how the game might proceed. The neural network output, as we will\n",
    "see later, is extremely raw and can suggest moves which are illegal (moving an opponent's piece) or \n",
    "even impossible (moving off the board). In addition to suggestiongm oves which will likely lead to victory\n",
    "for the curernt player, the tree search algorithm helps encode the rules of the game\n",
    "by by only exploring using legal moves. The tree\n",
    "search is not learned directly, but does take as inputs the neural network $f\\_\\theta$ and current game state. \n",
    "The output is a policy $\\pi$, a probability vector that is used to select the next move.\n",
    "\n",
    "### Learning\n",
    "\n",
    "Give the neural network and MCTS algorithm, play proceeds until a game ends at step $T$.\n",
    "At each step $t$ of the game we have neural network output $(p\\_t, v\\_t)$ as well as \n",
    "the policy $\\pi\\_t$ governing move selection. The final game result is $s\\_T \\in \\{-1, 0, 1\\}$. \n",
    "The loss $\\ell\\_t$ for a step is\n",
    "$$\n",
    "  \\ell\\_t = (v\\_t - s\\_T)^2 - \\pi\\_t^t\\log p\\_t + c \\|\\theta\\|^2,\n",
    "$$\n",
    "where $c$ is an $\\ell^2$-regularization parameter. Thus we are looking at mean squared\n",
    "error for the \n",
    "predicted output $v$ with cross-entropy forcing predicted moved probabilities $p$ to look like \n",
    "the MCTS policy $\\pi$ with a regularization term.\n",
    "The overall loss function is the average of this over the $T$ moves making up the game.\n",
    "\n",
    "After each game, we backpropagate from the loss function through the nerual network to\n",
    "change the neural network parameters $\\theta$. Note that while policies $\\pi\\_t$ do depend on \n",
    "$\\theta$ since the MCTS takes the nerual network as input, we will pretend it does not and thus \n",
    "does not affect backpropogation.\n",
    "\n",
    "\n",
    "## Implementation\n",
    "\n",
    "### Game State\n",
    "\n",
    "The state of a game is encoded similarly but not identically to what was done in the brandub\n",
    "package. The state tensor has is a stack of 7x7 binary tensors and some constant tensors.\n",
    "The first layer encodes the current player's pawns and the second layer is the current player's\n",
    "monarch. The current player might be the attacker and not have a monarch, but that is fine and\n",
    "the input plane will be all zero. These layers repeat with the opponent's positions. \n",
    "AlphaZero maintains 8 timesteps of history, but we will try to keep only 3, if that. While the connections\n",
    "from these planes into the neural net\n",
    "will be learned and truly will be input, I suspect their main use will be checking to see if a \n",
    "position has repeated leading to a draw. AlphaZero for Chess contains two more constant planes, \n",
    "either 1 or 0, depending on if the current position has repeated once or twice before.\n",
    "Probably the order matters, although I am not sure. Since brandub ends in a draw with only a \n",
    "single repeated position, we do not need to encode this information for the player.\n",
    "\n",
    "In addition we will include a constant plane that is not repeated to indicate which player is\n",
    "playing, either 0 for defense or 1 for attack.\n",
    "\n",
    "Overall, this leads to a game state tensor of size $7\\times7\\times(4\\times3 + 1)$, i.e.,\n",
    "we have 637 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_initial_game_state():\n",
    "    \"\"\"returns pytorch tensor encoding initial brandub game state\"\"\"\n",
    "    \n",
    "    game_state = torch.zeros(13, 7, 7, dtype=torch.float32)\n",
    "    \n",
    "    # attacking pawns\n",
    "    game_state[0, 3, 0] = 1\n",
    "    game_state[0, 3, 1] = 1\n",
    "    game_state[0, 3, 5] = 1\n",
    "    game_state[0, 3, 6] = 1\n",
    "    game_state[0, 0, 3] = 1\n",
    "    game_state[0, 1, 3] = 1\n",
    "    game_state[0, 5, 3] = 1\n",
    "    game_state[0, 6, 3] = 1\n",
    "    \n",
    "    # defensive pawns\n",
    "    game_state[2, 3, 2] = 1\n",
    "    game_state[2, 3, 4] = 1\n",
    "    game_state[2, 2, 3] = 1\n",
    "    game_state[2, 4, 3] = 1\n",
    "    \n",
    "    # defensive monarch\n",
    "    game_state[3, 3, 3] = 1\n",
    "    \n",
    "    # attack's turn\n",
    "    game_state[12, :, :] = 1\n",
    "    \n",
    "    return game_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write some functions to check for end game conditions. While I won't be optimizing this\n",
    "code too much, I won't bother to check types and board legality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monarch_index(game_state):\n",
    "    \"\"\" return which game state layer encodes the defensive monarch \n",
    "    for the current board \"\"\"\n",
    "    if game_state[12, 0, 0] == 1:\n",
    "        return 3\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "    \n",
    "def attack_victory(board, monarch_index):\n",
    "    \"\"\" attack wins if the monarch is not on the current board\"\"\"\n",
    "    \n",
    "    return board[monarch_index, :, :].max() == 0\n",
    "\n",
    "\n",
    "def defense_victory(board, monarch_index):\n",
    "    \"\"\" defense wins if the monarch is in the forest \"\"\"\n",
    "    \n",
    "    if board[monarch_index, 0, 0] == 1 or \\\n",
    "        board[monarch_index, 0, 6] == 1 or \\\n",
    "        board[monarch_index, 6, 0] == 1 or \\\n",
    "        board[monarch_index, 6, 6] == 1:\n",
    "        return True\n",
    "    \n",
    "    if monarch_index == 1:\n",
    "        no_attackers = board[2, :, :].sum() == 0\n",
    "    else:\n",
    "        no_attackers = board[0, :, :].sum() == 0\n",
    "    return no_attackers\n",
    "\n",
    "\n",
    "def get_relative_end_value(board, game_state):\n",
    "    \"\"\" returns 1, -1, 0, or None based on if the current\n",
    "    player is winning. In this setup, the board\n",
    "    is a new board we might want to potentially add to the\n",
    "    game_state before the players are switched. \"\"\"\n",
    "    \n",
    "    attacker_is_playing = game_state[12, 0, 0] == 1\n",
    "    monarch_index = 3 if attacker_is_playing else 1\n",
    "    \n",
    "    if attack_victory(board, monarch_index):\n",
    "        if attacker_is_playing:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    elif defense_victory(board, monarch_index):\n",
    "        if attacker_is_playing:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    elif drawn_game(board, game_state):\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_end_value(board, game_state):\n",
    "    \"\"\" return 1, -1, 0, or None based on attacker\n",
    "    winning, defender winning, draw, no result \"\"\"\n",
    "    \n",
    "    attacker_is_playing = game_state[12, 0, 0] == 1\n",
    "    monarch_index = 3 if attacker_is_playing else 1\n",
    "    \n",
    "    if attack_victory(board, monarch_index):\n",
    "        return 1\n",
    "    elif defense_victory(board, monarch_index):\n",
    "        return -1\n",
    "    elif drawn_game(board, game_state):\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def board_shadow(board, is_attack_turn):\n",
    "    \"\"\" return a 7x7 tensor encoding current state with\n",
    "    -1 being an attacking pawn, 1 a defensive pawn,\n",
    "    2 the monarch.\n",
    "    \n",
    "    board is a 4x7x7 tensor\n",
    "    is_attack_turn is a boolean\n",
    "    \n",
    "    This is used to look for draws with game states\"\"\"\n",
    "    \n",
    "    raw_shadow = board[0, :, :] + 2 * board[1, :, :] - \\\n",
    "        board[2, :, :] - 2 * board[3, :, :]\n",
    "    \n",
    "    if is_attack_turn:\n",
    "        return -1 * raw_shadow\n",
    "    else:\n",
    "        return raw_shadow\n",
    "    \n",
    "    \n",
    "def game_shadow(game_state):\n",
    "    \"\"\" returns the board shadow for the current board\n",
    "    and player. \"\"\"\n",
    "    \n",
    "    return board_shadow(game_state[:4, :, :], game_state[12, 0, 0] == 1)\n",
    "        \n",
    "\n",
    "def drawn_game(board, game_state):\n",
    "    \"\"\" take a new board and compare to the boards in the game state\n",
    "    to see if it would result in a draw.\n",
    "    \n",
    "    NOTE: This assumes that the current player for board is THE SAME AS\n",
    "    the current player in the game_state. \n",
    "    \n",
    "    NOTE: Do not bother checking most recent game state, as we\n",
    "    assume the presented board is after a move. \"\"\"\n",
    "    \n",
    "    is_attack_turn = game_state[12, 0, 0] == 1\n",
    "    new_board_shadow = board_shadow(board, is_attack_turn)\n",
    "    \n",
    "    # check two moves ago\n",
    "    board_shadow_1 = board_shadow(game_state[4:8, :, :], is_attack_turn)\n",
    "    \n",
    "    if torch.all(torch.eq(new_board_shadow, board_shadow_1)):\n",
    "        return True\n",
    "    else:\n",
    "        board_shadow_2 = board_shadow(game_state[8:12, :, :],\n",
    "                                     not is_attack_turn)\n",
    "        return torch.all(torch.eq(new_board_shadow, board_shadow_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0., -0., -0., -1., -0., -0., -0.],\n",
      "        [-0., -0., -0., -1., -0., -0., -0.],\n",
      "        [-0., -0., -0.,  1., -0., -0., -0.],\n",
      "        [-1., -1.,  1.,  2.,  1., -1., -1.],\n",
      "        [-0., -0., -0.,  1., -0., -0., -0.],\n",
      "        [-0., -0., -0., -1., -0., -0., -0.],\n",
      "        [-0., -0., -0., -1., -0., -0., -0.]])\n"
     ]
    }
   ],
   "source": [
    "initial_game_state = get_initial_game_state()\n",
    "print(game_shadow(initial_game_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movement\n",
    "\n",
    "Movement will be encoded as a $7\\times7\\times24$ tensor. The first two coordinates\n",
    "indicate the position on the board where a piece should be found and the\n",
    "final coordinate indicates the direction. The value modulo 6 then adding 1 (not\n",
    "modulo 6) is the amount of spaces to move, raw values between 0 and 5 indicate\n",
    "moving down, 6 to 11 moving up, 12 to 17 moving right, 18 to 23 moving\n",
    "left. $(7, 7, 4)$ will also be the shape of the probability vector put out by the neural net and\n",
    "will include invalid moves. The first step of the MCTS will be to reduce this\n",
    "only to valid moves. A singular move will be an index tensor that we can samples\n",
    "using the `Categorical` object in `torch.distributions.categorical`.\n",
    "\n",
    "We will need auxiliary functions to find valid moves and then perform\n",
    "those moves. Again, since we're trying to remove unnecessary cruft, we will\n",
    "not verify many things, such as piece existence. In the following code block\n",
    "we will write auxiliary functions involved with finding valid moves\n",
    "and the new position of a piece given some move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_forest(position_2d):\n",
    "    \"\"\" returns whether the first two coords of\n",
    "    position_2d is a forest\"\"\"\n",
    "    return position_2d[0] in [0, 6] and position_2d[1] in [0, 6]\n",
    "\n",
    "\n",
    "def is_castle(position_2d):\n",
    "    \"\"\" returns whether the first two coords of position_2d\n",
    "    is a castle\"\"\"\n",
    "    return position_2d[0] == 3 and position_2d[1] == 3\n",
    "   \n",
    "\n",
    "def find_piece_plane(board, position_2d, planes=[2, 3, 0, 1]):\n",
    "    \"\"\" returns the plane of piece at given (i, j) position \"\"\"\n",
    "    for k in planes:\n",
    "        if board[k, position_2d[0], position_2d[1]] == 1:\n",
    "            return k\n",
    "        \n",
    "        \n",
    "def find_piece_at_position(board, position_2d):\n",
    "    \"\"\" returns the (i, j, k) position of the piece at position (i, j) \n",
    "    if some is present else returns None \"\"\"\n",
    "    \n",
    "    k = find_piece_plane(board, position_2d, [2, 3, 0, 1])\n",
    "    if k is not None:\n",
    "        return (k, position_2d[0], position_2d[1])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "        \n",
    "def valid_moves(board, position):\n",
    "    \"\"\" find valid movement indices 0-24 for a piece\n",
    "    that is (assumed to be) at position, which is a tensor with length 2\"\"\"\n",
    "    \n",
    "    is_pawn = board[1, position[0], position[1]] == 0\n",
    "    \n",
    "    shadow = board[0, :, :] + board[1, :, :] + board[2, :, :] + board[3, :, :]\n",
    "    \n",
    "    def check_direction(direction_vector):\n",
    "        \"\"\" get valid moves in the given direction\n",
    "        direction must be [+/-1, 0] or [0, +/-1] torch tensors\n",
    "        \"\"\"\n",
    "\n",
    "        valid_moves = []\n",
    "\n",
    "        coord = 0 if direction_vector[0] != 0 else 1\n",
    "        positive_direction = direction_vector[coord] == 1\n",
    "        end_value = 6 if positive_direction else 0\n",
    "        \n",
    "        if coord == 0:\n",
    "            if positive_direction:\n",
    "                base = 0\n",
    "            else:\n",
    "                base = 6\n",
    "        else:\n",
    "            if positive_direction:\n",
    "                base = 12\n",
    "            else:\n",
    "                base = 18\n",
    "\n",
    "        keep_going = position[coord] != end_value\n",
    "        i = 0\n",
    "        while keep_going:\n",
    "            i += 1\n",
    "\n",
    "            new_pos = position + i * direction_vector\n",
    "\n",
    "            # stop if run into a piece\n",
    "            if shadow[tuple(new_pos)] == 1:\n",
    "                break\n",
    "\n",
    "            # ignore the castle\n",
    "            if is_castle(new_pos):\n",
    "                continue\n",
    "\n",
    "            keep_going = new_pos[coord] != end_value\n",
    "\n",
    "            # if pawn and at the wall, see if its a forest but don't add\n",
    "            if not keep_going and is_pawn and is_forest(new_pos):\n",
    "                break\n",
    "\n",
    "            valid_moves.append(base + i - 1)\n",
    "\n",
    "        return valid_moves\n",
    "    \n",
    "    direction_vectors = (\n",
    "        torch.tensor([1, 0]),\n",
    "        torch.tensor([-1, 0]),\n",
    "        torch.tensor([0, 1]),\n",
    "        torch.tensor([0, -1])\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        direction\n",
    "        for dvec in direction_vectors\n",
    "        for direction in check_direction(dvec)\n",
    "    ]\n",
    "\n",
    "\n",
    "def valid_move_indices(board):\n",
    "    move_indices = [\n",
    "        (move_index, pos[1], pos[2])\n",
    "        for pos in board[:2, :, :,].nonzero(as_tuple=False)\n",
    "        for move_index in valid_moves(board, pos[1:])\n",
    "    ]\n",
    "    \n",
    "    return move_indices\n",
    "\n",
    "\n",
    "def valid_move_tensor(board):\n",
    "    \"\"\" get tensor of all valid moves. Only pieces on the first\n",
    "    two planes can move. \n",
    "    \n",
    "    Returns 24x7x7 binary tensor \"\"\"\n",
    "    \n",
    "    # get all pieces that can move\n",
    "    move_indices = valid_move_indices(board)\n",
    "    \n",
    "    move_tensor = torch.zeros(24, 7, 7)\n",
    "    for move_index in move_indices:\n",
    "        move_tensor[move_index] = 1\n",
    "        \n",
    "    return move_tensor\n",
    "\n",
    "\n",
    "def find_new_position(board, move):\n",
    "    \"\"\" find the piece at the given\n",
    "    position and return a tuple containing\n",
    "    the pieces new location.\n",
    "    \n",
    "    NOTE: assumes piece is on the first or second plane \"\"\"\n",
    "    \n",
    "    # whether to move vertically or horizontally\n",
    "    move_vertical = move[0] < 12\n",
    "    \n",
    "    # how much to move\n",
    "    direction = 1 if move[0] % 12 < 6 else -1\n",
    "    move_val = direction * (move[0] % 6 + 1)\n",
    "    \n",
    "    # find where the piece is\n",
    "    plane = find_piece_plane(board, move[1:], [0, 1])\n",
    "    \n",
    "    if move_vertical:\n",
    "        return (plane, move[1] + move_val, move[2])\n",
    "    else:\n",
    "        return (plane, move[1], move[2] + move_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write functions that allow us to advance game state by performing a move\n",
    "and then capturing the pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_piece_captured(board, position):\n",
    "    \"\"\" determine if a piece at (i, j, k) is captured.\n",
    "    \n",
    "    NOTE: does not check that a piece is at the position\n",
    "    \n",
    "    NOTE: This does not validate that the raw_board is a\n",
    "    vaid raw_board. \"\"\"\n",
    "    \n",
    "    i = position[1]\n",
    "    j = position[2]\n",
    "\n",
    "    topography_inds_corners = [(0, 0), (0, 6), (6, 0), (6, 6)]\n",
    "\n",
    "    if position[0] == 0 or position[0] == 2:\n",
    "        # pawn logic\n",
    "        \n",
    "        if position[0] == 0:\n",
    "            enemy_board = board[2, :, :] + board[3, :, :]\n",
    "        else:\n",
    "            enemy_board = board[0, :, :] + board[1, :, :]\n",
    "        \n",
    "        if board[3, 3, 1] + board[3, 3, 3] == 0:\n",
    "            # no monarch? then castle is threat\n",
    "            topography_inds = topography_inds_corners + [(3, 3)]\n",
    "        else:\n",
    "            topography_inds = topography_inds_corners\n",
    "\n",
    "        topography = torch.zeros(7, 7)\n",
    "        for ind in topography_inds:\n",
    "            topography[ind] = 1\n",
    "        bad_things = enemy_board + topography\n",
    "\n",
    "        if i == 0 or i == 6:\n",
    "            # on top or bottom (can't be in corner)\n",
    "            if bad_things[i, j-1] == 1 and bad_things[i, j+1] == 1:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        elif j == 0 or j == 6:\n",
    "            # on left or right side (can't be in corner)\n",
    "            if bad_things[i-1, j] == 1 and bad_things[i+1, j] == 1:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            # otherwise just check\n",
    "            if bad_things[i, j-1] == 1 and bad_things[i, j+1] == 1:\n",
    "                return True\n",
    "            elif bad_things[i+1, j] == 1 and bad_things[i-1, j] == 1:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "    elif position[0] == 1 or position[0] == 3:\n",
    "        # monarch logic\n",
    "        \n",
    "        if position[2] == 1:\n",
    "            enemy_board = board[2, :, :]\n",
    "        else:\n",
    "            enemy_board = board[0, :, :]\n",
    "\n",
    "        if i != 3 or j != 3:\n",
    "            # empty castles are threats\n",
    "            topography_inds = topography_inds_corners + [(3, 3)]\n",
    "        else:\n",
    "            topography_inds = topography_inds_corners\n",
    "\n",
    "        topography = torch.zeros(7, 7)\n",
    "        for ind in topography_inds:\n",
    "            topography[ind] = 1\n",
    "        bad_things = enemy_board + topography\n",
    "\n",
    "        if (i, j) in [(3, 2), (3, 3), (3, 4), (2, 3), (4, 3)]:\n",
    "            # in or next to castle have to be surrounded\n",
    "            if bad_things[i+1, j] == 1 and bad_things[i-1, j] == 1 \\\n",
    "                and bad_things[i, j+1] == 1 and bad_things[i, j-1] == 1:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            if (i, j) in [(0, 0), (0, 6), (6, 0), (6, 6)]:\n",
    "                # safe in corner\n",
    "                return False\n",
    "            elif i == 0 or i == 6:\n",
    "                # top or bottom: non corner\n",
    "                if bad_things[i, j+1] == 1 and bad_things[i, j-1] == 1:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "            elif j == 0 or j == 6:\n",
    "                # left or right: non corner\n",
    "                if bad_things[i+1, j] == 1 and bad_things[i-1, j] == 1:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "            else:\n",
    "                # any other spot on board\n",
    "                if bad_things[i+1, j] == 1 and bad_things[i-1, j] == 1:\n",
    "                    return True\n",
    "                elif bad_things[i, j+1] == 1 and bad_things[i, j-1] == 1:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "    else:\n",
    "        msg = \"position[0] must be in {0, 1, 2, 3}\"\n",
    "        raise Exception(msg)\n",
    "\n",
    "\n",
    "def game_state_move(game_state, move):\n",
    "    \"\"\" advance game state by moving piece, performing captures, \n",
    "    flipping players, and returning a new game state. Also\n",
    "    returns if the game is over as needed.\n",
    "    \n",
    "    NOTE: asumes the move is valid but does not check \n",
    "    \n",
    "    This takes about 0.55 ms\"\"\"\n",
    "    \n",
    "    # perform the move\n",
    "    old_board = game_state[:4, :, :]\n",
    "    new_pos = find_new_position(game_state[:4, :, :], move)\n",
    "    old_pos = (new_pos[0], move[1], move[2])\n",
    "    \n",
    "    new_board_ = torch.zeros(4, 7, 7)\n",
    "    new_board_[:, :, :] = old_board\n",
    "    new_board_[old_pos] = 0\n",
    "    new_board_[new_pos] = 1\n",
    "    \n",
    "    # check any pieces near the given piece to see if they are captured\n",
    "    # if so, remove them from the new_board\n",
    "    i = new_pos[1]\n",
    "    j = new_pos[2]\n",
    "    \n",
    "    for check_pos in [(i, j), (i-1, j), (i+1, j), (i, j-1), (i, j+1)]:\n",
    "        \n",
    "        if check_pos[0] >= 0 and check_pos[0] <= 6 \\\n",
    "            and check_pos[1] >=0 and check_pos[1] <= 6:\n",
    "            \n",
    "            piece_pos_ = find_piece_at_position(new_board_, check_pos)\n",
    "            \n",
    "            if piece_pos_ is not None:\n",
    "                if is_piece_captured(new_board_, piece_pos_):\n",
    "                    new_board_[piece_pos_] = 0\n",
    "                    \n",
    "    \n",
    "    # see if the new_board_ is in a victory state\n",
    "    # return if so\n",
    "    end_game_value = get_relative_end_value(new_board_, game_state)\n",
    "        \n",
    "    if end_game_value is not None:\n",
    "        return None, end_game_value\n",
    "    else:\n",
    "        # flip players\n",
    "        new_board = torch.zeros(4, 7, 7, dtype=torch.float32)\n",
    "        new_board[:2, :, :] = new_board_[2:, :, :]\n",
    "        new_board[2:, :, :] = new_board_[:2, :, :]\n",
    "    \n",
    "        # form the new game_state\n",
    "        new_game_state = torch.zeros(13, 7, 7, dtype=torch.float32)\n",
    "\n",
    "        # update boards\n",
    "        new_game_state[:4, :, :] = new_board\n",
    "        new_game_state[4:8, :, :] = game_state[:4, :, :]\n",
    "        new_game_state[8:12, :, :] = game_state[4:8, :, :]\n",
    "\n",
    "        # update player\n",
    "        new_game_state[12, :, :] = 1 - game_state[12, :, :]\n",
    "    \n",
    "        return new_game_state, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now play a game starting from the initial game state provided by `get_initial_game_state()`\n",
    "and applying moves using `game_state_move()`. We would have to be careful to only supply\n",
    "valid moves though, because the board could easily get into an invalid state since we are\n",
    "not verifying many things. As written, `game_state_move()` returns two values, one of which will\n",
    "always be none. The `new_game_state` is returned if the game is not over and the second\n",
    "value is returned as `None`. If the game is over, `new_game_state` will be `None` (and returned first)\n",
    "and the second parameter will be 1 if the player who submitted the move won, -1 if they lost because\n",
    "of their move, and 0 if the game ended in a draw.\n",
    "\n",
    "### Monte Carlo Tree Search\n",
    "\n",
    "Now that we have a playable game that can also find valid moves, we should be able to implement the\n",
    "Monte Carlo Tree Search algorithm. The main references I'm using fo this are [this blog post](https://www.analyticsvidhya.com/blog/2019/01/monte-carlo-tree-search-introduction-algorithm-deepmind-alphago/) for explicit examples of how MCTS proceeds,\n",
    "the [mcstpy package](https://github.com/int8/monte-carlo-tree-search) from which I will liberally take code, and an AlphaZero \n",
    "explainer [here](https://jonathan-hui.medium.com/monte-carlo-tree-search-mcts-in-alphago-zero-8a403588276a). Also\n",
    "[this useful graphic](https://medium.com/applied-data-science/alphago-zero-explained-in-one-diagram-365f5abf67e0) and\n",
    "[this tutorial](https://web.stanford.edu/~surag/posts/alphazero.html)\n",
    "by a grad student.\n",
    "It appears that one of the main\n",
    "differences between the AlphaZero MCTS implementation and the standard implementation is that game state values\n",
    "are not obtained via rollout and are instead obtained directly from the neural net evaluator.\n",
    "We can test it out initially with a dummy move probability\n",
    "tensor generator and values.\n",
    "\n",
    "To program this, we will create a new `Node` class that is the base unit for the tree created during MCTS.\n",
    "This will not be functional programming, as I believe it makes more sense to keep nodes around and update\n",
    "the relevant values they hold versus a more functional approach. I'm open to hearing about how this is wrong,\n",
    "though!\n",
    "\n",
    "The node will know about its children, its parent, the probability that it was chosen by its parent\n",
    "initially, the predicted value of that position for whoever is playing it, and the raw probabilities \n",
    "for the next step. A node also needs to know how to backpropagate a value to its parent, as the\n",
    "nodes perceived value will change at every step. The trickiest part is thinking about\n",
    "what value to give to a node and what value to propagate. The neural network $f\\_\\theta$ always\n",
    "provides the value from the current player's perspective. Since the player of the child node\n",
    "is the opponent of its parent node, the child nodes value to the parent is the opposite of\n",
    "what the child perceives.\n",
    "\n",
    "But what does that mean for the backpropagation? A parent node chooses the node with\n",
    "the highest value (since the child values are given according to what the parent thinks).\n",
    "The child does the same thing, choosing what is a grandchild node to the original parent node.\n",
    "The grandchild will send the negative of its value to the child node because the child node needs\n",
    "to become less attractive to the original parent as the child chose a node to help itself, not\n",
    "its parent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 is_attacker,\n",
    "                 move_taken,\n",
    "                 parent,\n",
    "                 game_state,\n",
    "                 p,\n",
    "                 v,\n",
    "                 raw_child_p_tensor,\n",
    "                 game_over):\n",
    "        \n",
    "        self.__is_attacker = is_attacker\n",
    "        self.__move_taken = move_taken\n",
    "        self.__parent = parent\n",
    "        self.__children = []\n",
    "        self.__game_state = game_state\n",
    "        self.__p = p\n",
    "        self.__v = v\n",
    "        self.__w = float(v)\n",
    "        self.__q = torch.tensor(0, dtype=torch.float32)\n",
    "        self.__raw_child_p_tensor = raw_child_p_tensor\n",
    "        self.__game_over = game_over\n",
    "        self.__n = torch.tensor(0, dtype=torch.float32)\n",
    "    \n",
    "    @property\n",
    "    def is_attacker(self):\n",
    "        return self.__is_attacker\n",
    "    \n",
    "    @property\n",
    "    def move_taken(self):\n",
    "        return self.__move_taken\n",
    "    \n",
    "    @property\n",
    "    def parent(self):\n",
    "        return self.__parent\n",
    "    \n",
    "    @property\n",
    "    def children(self):\n",
    "        return self.__children\n",
    "    \n",
    "    @property\n",
    "    def game_state(self):\n",
    "        return self.__game_state\n",
    "    \n",
    "    @property\n",
    "    def board(self):\n",
    "        return self.__game_state[:4, :, :]\n",
    "    \n",
    "    @property\n",
    "    def p(self):\n",
    "        return self.__p\n",
    "    \n",
    "    @property\n",
    "    def v(self):\n",
    "        return self.__v\n",
    "    \n",
    "    @property\n",
    "    def w(self):\n",
    "        return self.__w\n",
    "    \n",
    "    @property\n",
    "    def n(self):\n",
    "        return self.__n\n",
    "    \n",
    "    @property\n",
    "    def raw_child_p_tensor(self):\n",
    "        return self.__raw_child_p_tensor\n",
    "    \n",
    "    @property\n",
    "    def q(self):\n",
    "        if self.n == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.w / self.n\n",
    "    \n",
    "    @property\n",
    "    def u(self):\n",
    "        # c_puct = 0.5 in this implementation\n",
    "        return self.q + 0.5 * self.p * (torch.sqrt(self.parent.n) / (1 + self.n))\n",
    "    \n",
    "    @property\n",
    "    def game_over(self):\n",
    "        return self.__game_over\n",
    "    \n",
    "    @property\n",
    "    def has_children(self):\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    def increment_n(self):\n",
    "        self.__n += 1\n",
    "        \n",
    "    def add_children(self, evaluator):\n",
    "        \"\"\" use the evaluator (the neural net) to\n",
    "        add child nodes for each possible move\"\"\"\n",
    "        \n",
    "        # find all posible valid moves and their raw probabilities\n",
    "        # then convert to new probabilities\n",
    "        move_tensor = valid_move_tensor(self.board)\n",
    "        \n",
    "        temp_p = move_tensor * self.raw_child_p_tensor\n",
    "        new_p = temp_p / temp_p.sum()\n",
    "        \n",
    "        for move_index in move_tensor.nonzero(as_tuple=False):\n",
    "            \n",
    "            new_game_state, end_game_value = game_state_move(self.game_state,\n",
    "                                                             move_index)\n",
    "            \n",
    "            node_p = new_p[move_index[0], move_index[1], move_index[2]]\n",
    "            \n",
    "            if new_game_state is not None:\n",
    "                node_child_p_tensor, node_v = evaluator(new_game_state)\n",
    "                new_node = Node(not self.is_attacker,\n",
    "                                move_index,\n",
    "                                self,\n",
    "                                new_game_state,\n",
    "                                node_p,\n",
    "                                -node_v,\n",
    "                                node_child_p_tensor,\n",
    "                                False)\n",
    "            else:\n",
    "                new_node = Node(not self.is_attacker,\n",
    "                                move_index,\n",
    "                                self,\n",
    "                                None,\n",
    "                                node_p,\n",
    "                                end_game_value,\n",
    "                                None,\n",
    "                                True)\n",
    "            \n",
    "            self.__children.append(new_node)\n",
    "            \n",
    "    def backpropagate(self, new_v):\n",
    "        \"\"\" add this to our v and to the parent v \"\"\"\n",
    "        self.__w += new_v\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(-new_v)\n",
    "            \n",
    "    def choose_favorite_child(self):\n",
    "        \"\"\" returns the child with the highest u value \"\"\"\n",
    "        try:\n",
    "            return max(self.children, key=lambda c: c.u)\n",
    "        except ValueError:\n",
    "            print(game_shadow(self.game_state))\n",
    "            print('is_attacker:', self.is_attacker)\n",
    "            raise\n",
    "    \n",
    "    def detach_parent(self):\n",
    "        \"\"\" set parent to None \"\"\"\n",
    "        self.__parent = None\n",
    "    \n",
    "    def get_child(self, move):\n",
    "        \"\"\" return child node based on the move taken \"\"\"\n",
    "        \n",
    "        for child in self.children:\n",
    "            if torch.all(torch.eq(child.move_taken, move)):\n",
    "                child.detach_parent()\n",
    "                return child"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the backpropagation gives the node the value given to it but then passes the opposite \n",
    "value to its own parent. In the code below we perform an iteration as we reasoned above: the\n",
    "chosen grandchild node gives its parent the negative of its own value and chain of\n",
    "backpropagations continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcts_iteration(root_node, evaluator):\n",
    "    \"\"\" run one iteration of the MCTS \"\"\"\n",
    "        \n",
    "    node = root_node\n",
    "        \n",
    "    keep_going = True\n",
    "    while keep_going:\n",
    "        \n",
    "        if node.n == 0 and node.parent is not None:\n",
    "            node.parent.backpropagate(-node.v)\n",
    "            node.increment_n()\n",
    "            keep_going = False\n",
    "        else:\n",
    "            if node.has_children:\n",
    "                node.increment_n()\n",
    "                node = node.choose_favorite_child()\n",
    "            else:\n",
    "                if node.game_over:\n",
    "                    node.parent.backpropagate(-node.v)\n",
    "                    node.increment_n()\n",
    "                    keep_going = False\n",
    "                else:\n",
    "                    node.add_children(evaluator)\n",
    "                    node.increment_n()\n",
    "                    node = node.choose_favorite_child()\n",
    "\n",
    "            \n",
    "def get_move_probabilities(root_node, evaluator, num_iterations):\n",
    "    \"\"\" run the mcts with a given number of iterations \n",
    "    \n",
    "    returns moves and their probabilities \"\"\"\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        mcts_iteration(root_node, evaluator)\n",
    "        \n",
    "    policy_tensor = torch.zeros(7, 7, 24, dtype=torch.float32)\n",
    "    \n",
    "    moves = [\n",
    "        child.move_taken\n",
    "        for child in root_node.children\n",
    "        if child.n > 0\n",
    "    ]\n",
    "    \n",
    "    visits = torch.tensor([\n",
    "        child.n\n",
    "        for child in root_node.children\n",
    "        if child.n > 0\n",
    "    ], dtype=torch.float32)\n",
    "        \n",
    "    return moves, visits / visits.sum()\n",
    "\n",
    "\n",
    "def convert_to_policy_tensor(moves, probs):\n",
    "    \"\"\" convert moves and probs lists into a policy tensor\"\"\"\n",
    "    \n",
    "    move_tensor = torch.zeros(24, 7, 7, dtype=torch.float32)\n",
    "    \n",
    "    for i in range(len(moves)):\n",
    "        move_tensor[tuple(moves[i])] = probs[i]\n",
    "        \n",
    "    return move_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now have the computer play a game against itself once we give it an\n",
    "evaluator. There's a lot we need to return after a game is over in order\n",
    "to evaluate the loss function, which we'll have it do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "\n",
    "def get_initial_node(evaulator):\n",
    "    \"\"\" get the initial node \"\"\"\n",
    "    \n",
    "    initial_state = get_initial_game_state()\n",
    "    p, v = evaulator(initial_state)\n",
    "    \n",
    "    return Node(is_attacker=True,\n",
    "                move_taken=None,\n",
    "                parent=None,\n",
    "                game_state=initial_state,\n",
    "                p=p,\n",
    "                v=-v,\n",
    "                raw_child_p_tensor=p,\n",
    "                game_over=False)\n",
    "\n",
    "\n",
    "def play_game(evaluator, num_iterations):\n",
    "    \"\"\" initialize and play one game. returns policy tensors,\n",
    "    predicted movement probabilities, properly signed game\n",
    "    results, and predicted game values. \"\"\"\n",
    "    \n",
    "    \n",
    "    predicted_movement_probs = []\n",
    "    predicted_game_values = []\n",
    "    policy_tensors = []\n",
    "    \n",
    "    node = get_initial_node(evaluator)\n",
    "    while not node.game_over:\n",
    "        \n",
    "        #if node.is_attacker:\n",
    "        #    print(\"\\nattack's turn\")\n",
    "        #else:\n",
    "        #    print(\"\\ndefense's turn\")\n",
    "        #print(game_shadow(node.game_state))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            moves, probs = get_move_probabilities(node,\n",
    "                                                  evaluator,\n",
    "                                                  num_iterations)\n",
    "            policy_tensors.append(convert_to_policy_tensor(moves, probs))\n",
    "        \n",
    "        # record values\n",
    "        predicted_movement_probs.append(node.raw_child_p_tensor)\n",
    "        predicted_game_values.append(-node.v)\n",
    "        \n",
    "        # make move\n",
    "        cat = Categorical(probs=probs)\n",
    "        move = moves[cat.sample()]\n",
    "        #print(move)\n",
    "        node = node.get_child(move)\n",
    "        node.detach_parent()\n",
    "    \n",
    "    # endgame node preserves who was the last player\n",
    "    if node.v == 0:\n",
    "        game_values = [0 for _ in range(len(predicted_game_values))]\n",
    "    elif node.is_attacker:\n",
    "        game_values = [\n",
    "            node.v * torch.pow(torch.tensor(-1, dtype=torch.float32), i % 2)\n",
    "            for i in range(len(predicted_game_values))\n",
    "        ]\n",
    "    else:\n",
    "        game_values = [\n",
    "            node.v * torch.pow(torch.tensor(-1, dtype=torch.float32), (i % 2) + 1)\n",
    "            for i in range(len(predicted_game_values))\n",
    "        ]\n",
    "    \n",
    "    return torch.cat(predicted_movement_probs), torch.cat(policy_tensors), \\\n",
    "        torch.cat(predicted_game_values), torch.tensor(game_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BrandubNet(nn.Module):\n",
    "\n",
    "    def __init__(self, random_state=47):\n",
    "        super(BrandubNet, self).__init__()\n",
    "        torch.manual_seed(random_state)\n",
    "        \n",
    "        # layers before heads\n",
    "        self.conv1 = nn.Conv2d(13, 5, 3, padding=1)  # 5 was arbitrary\n",
    "        self.bn1 = nn.BatchNorm2d(5)\n",
    "        self.conv2 = nn.Conv2d(5, 5, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(5)\n",
    "        self.conv3 = nn.Conv2d(5, 5, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(5)\n",
    "        self.conv4 = nn.Conv2d(5, 5, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(5)\n",
    "        \n",
    "        # policy head layers\n",
    "        self.conv_p = nn.Conv2d(5, 2, 1)  # 2 was arbitrary\n",
    "        self.bn_p = nn.BatchNorm2d(2)\n",
    "        self.fc_p = nn.Linear(2*7*7, 24*7*7)\n",
    "        self.max_p = nn.Softmax(1)\n",
    "        \n",
    "        # value head layers\n",
    "        self.conv_v = nn.Conv2d(5, 1, 1)  # 1 was arbitrary\n",
    "        self.bn_v = nn.BatchNorm2d(1)\n",
    "        self.fc1_v = nn.Linear(1*7*7, 25)  # 25 was arbitrary\n",
    "        self.fc2_v = nn.Linear(25, 1)\n",
    "\n",
    "    def forward(self, game_state):\n",
    "        \n",
    "        x = game_state.view(1, 13, 7, 7)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        \n",
    "        p = F.relu(self.bn_p(self.conv_p(x)))\n",
    "        p = p.view(1, 2*7*7)\n",
    "        p = F.relu(self.fc_p(p))\n",
    "        p = self.max_p(p)\n",
    "        p = p.view(24, 7, 7)\n",
    "        \n",
    "        v = F.relu(self.bn_v(self.conv_v(x)))\n",
    "        v = v.view(1, 1*7*7)\n",
    "        v = F.relu(self.fc1_v(v))\n",
    "        v = torch.tanh(self.fc2_v(v))\n",
    "        v = v.view(1)\n",
    "        \n",
    "        return p, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function for a game with $T$ iterations is\n",
    "$$\n",
    "c\\|\\theta\\|\\_2+ \\sum_{t=0}^T\\left[(z - v\\_t)^2 - \\pi\\_t^T \\log p\\_t\\right]\n",
    "$$\n",
    "where $z$ is the end result of the game (relative to that step) and $v_t$ and $p_t$ are the outputs of $f\\_\\theta$ \n",
    "for step $t$ and $\\pi_t$ is the policy. We do not have to add the $\\ell^2$ regularization of the parameters if we\n",
    "wish.\n",
    "\n",
    "The loss function is interesting to think about. In the classical setup of stochastic gradient descent the network\n",
    "would be optimized against just one term. This is akin to a small batch SGD where the batch is related by the\n",
    "fact that the batch comes from one game so the draws are not independent. It makes sense to keep them\n",
    "together then. Individual games will be more independent, but not fully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(AlphaLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, net, p, pi, v, z):\n",
    "        \"\"\" calculate loss from movement probabilities (p), MCTS policies (pi),\n",
    "        predicted game values (v) and actual game values (z). \"\"\"\n",
    "        \n",
    "        mse = torch.sum((z - v) ** 2)\n",
    "        cross_entropy = torch.sum(pi * p.log())\n",
    "        \n",
    "        theta = torch.cat([th.view(-1) for th in net.parameters()])\n",
    "        regularize = 0.01 * torch.norm(theta, p=2)\n",
    "        \n",
    "        return regularize + mse - cross_entropy\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to take a moment to call out the fact that my laptop has a GPU and I've got pytorch\n",
    "installed with CUDA 11.0 support. However... moving things to the GPU does not result in a\n",
    "raw speedup. I think that if I was able to somehow batch games properly that I could get\n",
    "a speedup, but I am a beginner for GPU stuff and will leave this for another time.\n",
    "I'll make a separate post later where I figure out how to do something simpler than\n",
    "reinforcement learning on GPU. I also do not have anything fancy, just a laptop with its\n",
    "GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "brandub_net = BrandubNet()\n",
    "game_state = get_initial_game_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "866 µs ± 6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = brandub_net(game_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "brandub_net = brandub_net.to('cuda')\n",
    "game_state = game_state.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.39 ms ± 20.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = brandub_net(game_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Finally, we can implement some learning! We'll use Adam as the optimizer. Initially I was\n",
    "trying to have the model update itself after every game that it played. This did not result in\n",
    "much improvement. This could also have been related to the fact that we were only giving it\n",
    "25 MCTS iterations. Upon further inspection, this meant that the model was only looking at most\n",
    "two moves ahead, which makes sense given that the branching factor (before symmetry) is probably\n",
    "decently large relative to that.\n",
    "\n",
    "After playing around some, I upped the MCTS iterations to 300 and decided to train in epochs.\n",
    "For the first two epochs here, I went with 250 games per epoch. Then I decreased it to 100 games\n",
    "per epoch. There's no great reason behind this, I just did not want to throw away the training\n",
    "I had done. The first epoch was trained as you see below. If you look at old commits you'll\n",
    "see "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from time import perf_counter\n",
    "import os\n",
    "\n",
    "checkpoint_dir = 'saved_models/'\n",
    "base_file_name = 'checkpoint_'\n",
    "\n",
    "if os.path.exists(checkpoint_dir + base_file_name + '0'):\n",
    "    \n",
    "    checkpoint_files = [\n",
    "        f for f in os.listdir(checkpoint_dir) if f[:11] == base_file_name\n",
    "    ]\n",
    "    checkpoint_file = max(checkpoint_files, key=lambda x: int(x[11:]))\n",
    "    checkpoint = torch.load(checkpoint_dir + checkpoint_file)\n",
    "    \n",
    "    brandub_net = BrandubNet()\n",
    "    optimizer = Adam(brandub_net.parameters())\n",
    "    \n",
    "    brandub_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch_num = checkpoint['epochs']\n",
    "    losses = checkpoint['losses']\n",
    "    elapsed_time = checkpoint['elapsed_time']\n",
    "    \n",
    "    alpha_loss = AlphaLoss()\n",
    "    \n",
    "    print_early_info = True\n",
    "\n",
    "else:\n",
    "    \n",
    "    brandub_net = BrandubNet()\n",
    "    optimizer = Adam(brandub_net.parameters())\n",
    "    alpha_loss = AlphaLoss()\n",
    "    losses = []\n",
    "    epoch_num = 0\n",
    "    elapsed_time = 0.0\n",
    "    \n",
    "    print_early_info = False\n",
    "    \n",
    "    checkpoint = {\n",
    "        'model_state_dict': brandub_net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'losses': losses,\n",
    "        'epochs': epoch_num,\n",
    "        'elapsed_time': elapsed_time\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, checkpoint_dir + base_file_name + str(epoch_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 --- avg loss per game: 349.0643310546875\n",
      "20:16 --- pid: 1 --- game: 1 / 125\n",
      "20:16 --- pid: 2 --- game: 1 / 125\n",
      "20:20 --- pid: 2 --- game: 2 / 125 --- avg_time: 3.31 --- avg loss: 52.7751\n",
      "20:29 --- pid: 2 --- game: 3 / 125 --- avg_time: 6.31 --- avg loss: 114.6027\n",
      "20:32 --- pid: 1 --- game: 2 / 125 --- avg_time: 15.59 --- avg loss: 172.2403\n",
      "20:40 --- pid: 1 --- game: 3 / 125 --- avg_time: 11.74 --- avg loss: 172.8431\n",
      "20:40 --- pid: 2 --- game: 4 / 125 --- avg_time: 7.88 --- avg loss: 154.6968\n",
      "20:50 --- pid: 2 --- game: 5 / 125 --- avg_time: 8.49 --- avg loss: 171.5619\n",
      "20:57 --- pid: 1 --- game: 4 / 125 --- avg_time: 13.57 --- avg loss: 221.6246\n",
      "21:16 --- pid: 2 --- game: 6 / 125 --- avg_time: 11.95 --- avg loss: 237.1873\n",
      "21:28 --- pid: 1 --- game: 5 / 125 --- avg_time: 17.77 --- avg loss: 335.8085\n",
      "21:34 --- pid: 1 --- game: 6 / 125 --- avg_time: 15.47 --- avg loss: 303.5883\n",
      "21:40 --- pid: 2 --- game: 7 / 125 --- avg_time: 13.99 --- avg loss: 281.2534\n",
      "21:45 --- pid: 1 --- game: 7 / 125 --- avg_time: 14.79 --- avg loss: 293.4268\n",
      "21:53 --- pid: 2 --- game: 8 / 125 --- avg_time: 13.74 --- avg loss: 276.4807\n",
      "22:10 --- pid: 1 --- game: 8 / 125 --- avg_time: 16.22 --- avg loss: 330.7153\n",
      "22:14 --- pid: 1 --- game: 9 / 125 --- avg_time: 14.70 --- avg loss: 308.0896\n",
      "22:26 --- pid: 1 --- game: 10 / 125 --- avg_time: 14.41 --- avg loss: 308.9632\n",
      "22:29 --- pid: 2 --- game: 9 / 125 --- avg_time: 16.55 --- avg loss: 352.8289\n",
      "22:36 --- pid: 1 --- game: 11 / 125 --- avg_time: 13.90 --- avg loss: 305.4751\n",
      "22:39 --- pid: 2 --- game: 10 / 125 --- avg_time: 15.81 --- avg loss: 346.4070\n",
      "22:48 --- pid: 1 --- game: 12 / 125 --- avg_time: 13.78 --- avg loss: 301.1979\n",
      "23:03 --- pid: 1 --- game: 13 / 125 --- avg_time: 13.90 --- avg loss: 308.9784\n",
      "23:09 --- pid: 2 --- game: 11 / 125 --- avg_time: 17.21 --- avg loss: 390.8699\n",
      "23:22 --- pid: 1 --- game: 14 / 125 --- avg_time: 14.25 --- avg loss: 315.4473\n",
      "23:35 --- pid: 1 --- game: 15 / 125 --- avg_time: 14.19 --- avg loss: 316.5420\n",
      "23:45 --- pid: 2 --- game: 12 / 125 --- avg_time: 18.92 --- avg loss: 434.4679\n",
      "23:55 --- pid: 1 --- game: 16 / 125 --- avg_time: 14.55 --- avg loss: 323.6783\n",
      "00:02 --- pid: 2 --- game: 13 / 125 --- avg_time: 18.79 --- avg loss: 431.9525\n",
      "00:04 --- pid: 1 --- game: 17 / 125 --- avg_time: 14.21 --- avg loss: 321.5758\n",
      "00:17 --- pid: 1 --- game: 18 / 125 --- avg_time: 14.15 --- avg loss: 325.9190\n",
      "00:19 --- pid: 2 --- game: 14 / 125 --- avg_time: 18.63 --- avg loss: 429.6614\n",
      "00:24 --- pid: 1 --- game: 19 / 125 --- avg_time: 13.75 --- avg loss: 317.0816\n",
      "00:36 --- pid: 2 --- game: 15 / 125 --- avg_time: 18.54 --- avg loss: 425.4730\n",
      "00:45 --- pid: 2 --- game: 16 / 125 --- avg_time: 17.87 --- avg loss: 414.8362\n",
      "00:49 --- pid: 1 --- game: 20 / 125 --- avg_time: 14.34 --- avg loss: 326.7967\n",
      "00:54 --- pid: 2 --- game: 17 / 125 --- avg_time: 17.36 --- avg loss: 403.6321\n",
      "00:56 --- pid: 1 --- game: 21 / 125 --- avg_time: 13.96 --- avg loss: 318.0434\n",
      "01:09 --- pid: 2 --- game: 18 / 125 --- avg_time: 17.21 --- avg loss: 400.6870\n",
      "01:15 --- pid: 1 --- game: 22 / 125 --- avg_time: 14.22 --- avg loss: 318.1924\n",
      "01:18 --- pid: 2 --- game: 19 / 125 --- avg_time: 16.78 --- avg loss: 391.2938\n",
      "01:26 --- pid: 2 --- game: 20 / 125 --- avg_time: 16.29 --- avg loss: 380.5463\n",
      "01:35 --- pid: 1 --- game: 23 / 125 --- avg_time: 14.47 --- avg loss: 323.8633\n",
      "01:47 --- pid: 2 --- game: 21 / 125 --- avg_time: 16.55 --- avg loss: 384.4560\n",
      "01:53 --- pid: 2 --- game: 22 / 125 --- avg_time: 16.04 --- avg loss: 374.9216\n",
      "01:57 --- pid: 1 --- game: 24 / 125 --- avg_time: 14.80 --- avg loss: 330.5011\n",
      "02:08 --- pid: 2 --- game: 23 / 125 --- avg_time: 15.97 --- avg loss: 376.8034\n",
      "02:21 --- pid: 1 --- game: 25 / 125 --- avg_time: 15.18 --- avg loss: 348.8464\n",
      "02:29 --- pid: 2 --- game: 24 / 125 --- avg_time: 16.19 --- avg loss: 382.9659\n",
      "02:31 --- pid: 1 --- game: 26 / 125 --- avg_time: 14.98 --- avg loss: 343.4647\n",
      "02:41 --- pid: 2 --- game: 25 / 125 --- avg_time: 16.01 --- avg loss: 377.8667\n",
      "02:42 --- pid: 1 --- game: 27 / 125 --- avg_time: 14.81 --- avg loss: 342.6530\n",
      "02:59 --- pid: 2 --- game: 26 / 125 --- avg_time: 16.11 --- avg loss: 375.5650\n",
      "03:02 --- pid: 1 --- game: 28 / 125 --- avg_time: 15.01 --- avg loss: 348.1844\n",
      "03:15 --- pid: 1 --- game: 29 / 125 --- avg_time: 14.96 --- avg loss: 348.3805\n",
      "03:16 --- pid: 2 --- game: 27 / 125 --- avg_time: 16.12 --- avg loss: 374.1515\n",
      "03:36 --- pid: 1 --- game: 30 / 125 --- avg_time: 15.14 --- avg loss: 350.6423\n",
      "03:40 --- pid: 2 --- game: 28 / 125 --- avg_time: 16.41 --- avg loss: 378.9492\n",
      "03:47 --- pid: 2 --- game: 29 / 125 --- avg_time: 16.08 --- avg loss: 371.3040\n",
      "03:49 --- pid: 1 --- game: 31 / 125 --- avg_time: 15.07 --- avg loss: 346.8092\n",
      "03:59 --- pid: 1 --- game: 32 / 125 --- avg_time: 14.91 --- avg loss: 343.0004\n",
      "04:04 --- pid: 1 --- game: 33 / 125 --- avg_time: 14.61 --- avg loss: 336.9604\n",
      "04:06 --- pid: 2 --- game: 30 / 125 --- avg_time: 16.21 --- avg loss: 372.8452\n",
      "04:12 --- pid: 1 --- game: 34 / 125 --- avg_time: 14.42 --- avg loss: 333.1583\n",
      "04:14 --- pid: 2 --- game: 31 / 125 --- avg_time: 15.93 --- avg loss: 367.9951\n",
      "04:21 --- pid: 1 --- game: 35 / 125 --- avg_time: 14.26 --- avg loss: 330.0686\n",
      "04:22 --- pid: 1 --- game: 36 / 125 --- avg_time: 13.87 --- avg loss: 321.8100\n",
      "04:43 --- pid: 1 --- game: 37 / 125 --- avg_time: 14.08 --- avg loss: 325.2449\n",
      "04:47 --- pid: 2 --- game: 32 / 125 --- avg_time: 16.46 --- avg loss: 387.8845\n",
      "04:57 --- pid: 1 --- game: 38 / 125 --- avg_time: 14.07 --- avg loss: 322.8221\n",
      "05:00 --- pid: 2 --- game: 33 / 125 --- avg_time: 16.36 --- avg loss: 385.8065\n",
      "05:07 --- pid: 1 --- game: 39 / 125 --- avg_time: 13.97 --- avg loss: 321.5697\n",
      "05:09 --- pid: 2 --- game: 34 / 125 --- avg_time: 16.15 --- avg loss: 379.6779\n",
      "05:18 --- pid: 1 --- game: 40 / 125 --- avg_time: 13.88 --- avg loss: 319.8236\n",
      "05:18 --- pid: 2 --- game: 35 / 125 --- avg_time: 15.92 --- avg loss: 373.6869\n",
      "05:31 --- pid: 1 --- game: 41 / 125 --- avg_time: 13.86 --- avg loss: 318.2578\n",
      "05:37 --- pid: 1 --- game: 42 / 125 --- avg_time: 13.67 --- avg loss: 313.5865\n",
      "05:37 --- pid: 2 --- game: 36 / 125 --- avg_time: 16.02 --- avg loss: 376.2166\n",
      "05:44 --- pid: 2 --- game: 37 / 125 --- avg_time: 15.78 --- avg loss: 371.5294\n",
      "06:04 --- pid: 2 --- game: 38 / 125 --- avg_time: 15.87 --- avg loss: 374.3918\n",
      "06:10 --- pid: 2 --- game: 39 / 125 --- avg_time: 15.61 --- avg loss: 369.1877\n",
      "06:27 --- pid: 2 --- game: 40 / 125 --- avg_time: 15.66 --- avg loss: 370.7535\n",
      "06:30 --- pid: 1 --- game: 43 / 125 --- avg_time: 14.61 --- avg loss: 339.9748\n",
      "06:36 --- pid: 1 --- game: 44 / 125 --- avg_time: 14.40 --- avg loss: 335.4790\n",
      "06:36 --- pid: 2 --- game: 41 / 125 --- avg_time: 15.49 --- avg loss: 367.4366\n",
      "06:49 --- pid: 1 --- game: 45 / 125 --- avg_time: 14.37 --- avg loss: 336.2757\n",
      "06:49 --- pid: 2 --- game: 42 / 125 --- avg_time: 15.42 --- avg loss: 367.4170\n",
      "06:56 --- pid: 2 --- game: 43 / 125 --- avg_time: 15.22 --- avg loss: 362.9641\n",
      "07:04 --- pid: 2 --- game: 44 / 125 --- avg_time: 15.06 --- avg loss: 358.9454\n",
      "07:10 --- pid: 1 --- game: 46 / 125 --- avg_time: 14.52 --- avg loss: 338.4522\n",
      "07:14 --- pid: 1 --- game: 47 / 125 --- avg_time: 14.30 --- avg loss: 333.8332\n",
      "07:18 --- pid: 2 --- game: 45 / 125 --- avg_time: 15.02 --- avg loss: 358.0492\n",
      "07:23 --- pid: 2 --- game: 46 / 125 --- avg_time: 14.82 --- avg loss: 353.7319\n",
      "07:27 --- pid: 1 --- game: 48 / 125 --- avg_time: 14.27 --- avg loss: 332.5710\n",
      "07:33 --- pid: 1 --- game: 49 / 125 --- avg_time: 14.09 --- avg loss: 328.3919\n",
      "07:37 --- pid: 2 --- game: 47 / 125 --- avg_time: 14.80 --- avg loss: 352.6149\n",
      "07:45 --- pid: 1 --- game: 50 / 125 --- avg_time: 14.04 --- avg loss: 328.8673\n",
      "07:50 --- pid: 2 --- game: 48 / 125 --- avg_time: 14.76 --- avg loss: 353.2267\n",
      "07:57 --- pid: 1 --- game: 51 / 125 --- avg_time: 14.01 --- avg loss: 328.9933\n",
      "08:09 --- pid: 1 --- game: 52 / 125 --- avg_time: 13.96 --- avg loss: 327.5708\n",
      "08:10 --- pid: 2 --- game: 49 / 125 --- avg_time: 14.86 --- avg loss: 356.7935\n",
      "08:19 --- pid: 2 --- game: 50 / 125 --- avg_time: 14.75 --- avg loss: 354.1276\n",
      "08:20 --- pid: 1 --- game: 53 / 125 --- avg_time: 13.92 --- avg loss: 326.8196\n",
      "08:33 --- pid: 2 --- game: 51 / 125 --- avg_time: 14.73 --- avg loss: 353.7457\n",
      "08:37 --- pid: 1 --- game: 54 / 125 --- avg_time: 13.97 --- avg loss: 326.4211\n",
      "08:41 --- pid: 2 --- game: 52 / 125 --- avg_time: 14.60 --- avg loss: 350.5293\n",
      "08:46 --- pid: 1 --- game: 55 / 125 --- avg_time: 13.88 --- avg loss: 324.2098\n",
      "08:50 --- pid: 2 --- game: 53 / 125 --- avg_time: 14.49 --- avg loss: 347.7444\n",
      "08:56 --- pid: 2 --- game: 54 / 125 --- avg_time: 14.32 --- avg loss: 344.4232\n",
      "08:58 --- pid: 1 --- game: 56 / 125 --- avg_time: 13.84 --- avg loss: 323.5217\n",
      "09:20 --- pid: 2 --- game: 55 / 125 --- avg_time: 14.50 --- avg loss: 346.3737\n",
      "09:24 --- pid: 1 --- game: 57 / 125 --- avg_time: 14.06 --- avg loss: 327.6847\n",
      "09:33 --- pid: 2 --- game: 56 / 125 --- avg_time: 14.48 --- avg loss: 345.6717\n",
      "09:57 --- pid: 2 --- game: 57 / 125 --- avg_time: 14.66 --- avg loss: 351.1083\n",
      "09:58 --- pid: 1 --- game: 58 / 125 --- avg_time: 14.41 --- avg loss: 334.9673\n",
      "10:03 --- pid: 2 --- game: 58 / 125 --- avg_time: 14.50 --- avg loss: 347.9984\n",
      "10:07 --- pid: 1 --- game: 59 / 125 --- avg_time: 14.32 --- avg loss: 333.3130\n"
     ]
    }
   ],
   "source": [
    "import torch.multiprocessing as mp\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "if print_early_info:\n",
    "    for i in range(epoch_num):\n",
    "        msg = 'epoch: {} --- avg loss per game: {}'.format(i+1, losses[i])\n",
    "        print(msg)\n",
    "        \n",
    "num_games = 250\n",
    "num_mcts_iters = 300\n",
    "num_epochs = 2\n",
    "\n",
    "def run_half_epoch(queue, process_num, num_games, seed):\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    process_losses = torch.zeros(int(num_games), dtype=torch.float32)\n",
    "    process_timings = []\n",
    "    \n",
    "    for game in range(num_games):\n",
    "        \n",
    "        now = datetime.now().strftime('%H:%M')\n",
    "        msg = now + ' --- pid: {} --- game: {} / {}'.format(process_num, game+1, num_games)\n",
    "        \n",
    "        if game > 0:\n",
    "            msg += ' --- avg_time: {:.2f}'.format(torch.mean(torch.tensor(process_timings, dtype=torch.float32)))\n",
    "            msg += ' --- avg loss: {:.4f}'.format(torch.mean(process_losses[:game+1]))\n",
    "            \n",
    "        print(msg)\n",
    "        \n",
    "        game_start = perf_counter()\n",
    "        p, pi, v, z = play_game(brandub_net.forward, num_mcts_iters)\n",
    "        process_losses[game] = alpha_loss.forward(brandub_net, p, pi, v, z)\n",
    "        game_end = perf_counter()\n",
    "        \n",
    "        process_timings.append((game_end - game_start) / 60)\n",
    "        \n",
    "    queue.put(torch.sum(process_losses))\n",
    "    \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    epoch_num += 1\n",
    "    start = perf_counter()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    queue = mp.Queue()\n",
    "    \n",
    "    process_num_games = int(num_games/2)\n",
    "    processes = [\n",
    "        mp.Process(target=run_half_epoch,\n",
    "                   args=(queue,\n",
    "                         1,\n",
    "                         process_num_games,\n",
    "                         torch.randint(1000000, (1,)))\n",
    "                    ),\n",
    "        mp.Process(target=run_half_epoch,\n",
    "                   args=(queue,\n",
    "                         2,\n",
    "                         process_num_games,\n",
    "                         torch.randint(1000000, (1,)))\n",
    "                    )\n",
    "    ]\n",
    "    \n",
    "    for p in processes:\n",
    "        p.start()\n",
    "        \n",
    "    for p in processes:\n",
    "        p.join()\n",
    "        \n",
    "    epoch_loss = torch.cat([queue.get() for _ in processes]).sum()\n",
    "    epoch_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    end = perf_counter()\n",
    "    elapsed_time += (end - start) / 60.0\n",
    "        \n",
    "    losses.append(float(epoch_loss / num_games))\n",
    "    \n",
    "    print('epoch:',\n",
    "          epoch_num,\n",
    "          '-- avg loss per game:',\n",
    "          losses[-1])\n",
    "\n",
    "    checkpoint = {\n",
    "        'model_state_dict': brandub_net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'losses': losses,\n",
    "        'epochs': epoch_num,\n",
    "        'elapsed_time': elapsed_time\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, checkpoint_dir + base_file_name + str(epoch_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something about the processes is slowing us down. At worst I'd expect everything to take\n",
    "twice as long, no? Not sure why this does not work. Either way, it seems that it takes\n",
    "about 50 games to about get the average loss right. Perhaps I will move this\n",
    "to 100 games per epoch from the 250 I'm doing now. Maybe there is enough overhead with\n",
    "the notebook itself that maxing out the CPU is actually inviting interference from\n",
    "other processes. Either way, I don't want to kill this even though it won't finish until\n",
    "this evening. Or maybe the direction we moved in just encouraged much longer games.\n",
    "That's not a great sign if so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gitpage",
   "language": "python",
   "name": "gitpage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
