{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: code-post\n",
    "title: Pedicting rooting interests on reddit\n",
    "description: Going to see if we can predict neutral fan rooting interests from reddit posts.\n",
    "tags: [neural nets]\n",
    "---\n",
    "\n",
    "In this notebook / post I'm going to try to see if we can predict who a\n",
    "supposedly neutral fan is rooting for in the (as of this writing) about to\n",
    "end Lakers-Heat NBA finals. To do this, I'm going to attempt to scrape posts\n",
    "from game threads by Heat and Lakers fans to train an LSTM based model. As long\n",
    "as I can scrape the flair from from the reddit API, this should be doable. If\n",
    "not, then I will have to scrape from the team specific subreddits and try that\n",
    "way.\n",
    "\n",
    "Outline:\n",
    "- Accessing Reddit\n",
    "  - Reddit's API\n",
    "  - Getting posts\n",
    "- Making a dataset\n",
    "  - Encoding posts\n",
    "  - Saving the data\n",
    "- Training a model\n",
    "- Test set predictions\n",
    "\n",
    "## Accessing reddit\n",
    "\n",
    "If you don't know what [reddit](https://reddit.com/) is, it's a website that is\n",
    "organized into communities called _subreddits_. The site has _users_ which belong to\n",
    "multiple subreddits. Each subreddit contains a sequence of _submissions_ (also called\n",
    "_posts_) which can be links\n",
    "to other sites, images, or text. Each post contains _comments_ which are text only\n",
    "an are made by the users. The _comment section_ is organized as a forest of trees\n",
    "wich top level comments and then comments nested below each top level comment. Each\n",
    "user can have a _flair_ which varies with the subreddit they are posting in. The\n",
    "flair and username are posted along with each comment. The flair will contains a\n",
    "small image as well as text. Not every user has a flair associated to it for every\n",
    "subreddit to which they belong. It is not always required to be a member of the\n",
    "subreddit in order to post, although this varies by community.\n",
    "\n",
    "We will be using the NBA subreddit which as of this writing has ~3.5 million users.\n",
    "Users in this subreddit have flairs which denote which team the user is a supporter\n",
    "of. Mine is for the Cleveland Cavaliers.\n",
    "\n",
    "### PRAW - Exploring Reddit\n",
    "\n",
    "In order to access reddit, we need to have a reddit account and then create an application\n",
    "with that account as a developer of the application. Go to [the apps page](https://www.reddit.com/prefs/apps)\n",
    "to create an application. (Please read the terms and conditions!)\n",
    "You need to store the name of the app as the `user_agent`, the\n",
    "`client_id` is the 14 character string that is below the app name once it's created, \n",
    "you'll have a 27 character secret secret that is generated and is the `client_secret`, \n",
    "and you'll need your `username`. I have put these credentials in an encrypted YAML file\n",
    "that I created using ansible-vault.\n",
    "\n",
    "We will be using [PRAW](https://github.com/praw-dev/praw), the Python Reddit API\n",
    "Wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import yaml\n",
    "from ansible_vault import Vault\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ····················\n"
     ]
    }
   ],
   "source": [
    "vault = Vault(getpass())\n",
    "with open('redditcreds.yml', 'r') as f:\n",
    "    reddit_creds = vault.load(f.read())\n",
    "\n",
    "reddit = praw.Reddit(username=reddit_creds['username'],\n",
    "                     user_agent=reddit_creds['user_agent'],\n",
    "                     client_id=reddit_creds['client_id'],\n",
    "                     client_secret=reddit_creds['client_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get into the [NBA subreddit](https://www.reddit.com/r/nba) and search for posts which are _game threads_\n",
    "to which users will post while a game is ongoing. These are posted automatically and so they have a \n",
    "predictible title format which makes searching for them and then filtering the received submisisons\n",
    "by title easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit('nba')\n",
    "\n",
    "submissions = subreddit.search(query='title:\"GAME THREAD\"',\n",
    "                               time_filter=\"month\")\n",
    "game_threads = [\n",
    "    s for s in submissions \n",
    "    if\n",
    "        s.title[:11] == 'GAME THREAD' \n",
    "        and (\"Lakers\" in s.title or \"Heat\" in s.title)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAME THREAD: Miami Heat (44-29) @ Los Angeles Lakers (52-19) - (October 09, 2020)\n",
      "GAME THREAD: Miami Heat (44-29) @ Los Angeles Lakers (52-19) - (September 30, 2020)\n",
      "GAME THREAD: Los Angeles Lakers (52-19) @ Miami Heat (44-29) - (October 06, 2020)\n",
      "GAME THREAD: Los Angeles Lakers (52-19) @ Miami Heat (44-29) - (October 04, 2020)\n",
      "GAME THREAD: Miami Heat (44-29) @ Los Angeles Lakers (52-19) - (October 02, 2020)\n",
      "GAME THREAD: Denver Nuggets (46-27) @ Los Angeles Lakers (52-19) - (September 26, 2020)\n",
      "GAME THREAD: Los Angeles Lakers (52-19) @ Denver Nuggets (46-27) - (September 24, 2020)\n",
      "GAME THREAD: Los Angeles Lakers (52-19) @ Denver Nuggets (46-27) - (September 22, 2020)\n",
      "GAME THREAD: Denver Nuggets (46-27) @ Los Angeles Lakers (52-19) - (September 18, 2020)\n",
      "GAME THREAD: Boston Celtics (48-24) @ Miami Heat (44-29) - (September 27, 2020)\n",
      "GAME THREAD: Denver Nuggets (46-27) @ Los Angeles Lakers (52-19) - (September 20, 2020)\n",
      "GAME THREAD: Miami Heat (44-29) @ Boston Celtics (48-24) - (September 25, 2020)\n",
      "GAME THREAD: Boston Celtics (48-24) @ Miami Heat (44-29) - (September 23, 2020)\n",
      "GAME THREAD: Miami Heat (44-29) @ Boston Celtics (48-24) - (September 15, 2020)\n",
      "GAME THREAD: Miami Heat (44-29) @ Boston Celtics (48-24) - (September 17, 2020)\n",
      "GAME THREAD: Boston Celtics (48-24) @ Miami Heat (44-29) - (September 19, 2020)\n",
      "GAME THREAD: Houston Rockets (44-28) @ Los Angeles Lakers (52-19) - (September 12, 2020)\n",
      "GAME THREAD: Los Angeles Lakers (52-19) @ Houston Rockets (44-28) - (September 10, 2020)\n"
     ]
    }
   ],
   "source": [
    "for t in game_threads:\n",
    "    print(t.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comments for a submission are contained in PRAW `CommentForest`. If we do not\n",
    "care about the structure, we can flatten this to a list. Note that the list\n",
    "will contain both `Comment` objects as well as `MoreComments` objectcs. It is possible to replace\n",
    "the `MoreComments` objects using the `replace_more` function, but each replacement\n",
    "requires calling the reddit API. By default, 32 of the `MoreComments` objects will\n",
    "be replaced, which I will keep but I will also limit to those which contain at least\n",
    "5 more comments in them. Doing this removes all `MoreComments` instances from\n",
    "the list of comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAME THREAD: Miami Heat (44-29) @ Los Angeles Lakers (52-19) - (October 09, 2020)\n",
      "number of comments: 28455\n"
     ]
    }
   ],
   "source": [
    "game_thread = game_threads[0]\n",
    "print(game_thread.title)\n",
    "print(\"number of comments:\", game_thread.num_comments)\n",
    "game_thread.comments.replace_more(limit=32, threshold=5)\n",
    "comments = game_thread.comments.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is a random comment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment author:    StoneColdAM\n",
      "comment score:     28 (28-0)\n",
      "author flair text: Lakers\n",
      "body:\n",
      "\n",
      " Dwight Howard strategy: get Jimmy Butler ejected.\n"
     ]
    }
   ],
   "source": [
    "c = comments[10]\n",
    "print('comment author:   ', c.author.name)\n",
    "print('comment score:    ', c.score, '({}-{})'.format(c.ups, c.downs))\n",
    "print('author flair text:', c.author_flair_text)\n",
    "print('body:\\n\\n', c.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training set, the goal will be to find authors which are flaired as Lakers or Heat or have\n",
    "flair text that contains LAL or MIA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of comments by Lakers users: 1042\n",
      "number of comments by Heat users  : 275\n"
     ]
    }
   ],
   "source": [
    "lakers_comments = [\n",
    "    c for c in comments\n",
    "    if\n",
    "        c.author_flair_text is not None\n",
    "        and (\n",
    "            c.author_flair_text == 'Lakers'\n",
    "            or '[LAL]' in c.author_flair_text\n",
    "        )\n",
    "]\n",
    "print('number of comments by Lakers users:', len(lakers_comments))\n",
    "\n",
    "heat_comments = [\n",
    "    c for c in comments\n",
    "    if\n",
    "        c.author_flair_text is not None\n",
    "        and (\n",
    "            c.author_flair_text == 'Heat'\n",
    "            or '[MIA]' in c.author_flair_text\n",
    "        )\n",
    "]\n",
    "print('number of comments by Heat users  :', len(heat_comments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pushshift - Gathering data\n",
    "\n",
    "It turns out that PRAW is fairly limited and the [Pushshift API](https://github.com/pushshift/api) is\n",
    "more powerful than it. There is a wrapper for this API called [PSAW](https://github.com/dmarx/psaw).\n",
    "PSAW and PRAW interact with each other nicely: if you pass a PRAW `Reddit` instance to PSAW's\n",
    "`PushShiftAPI` class, pushshift gathers the IDs you want but then returns PRAW objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "import datetime as dt\n",
    "\n",
    "api = PushshiftAPI(reddit)\n",
    "\n",
    "# we'll search month by month for submissions\n",
    "game_threads = []\n",
    "\n",
    "months = [\n",
    "    int(dt.datetime(2019, 10, 19).timestamp()),\n",
    "    int(dt.datetime(2019, 11, 1).timestamp()),\n",
    "    int(dt.datetime(2019, 12, 1).timestamp()),\n",
    "    int(dt.datetime(2020, 1, 1).timestamp()),\n",
    "    int(dt.datetime(2020, 2, 1).timestamp()),\n",
    "    int(dt.datetime(2020, 3, 1).timestamp()),\n",
    "    int(dt.datetime(2020, 4, 1).timestamp()),\n",
    "    int(dt.datetime(2020, 5, 1).timestamp()),\n",
    "    int(dt.datetime(2020, 6, 1).timestamp()),\n",
    "    int(dt.datetime(2020, 7, 1).timestamp()),\n",
    "    int(dt.datetime(2020, 8, 1).timestamp()),\n",
    "    int(dt.datetime(2020, 9, 1).timestamp()),\n",
    "    int(dt.datetime(2020, 10, 1).timestamp()),\n",
    "    int(dt.datetime(2020, 11, 1).timestamp())\n",
    "]\n",
    "\n",
    "for i in range(len(months)-1):\n",
    "    submissions = list(api.search_submissions(after=months[i],\n",
    "                                              before=months[i+1],\n",
    "                                              q='game thread',\n",
    "                                              subreddit='nba',\n",
    "                                              author='NBA_MOD',\n",
    "                                              limit=5000))\n",
    "    \n",
    "    game_threads += [\n",
    "        s for s in submissions\n",
    "        if\n",
    "            s.title[:11] == 'GAME THREAD'\n",
    "            and (\"Lakers\" in s.title or \"Heat\" in s.title)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have game threads, it's time to collect relevant comments from each game.\n",
    "We'll be using PRAW for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scouring game thread: 180/180\n",
      "number of lakers comments: 47535\n",
      "  number of heat comments: 12907\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "lakers_comments = []\n",
    "heat_comments = []\n",
    "\n",
    "num_game_threads = len(game_threads)\n",
    "i = 1\n",
    "\n",
    "def check_lakers(comment):\n",
    "    if comment.author_flair_text is not None:\n",
    "        if comment.author_flair_text == 'Lakers':\n",
    "            return True\n",
    "        if '[LAL]' in comment.author_flair_text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def check_heat(comment):\n",
    "    if comment.author_flair_text is not None:\n",
    "        if comment.author_flair_text == 'Heat':\n",
    "            return True\n",
    "        if '[MIA]' in comment.author_flair_text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "for game_thread in game_threads:\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print('scouring game thread: {}/{}'.format(i, num_game_threads))\n",
    "    print('number of lakers comments:', len(lakers_comments))\n",
    "    print('  number of heat comments:', len(heat_comments))\n",
    "    \n",
    "    has_lakers = 'Lakers' in game_thread.title\n",
    "    has_heat = 'Heat' in game_thread.title\n",
    "    \n",
    "    # NOTE: this part takes a long time\n",
    "    game_thread.comments.replace_more(limit=32, threshold=2)\n",
    "    comments = game_thread.comments.list()\n",
    "    \n",
    "    if has_lakers and has_heat:\n",
    "        lakers = []\n",
    "        heat = []\n",
    "        \n",
    "        for c in comments:\n",
    "            if check_lakers(c) and len(c.body.split()) > 3:\n",
    "                lakers += [c]\n",
    "            elif check_heat(c) and len(c.body.split()) > 3:\n",
    "                heat += [c]\n",
    "                \n",
    "        lakers_comments += lakers\n",
    "        heat_comments += heat\n",
    "        \n",
    "    elif has_lakers:\n",
    "        lakers =[]\n",
    "        \n",
    "        for c in comments:\n",
    "            if check_lakers(c) and len(c.body.split()) > 3:\n",
    "                lakers += [c]\n",
    "                \n",
    "        lakers_comments += lakers\n",
    "      \n",
    "    elif has_heat:\n",
    "        heat = []\n",
    "        \n",
    "        for c in comments:\n",
    "            if check_heat(c) and len(c.body.split()) > 3:\n",
    "                heat += [c]\n",
    "                \n",
    "        heat_comments += heat\n",
    "        \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/lakers_comments.pkl', 'wb') as f:\n",
    "    pickle.dump(lakers_comments, f)\n",
    "    \n",
    "with open('data/heat_comments.pkl', 'wb') as f:\n",
    "    pickle.dump(heat_comments, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perparing to clean\n",
    "\n",
    "Cleaning text is tedious and many people have already though about what to do. \n",
    "I am taking code from [this blog post](https://hub.packtpub.com/clean-social-media-data-analysis-python/).\n",
    "The cleaning we're doing is just about replacing weird and unwanted characters and then\n",
    "lemmatizing.\n",
    "\n",
    "I have played around with the stop words a bit. Since this is really a form of\n",
    "sentiment analysis, I put back in the negations. Since this is basketball, I also\n",
    "want numbers such as two and three to be allowed. I want some ofthe directions like\n",
    "up and down and over and under.\n",
    "\n",
    "Since I'm going to use spaCy's vector encoding for the words instead of a dummy\n",
    "one-hot encoding, I thought about replacing the player names with a placeholder\n",
    "such as `heatplayer`, but this appears to be mostly unnecessary, as the vectorizer\n",
    "that is build in already knows names such as `lebron`. One can confirm this with\n",
    "`nlp('lebron')[0].vector` and see it is not hte zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "regexes = {\n",
    "    'weird_chars': re.compile(r'[\\?;\\(\\)\\\\.,\\!:–\\-\\\"\\[\\]“”]'),\n",
    "    'newlines': re.compile(r'\\n'),\n",
    "    'html': re.compile(r'<[^<]+/>', re.MULTILINE),\n",
    "    'urls': re.compile(r'^https?://.*[rn]*', re.MULTILINE),\n",
    "    'spaces': re.compile(r'\\s{2,}'),\n",
    "    'u': re.compile(r'\\bu\\b')\n",
    "}\n",
    "\n",
    "allowed_stops = ['no', 'never', 'not', 'none', 'up', 'down', \n",
    "                 'back', 'over', 'under', 'two', 'three']\n",
    "stop_words = [\n",
    "    word for word in nlp.Defaults.stop_words \n",
    "    if word not in allowed_stops\n",
    "]\n",
    "stop_words += ['-PRON-']\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\" substitute things based on the regexes above \"\"\"\n",
    "    text = text.lower()\n",
    "    \n",
    "    # custom replacements we decided on\n",
    "    text = text.replace('`', '\\'')\n",
    "    text = text.replace('’', '\\'')\n",
    "    \n",
    "    text  = text.replace(\"won't\", \"will not\")\n",
    "    text  = text.replace(\"n't\", \" not\")\n",
    "    \n",
    "    text = regexes['u'].sub('you', text)\n",
    "    text = regexes['html'].sub(' ', text)\n",
    "    text = regexes['urls'].sub(' ', text)\n",
    "    text = regexes['weird_chars'].sub(' ', text)\n",
    "    text = regexes['newlines'].sub(' ', text)\n",
    "    text = regexes['spaces'].sub(' ', text)\n",
    "    \n",
    "    # removing the multiletters such as 'happppy' -> 'happy'\n",
    "    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
    "    \n",
    "    # lemmatize\n",
    "    text = ' '.join([word.lemma_ for word in nlp(text)])\n",
    "    \n",
    "    # remove stopwords\n",
    "    text = ' '.join([w for w in text.split(' ') if w not in stop_words])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's just print some examples of what we end up with when cleaning the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| He's coming back from an injury.\n",
      "|\t\t-----------\n",
      "| come back injury\n",
      "\n",
      "\n",
      "\n",
      "| Y'all thought that we were joking about the championship?\n",
      "|\t\t-----------\n",
      "| think joke championship\n",
      "\n",
      "\n",
      "\n",
      "| Flagrant 2 requires it to be excessively violent, which it pretty clearly wasn’t. Easy flagrant 1 call\n",
      "|\t\t-----------\n",
      "| flagrant 2 require excessively violent pretty clearly not easy flagrant 1\n",
      "\n",
      "\n",
      "\n",
      "| Get AD back in pls\n",
      "|\t\t-----------\n",
      "| ad back pls\n",
      "\n",
      "\n",
      "\n",
      "| Are we going to win a fucking championship? I can't believe this. I thought the warriors would be on top forever (and then some).\n",
      "|\t\t-----------\n",
      "| win fucking championship not believe think warrior forever\n"
     ]
    }
   ],
   "source": [
    "def print_cleantext(text):\n",
    "    print('|', text)\n",
    "    print('|\\t\\t-----------')\n",
    "    print('|',clean_text(text))\n",
    "\n",
    "print_cleantext(heat_comments[50].body)\n",
    "print('\\n\\n')\n",
    "print_cleantext(heat_comments[1047].body)\n",
    "print('\\n\\n')\n",
    "print_cleantext(lakers_comments[500].body)\n",
    "print('\\n\\n')\n",
    "print_cleantext(lakers_comments[-200].body)\n",
    "print('\\n\\n')\n",
    "print_cleantext(lakers_comments[14477].body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and saving data\n",
    "\n",
    "Now we do initiate the (long?) work of converting the comments into vectors which we\n",
    "can then save. We have approximately 60,000 comments total. Just for a ballpark estimate,\n",
    "let's suppose there are 10 words per comment. Thus we need to store about 600k vectors.\n",
    "How much space does each vector take? We can figure out as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size in bytes: 96\n",
      "Estimate size to store: 57.6 MB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "vector_size = sys.getsizeof(nlp('lebron')[0].vector)\n",
    "print('Vector size in bytes:', vector_size)\n",
    "print('Estimate size to store:', vector_size * 600_000 / 1e6, 'MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is well within our ability to store things in memory or on disk even if we were off in our average comment size\n",
    "by a factor of ten, so no worries there for this application. We'll store the data as a list of tuples where each\n",
    "tuple contains both a numpy array of shape (n, 300) where n is the number of words in the comment and a 0 or 1\n",
    "depending on if the comment is from a Heat fan (0) or a Lakers fan (1). Previously we guaranteed that the comments\n",
    "had a certain length, but stop word removal will shorten comments, so we will drop some comments here if there are \n",
    "not enough words after cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def comment_to_numpy_array(comment, min_words=3):\n",
    "    \n",
    "    # first step is to clean the comment\n",
    "    clean_comment = clean_text(comment)\n",
    "    \n",
    "    # next is to get an nlp object\n",
    "    nlp_comment = nlp(clean_comment)\n",
    "    \n",
    "    # get nonzero vectors\n",
    "    word_vectors = [\n",
    "        word.vector for word in nlp_comment\n",
    "        if np.count_nonzero(word.vector) > 0\n",
    "    ]\n",
    "    \n",
    "    # return sufficiently long comments\n",
    "    if len(word_vectors) >= min_words:\n",
    "        return word_vectors\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inital_lakers_comments = [\n",
    "    comment_to_numpy_array(comment.body) for comment in lakers_comments\n",
    "]\n",
    "\n",
    "lakers_vector_comments = [\n",
    "    (comment, 1) for comment in initial_lakers_comments\n",
    "    if comment is not None\n",
    "]\n",
    "\n",
    "heat_lakers_comments = [\n",
    "    comment_to_numpy_array(comment.body) for comment in heat_comments\n",
    "]\n",
    "\n",
    "heat_vector_comments = [\n",
    "    (comment, 1) for comment in initial_heat_comments\n",
    "    if comment is not None\n",
    "]\n",
    "\n",
    "with open('data/lakers_vector_comments.pkl', 'wb') as f:\n",
    "    pickle.dump(lakers_vector_comments, f)\n",
    "    \n",
    "with open('data/heat_vector_comments.pkl', 'wb') as f:\n",
    "    pickle.dump(heat_vector_comments, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gitpage] *",
   "language": "python",
   "name": "conda-env-gitpage-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
