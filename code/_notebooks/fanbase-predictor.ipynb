{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: code-post\n",
    "title: Pedicting rooting interests on reddit\n",
    "description: Going to see if we can predict neutral fan rooting interests from reddit posts.\n",
    "tags: [neural nets]\n",
    "---\n",
    "\n",
    "In this notebook / post I'm going to try to see if we can predict who a\n",
    "supposedly neutral fan is rooting for in the (as of this writing) about to\n",
    "end Lakers-Heat NBA finals. To do this, I'm going to attempt to scrape posts\n",
    "from game threads by Heat and Lakers fans to train an LSTM based model. As long\n",
    "as I can scrape the flair from from the reddit API, this should be doable. If\n",
    "not, then I will have to scrape from the team specific subreddits and try that\n",
    "way.\n",
    "\n",
    "I'm going to write the disclaimer here: I don't think I'll find much signal in\n",
    "these posts. Many of the posts will be pretty neutral, so it'll be hard to\n",
    "distinguish fans based off a single post. The most likely scenario to me is that\n",
    "it picks up on the player names for each team and guess based on that which \n",
    "might work for a game not between the two teams. But let's find out! I'm going\n",
    "to commit the notebook with this caveat first (check the commits!).\n",
    "\n",
    "Outline:\n",
    "- Data Collection\n",
    "  - PRAW - Exploring Reddit\n",
    "  - Pushshift - Gathering data\n",
    "  - Preparing to clean - Data exploration\n",
    "  - Cleaning and saving data\n",
    "- Modeling\n",
    "  - Dataset\n",
    "  - Model\n",
    "  - Training\n",
    "- Test set predictions\n",
    "  - Get and predict test set\n",
    "  - Explore posts and users\n",
    "\n",
    "## Data Collection\n",
    "\n",
    "If you don't know what [reddit](https://reddit.com/) is, it's a website that is\n",
    "organized into communities called _subreddits_. The site has _users_ which belong to\n",
    "multiple subreddits. Each subreddit contains a sequence of _submissions_ (also called\n",
    "_posts_) which can be links\n",
    "to other sites, images, or text. Each post contains _comments_ which are text only\n",
    "an are made by the users. The _comment section_ is organized as a forest of trees\n",
    "wich top level comments and then comments nested below each top level comment. Each\n",
    "user can have a _flair_ which varies with the subreddit they are posting in. The\n",
    "flair and username are posted along with each comment. The flair will contains a\n",
    "small image as well as text. Not every user has a flair associated to it for every\n",
    "subreddit to which they belong. It is not always required to be a member of the\n",
    "subreddit in order to post, although this varies by community.\n",
    "\n",
    "We will be using the NBA subreddit which as of this writing has ~3.5 million users.\n",
    "Users in this subreddit have flairs which denote which team the user is a supporter\n",
    "of. Mine is for the Cleveland Cavaliers.\n",
    "\n",
    "### PRAW - Exploring Reddit\n",
    "\n",
    "In order to access reddit, we need to have a reddit account and then create an application\n",
    "with that account as a developer of the application. Go to [the apps page](https://www.reddit.com/prefs/apps)\n",
    "to create an application. (Please read the terms and conditions!)\n",
    "You need to store the name of the app as the `user_agent`, the\n",
    "`client_id` is the 14 character string that is below the app name once it's created, \n",
    "you'll have a 27 character secret secret that is generated and is the `client_secret`, \n",
    "and you'll need your `username`. I have put these credentials in an encrypted YAML file\n",
    "that I created using ansible-vault.\n",
    "\n",
    "We will be using [PRAW](https://github.com/praw-dev/praw), the Python Reddit API\n",
    "Wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import yaml\n",
    "from ansible_vault import Vault\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ····················\n"
     ]
    }
   ],
   "source": [
    "vault = Vault(getpass())\n",
    "with open('redditcreds.yml', 'r') as f:\n",
    "    reddit_creds = vault.load(f.read())\n",
    "\n",
    "reddit = praw.Reddit(username=reddit_creds['username'],\n",
    "                     user_agent=reddit_creds['user_agent'],\n",
    "                     client_id=reddit_creds['client_id'],\n",
    "                     client_secret=reddit_creds['client_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get into the [NBA subreddit](https://www.reddit.com/r/nba) and search for posts which are _game threads_\n",
    "to which users will post while a game is ongoing. These are posted automatically and so they have a \n",
    "predictible title format which makes searching for them and then filtering the received submisisons\n",
    "by title easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit('nba')\n",
    "\n",
    "submissions = subreddit.search(query='title:\"GAME THREAD\"',\n",
    "                               time_filter=\"month\")\n",
    "game_threads = [\n",
    "    s for s in submissions \n",
    "    if\n",
    "        s.title[:11] == 'GAME THREAD' \n",
    "        and (\"Lakers\" in s.title or \"Heat\" in s.title)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAME THREAD: Miami Heat (44-29) @ Los Angeles Lakers (52-19) - (October 09, 2020)\n",
      "GAME THREAD: Miami Heat (44-29) @ Los Angeles Lakers (52-19) - (September 30, 2020)\n",
      "GAME THREAD: Los Angeles Lakers (52-19) @ Miami Heat (44-29) - (October 06, 2020)\n",
      "GAME THREAD: Los Angeles Lakers (52-19) @ Miami Heat (44-29) - (October 04, 2020)\n",
      "GAME THREAD: Miami Heat (44-29) @ Los Angeles Lakers (52-19) - (October 02, 2020)\n",
      "GAME THREAD: Denver Nuggets (46-27) @ Los Angeles Lakers (52-19) - (September 26, 2020)\n",
      "GAME THREAD: Los Angeles Lakers (52-19) @ Denver Nuggets (46-27) - (September 24, 2020)\n",
      "GAME THREAD: Los Angeles Lakers (52-19) @ Denver Nuggets (46-27) - (September 22, 2020)\n",
      "GAME THREAD: Denver Nuggets (46-27) @ Los Angeles Lakers (52-19) - (September 18, 2020)\n",
      "GAME THREAD: Boston Celtics (48-24) @ Miami Heat (44-29) - (September 27, 2020)\n",
      "GAME THREAD: Denver Nuggets (46-27) @ Los Angeles Lakers (52-19) - (September 20, 2020)\n",
      "GAME THREAD: Miami Heat (44-29) @ Boston Celtics (48-24) - (September 25, 2020)\n",
      "GAME THREAD: Boston Celtics (48-24) @ Miami Heat (44-29) - (September 23, 2020)\n",
      "GAME THREAD: Miami Heat (44-29) @ Boston Celtics (48-24) - (September 15, 2020)\n",
      "GAME THREAD: Miami Heat (44-29) @ Boston Celtics (48-24) - (September 17, 2020)\n",
      "GAME THREAD: Boston Celtics (48-24) @ Miami Heat (44-29) - (September 19, 2020)\n",
      "GAME THREAD: Houston Rockets (44-28) @ Los Angeles Lakers (52-19) - (September 12, 2020)\n",
      "GAME THREAD: Los Angeles Lakers (52-19) @ Houston Rockets (44-28) - (September 10, 2020)\n"
     ]
    }
   ],
   "source": [
    "for t in game_threads:\n",
    "    print(t.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comments for a submission are contained in PRAW `CommentForest`. If we do not\n",
    "care about the structure, we can flatten this to a list. Note that the list\n",
    "will contain both `Comment` objects as well as `MoreComments` objectcs. It is possible to replace\n",
    "the `MoreComments` objects using the `replace_more` function, but each replacement\n",
    "requires calling the reddit API. By default, 32 of the `MoreComments` objects will\n",
    "be replaced, which I will keep but I will also limit to those which contain at least\n",
    "5 more comments in them. Doing this removes all `MoreComments` instances from\n",
    "the list of comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAME THREAD: Miami Heat (44-29) @ Los Angeles Lakers (52-19) - (October 09, 2020)\n",
      "number of comments: 28455\n"
     ]
    }
   ],
   "source": [
    "game_thread = game_threads[0]\n",
    "print(game_thread.title)\n",
    "print(\"number of comments:\", game_thread.num_comments)\n",
    "game_thread.comments.replace_more(limit=32, threshold=5)\n",
    "comments = game_thread.comments.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is a random comment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment author:    StoneColdAM\n",
      "comment score:     28 (28-0)\n",
      "author flair text: Lakers\n",
      "body:\n",
      "\n",
      " Dwight Howard strategy: get Jimmy Butler ejected.\n"
     ]
    }
   ],
   "source": [
    "c = comments[10]\n",
    "print('comment author:   ', c.author.name)\n",
    "print('comment score:    ', c.score, '({}-{})'.format(c.ups, c.downs))\n",
    "print('author flair text:', c.author_flair_text)\n",
    "print('body:\\n\\n', c.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training set, the goal will be to find authors which are flaired as Lakers or Heat or have\n",
    "flair text that contains LAL or MIA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of comments by Lakers users: 1042\n",
      "number of comments by Heat users  : 275\n"
     ]
    }
   ],
   "source": [
    "lakers_comments = [\n",
    "    c for c in comments\n",
    "    if\n",
    "        c.author_flair_text is not None\n",
    "        and (\n",
    "            c.author_flair_text == 'Lakers'\n",
    "            or '[LAL]' in c.author_flair_text\n",
    "        )\n",
    "]\n",
    "print('number of comments by Lakers users:', len(lakers_comments))\n",
    "\n",
    "heat_comments = [\n",
    "    c for c in comments\n",
    "    if\n",
    "        c.author_flair_text is not None\n",
    "        and (\n",
    "            c.author_flair_text == 'Heat'\n",
    "            or '[MIA]' in c.author_flair_text\n",
    "        )\n",
    "]\n",
    "print('number of comments by Heat users  :', len(heat_comments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pushshift - Gathering data\n",
    "\n",
    "It turns out that PRAW is fairly limited and the [Pushshift API](https://github.com/pushshift/api) is\n",
    "more powerful than it. There is a wrapper for this API called [PSAW](https://github.com/dmarx/psaw).\n",
    "PSAW and PRAW interact with each other nicely: if you pass a PRAW `Reddit` instance to PSAW's\n",
    "`PushShiftAPI` class, pushshift gathers the IDs you want but then returns PRAW objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "import datetime as dt\n",
    "\n",
    "api = PushshiftAPI(reddit)\n",
    "\n",
    "# we'll search month by month for submissions\n",
    "game_threads = []\n",
    "\n",
    "months = [\n",
    "    int(dt.datetime(2019, 10, 19).timestamp()),\n",
    "    int(dt.datetime(2019, 11, 1).timestamp()),\n",
    "    int(dt.datetime(2019, 12, 1).timestamp()),\n",
    "    int(dt.datetime(2020, 1, 1).timestamp()),\n",
    "    int(dt.datetime(2020, 2, 1).timestamp()),\n",
    "    int(dt.datetime(2020, 3, 1).timestamp()),\n",
    "    int(dt.datetime(2020, 4, 1).timestamp()),\n",
    "    int(dt.datetime(2020, 5, 1).timestamp()),\n",
    "    int(dt.datetime(2020, 6, 1).timestamp()),\n",
    "    int(dt.datetime(2020, 7, 1).timestamp()),\n",
    "    int(dt.datetime(2020, 8, 1).timestamp()),\n",
    "    int(dt.datetime(2020, 9, 1).timestamp()),\n",
    "    int(dt.datetime(2020, 10, 1).timestamp()),\n",
    "    int(dt.datetime(2020, 11, 1).timestamp())\n",
    "]\n",
    "\n",
    "for i in range(len(months)-1):\n",
    "    submissions = list(api.search_submissions(after=months[i],\n",
    "                                              before=months[i+1],\n",
    "                                              q='game thread',\n",
    "                                              subreddit='nba',\n",
    "                                              author='NBA_MOD',\n",
    "                                              limit=5000))\n",
    "    \n",
    "    game_threads += [\n",
    "        s for s in submissions\n",
    "        if\n",
    "            s.title[:11] == 'GAME THREAD'\n",
    "            and (\"Lakers\" in s.title or \"Heat\" in s.title)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have game threads, it's time to collect relevant comments from each game.\n",
    "We'll be using PRAW for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scouring game thread: 180/180\n",
      "number of lakers comments: 47535\n",
      "  number of heat comments: 12907\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "lakers_comments = []\n",
    "heat_comments = []\n",
    "\n",
    "num_game_threads = len(game_threads)\n",
    "i = 1\n",
    "\n",
    "def check_lakers(comment):\n",
    "    if comment.author_flair_text is not None:\n",
    "        if comment.author_flair_text == 'Lakers':\n",
    "            return True\n",
    "        if '[LAL]' in comment.author_flair_text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def check_heat(comment):\n",
    "    if comment.author_flair_text is not None:\n",
    "        if comment.author_flair_text == 'Heat':\n",
    "            return True\n",
    "        if '[MIA]' in comment.author_flair_text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "for game_thread in game_threads:\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print('scouring game thread: {}/{}'.format(i, num_game_threads))\n",
    "    print('number of lakers comments:', len(lakers_comments))\n",
    "    print('  number of heat comments:', len(heat_comments))\n",
    "    \n",
    "    has_lakers = 'Lakers' in game_thread.title\n",
    "    has_heat = 'Heat' in game_thread.title\n",
    "    \n",
    "    # NOTE: this part takes a long time\n",
    "    game_thread.comments.replace_more(limit=32, threshold=2)\n",
    "    comments = game_thread.comments.list()\n",
    "    \n",
    "    if has_lakers and has_heat:\n",
    "        lakers = []\n",
    "        heat = []\n",
    "        \n",
    "        for c in comments:\n",
    "            if check_lakers(c) and len(c.body.split()) > 3:\n",
    "                lakers += [c]\n",
    "            elif check_heat(c) and len(c.body.split()) > 3:\n",
    "                heat += [c]\n",
    "                \n",
    "        lakers_comments += lakers\n",
    "        heat_comments += heat\n",
    "        \n",
    "    elif has_lakers:\n",
    "        lakers =[]\n",
    "        \n",
    "        for c in comments:\n",
    "            if check_lakers(c) and len(c.body.split()) > 3:\n",
    "                lakers += [c]\n",
    "                \n",
    "        lakers_comments += lakers\n",
    "      \n",
    "    elif has_heat:\n",
    "        heat = []\n",
    "        \n",
    "        for c in comments:\n",
    "            if check_heat(c) and len(c.body.split()) > 3:\n",
    "                heat += [c]\n",
    "                \n",
    "        heat_comments += heat\n",
    "        \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/lakers_comments.pkl', 'wb') as f:\n",
    "    pickle.dump(lakers_comments, f)\n",
    "    \n",
    "with open('data/heat_comments.pkl', 'wb') as f:\n",
    "    pickle.dump(heat_comments, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perparing to clean - data exploration\n",
    "\n",
    "Cleaning text is tedious and many people have already though about what to do. \n",
    "I am taking code from [this blog post](https://hub.packtpub.com/clean-social-media-data-analysis-python/).\n",
    "The cleaning we're doing is just about replacing weird and unwanted characters and then\n",
    "lemmatizing.\n",
    "\n",
    "I have played around with the stop words a bit. Since this is really a form of\n",
    "sentiment analysis, I put back in the negations. Since this is basketball, I also\n",
    "want numbers such as two and three to be allowed. I want some ofthe directions like\n",
    "up and down and over and under.\n",
    "\n",
    "Since I'm going to use spaCy's vector encoding for the words instead of a dummy\n",
    "one-hot encoding, I thought about replacing the player names with a placeholder\n",
    "such as `heatplayer`, but this appears to be mostly unnecessary, as the vectorizer\n",
    "that is build in already knows names such as `lebron`. One can confirm this with\n",
    "`nlp('lebron')[0].vector` and see it is not hte zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's reload things as needed\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('data/lakers_comments.pkl', 'rb') as f:\n",
    "    lakers_comments = pickle.load(f)\n",
    "    \n",
    "with open('data/heat_comments.pkl', 'rb') as f:\n",
    "    heat_comments = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "regexes = {\n",
    "    'weird_chars': re.compile(r'[\\?;\\(\\)\\\\.,\\!:–\\-\\\"\\[\\]“”]'),\n",
    "    'newlines': re.compile(r'\\n'),\n",
    "    'html': re.compile(r'<[^<]+/>', re.MULTILINE),\n",
    "    'urls': re.compile(r'^https?://.*[rn]*', re.MULTILINE),\n",
    "    'spaces': re.compile(r'\\s{2,}'),\n",
    "    'u': re.compile(r'\\bu\\b')\n",
    "}\n",
    "\n",
    "allowed_stops = ['no', 'never', 'not', 'none', 'up', 'down', \n",
    "                 'back', 'over', 'under', 'two', 'three']\n",
    "stop_words = [\n",
    "    word for word in nlp.Defaults.stop_words \n",
    "    if word not in allowed_stops\n",
    "]\n",
    "stop_words += ['-PRON-']\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\" substitute things based on the regexes above \"\"\"\n",
    "    text = text.lower()\n",
    "    \n",
    "    # custom replacements we decided on\n",
    "    text = text.replace('`', '\\'')\n",
    "    text = text.replace('’', '\\'')\n",
    "    \n",
    "    text  = text.replace(\"won't\", \"will not\")\n",
    "    text  = text.replace(\"n't\", \" not\")\n",
    "    \n",
    "    text = regexes['u'].sub('you', text)\n",
    "    text = regexes['html'].sub(' ', text)\n",
    "    text = regexes['urls'].sub(' ', text)\n",
    "    text = regexes['weird_chars'].sub(' ', text)\n",
    "    text = regexes['newlines'].sub(' ', text)\n",
    "    text = regexes['spaces'].sub(' ', text)\n",
    "    \n",
    "    # removing the multiletters such as 'happppy' -> 'happy'\n",
    "    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
    "    \n",
    "    # lemmatize\n",
    "    text = ' '.join([word.lemma_ for word in nlp(text)])\n",
    "    \n",
    "    # remove stopwords\n",
    "    text = ' '.join([w for w in text.split(' ') if w not in stop_words])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's just print some examples of what we end up with when cleaning the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| He's coming back from an injury.\n",
      "|\t\t-----------\n",
      "| come back injury\n",
      "\n",
      "\n",
      "\n",
      "| Y'all thought that we were joking about the championship?\n",
      "|\t\t-----------\n",
      "| think joke championship\n",
      "\n",
      "\n",
      "\n",
      "| Flagrant 2 requires it to be excessively violent, which it pretty clearly wasn’t. Easy flagrant 1 call\n",
      "|\t\t-----------\n",
      "| flagrant 2 require excessively violent pretty clearly not easy flagrant 1\n",
      "\n",
      "\n",
      "\n",
      "| Get AD back in pls\n",
      "|\t\t-----------\n",
      "| ad back pls\n",
      "\n",
      "\n",
      "\n",
      "| Are we going to win a fucking championship? I can't believe this. I thought the warriors would be on top forever (and then some).\n",
      "|\t\t-----------\n",
      "| win fucking championship not believe think warrior forever\n"
     ]
    }
   ],
   "source": [
    "def print_cleantext(text):\n",
    "    print('|', text)\n",
    "    print('|\\t\\t-----------')\n",
    "    print('|',clean_text(text))\n",
    "\n",
    "print_cleantext(heat_comments[50].body)\n",
    "print('\\n\\n')\n",
    "print_cleantext(heat_comments[1047].body)\n",
    "print('\\n\\n')\n",
    "print_cleantext(lakers_comments[500].body)\n",
    "print('\\n\\n')\n",
    "print_cleantext(lakers_comments[-200].body)\n",
    "print('\\n\\n')\n",
    "print_cleantext(lakers_comments[14477].body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and saving data\n",
    "\n",
    "Now we do initiate the (long?) work of converting the comments into vectors which we\n",
    "can then save. We have approximately 60,000 comments total. Just for a ballpark estimate,\n",
    "let's suppose there are 10 words per comment. Thus we need to store about 600k vectors.\n",
    "How much space does each vector take? We can figure out as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size in bytes: 96\n",
      "Estimate size to store: 57.6 MB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "vector_size = sys.getsizeof(nlp('lebron')[0].vector)\n",
    "print('Vector size in bytes:', vector_size)\n",
    "print('Estimate size to store:', vector_size * 600_000 / 1e6, 'MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is well within our ability to store things in memory or on disk even if we were off in our average comment size\n",
    "by a factor of ten, so no worries there for this application. We'll store the data as a list of tuples where each\n",
    "tuple contains both a numpy array of shape (n, 300) where n is the number of words in the comment and a 0 or 1\n",
    "depending on if the comment is from a Heat fan (0) or a Lakers fan (1). Previously we guaranteed that the comments\n",
    "had a certain length, but stop word removal will shorten comments, so we will drop some comments here if there are \n",
    "not enough words after cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def comment_to_numpy_array(comment, min_words=3):\n",
    "    \n",
    "    # first step is to clean the comment\n",
    "    clean_comment = clean_text(comment)\n",
    "    \n",
    "    # next is to get an nlp object\n",
    "    nlp_comment = nlp(clean_comment)\n",
    "    \n",
    "    # get nonzero vectors\n",
    "    word_vectors = [\n",
    "        word.vector for word in nlp_comment\n",
    "        if np.count_nonzero(word.vector) > 0\n",
    "    ]\n",
    "    \n",
    "    # return sufficiently long comments\n",
    "    if len(word_vectors) >= min_words:\n",
    "        return word_vectors\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lakers_comments = [\n",
    "    comment_to_numpy_array(comment.body) for comment in lakers_comments\n",
    "]\n",
    "\n",
    "lakers_vector_comments = [\n",
    "    (comment, 1) for comment in initial_lakers_comments\n",
    "    if comment is not None\n",
    "]\n",
    "\n",
    "with open('data/lakers_vector_comments.pkl', 'wb') as f:\n",
    "    pickle.dump(lakers_vector_comments, f)\n",
    "\n",
    "initial_heat_comments = [\n",
    "    comment_to_numpy_array(comment.body) for comment in heat_comments\n",
    "]\n",
    "\n",
    "heat_vector_comments = [\n",
    "    (comment, 0) for comment in initial_heat_comments\n",
    "    if comment is not None\n",
    "]\n",
    "    \n",
    "with open('data/heat_vector_comments.pkl', 'wb') as f:\n",
    "    pickle.dump(heat_vector_comments, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual sizes of these files on disk were 96 MB for the Heat comments and 347 MB for the Lakers comments.\n",
    "\n",
    "## Modeling\n",
    "\n",
    "Now we go through the modeling process, by which we mean the process of using PyTorch to build \n",
    "and train a model to predict whether a post is by a Lakers fan or by a Heat fan.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "This section is about creating a PyTorch `Dataset` that loads the data\n",
    "that we scraped from Reddit and cleaned in the data collection section. Data cleaning\n",
    "is data modeling, while what we do here is not.\n",
    "\n",
    "We will try to use the `WeightedRandomSampler` (with replacement) so that we can return\n",
    "comments from Lakers and Heat fans with equal probability even though the two\n",
    "classes are imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, we load in what we saved if we need to\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('data/lakers_vector_comments.pkl', 'rb') as f:\n",
    "    lakers_vector_comments = pickle.load(f)\n",
    "    \n",
    "with open('data/heat_vector_comments.pkl', 'rb') as f:\n",
    "    heat_vector_comments = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, lakers_vector_comments, heat_vector_comments):\n",
    "        \"\"\" we will combine the two datasets into one dataset\"\"\"\n",
    "        \n",
    "        self._len_lakers = len(lakers_vector_comments)\n",
    "        self._len_heat = len(heat_vector_comments)\n",
    "        \n",
    "        self._samples = [\n",
    "            torch.tensor(c[0]) for c in lakers_vector_comments + heat_vector_comments\n",
    "        ]\n",
    "        self._labels = [\n",
    "            torch.tensor(c[1]) for c in lakers_vector_comments + heat_vector_comments\n",
    "        ]\n",
    "        \n",
    "    @property\n",
    "    def len_lakers(self):\n",
    "        return self._len_lakers\n",
    "    \n",
    "    @property\n",
    "    def len_heat(self):\n",
    "        return self._len_heat\n",
    "    \n",
    "    @property\n",
    "    def samples(self):\n",
    "        return self._samples\n",
    "    \n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len_lakers + self.len_heat\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        return self.samples[idx], self.labels[idx]\n",
    "    \n",
    "training_data = TrainDataset(lakers_vector_comments, heat_vector_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2179,  0.4413, -0.4320,  ..., -0.1647,  0.3225,  0.3058],\n",
       "        [-0.7036,  0.1382,  0.6530,  ..., -0.1667, -0.0218,  0.3645],\n",
       "        [ 0.6381, -0.2333,  0.2290,  ..., -0.0055, -0.4417, -0.2157],\n",
       "        ...,\n",
       "        [ 0.3504,  0.1946, -0.4185,  ...,  0.5152,  0.0354, -0.1521],\n",
       "        [ 0.5428, -0.2805,  0.7388,  ..., -0.1633, -0.4704,  0.1079],\n",
       "        [-0.5329, -0.2012, -0.0371,  ..., -0.0696,  0.0811,  0.2704]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up the sampler so that it chooses Lakers and Heat comments with equal probability.\n",
    "We do this because we expect (foolishly, probably) that comments by neutral fans on game\n",
    "threads are equally likely to be for the Lakers as for the Heat. This is almost certainly\n",
    "false since LeBron plays for the Lakers and is undoubtedly the most popular player in\n",
    "the league, but we'll use this assumption for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "lakers_weight = training_data.len_heat / len(training_data)\n",
    "heat_weight = training_data.len_lakers / len(training_data)\n",
    "\n",
    "weights = np.concatenate([\n",
    "    lakers_weight * np.ones(training_data.len_lakers),\n",
    "    heat_weight * np.ones(training_data.len_heat)\n",
    "])\n",
    "\n",
    "sampler = WeightedRandomSampler(weights, len(weights), replacement=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a `DataLoader` to give use random samples. We will not be doing batches, since I'm \n",
    "unsure how to do that with samples of different lengths without padding the shorter comments, which I\n",
    "have decided not to do out of inertia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(training_data,\n",
    "                          batch_size=1,\n",
    "                          shuffle=False, # required by sampler type\n",
    "                          sampler=sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Model\n",
    "\n",
    "And now we create our LSTM based model to predict whether a comment comes from a Lakers fan or a Heat fan.\n",
    "\n",
    "It seems to be an open question of how large the hidden state should be in an LSTM. It seems that some number\n",
    "between the input and output layers should work and ultimately this is something that would need to be\n",
    "optimized for on a validation set. I'm not going to bother with that and just set it to be 128, i.e., a power of\n",
    "2 that is just under half of the input size and hope that works out well enough. Some discussions can be found on [stackexchange](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw/136542#136542)\n",
    "and [quora](https://www.quora.com/In-LSTM-how-do-you-figure-out-what-size-the-weights-are-supposed-to-be) so take\n",
    "that how you want, dear reader. The PyTorch LSTM docs are [here](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BasketballNet(nn.Module):\n",
    "\n",
    "    def __init__(self, random_state=47):\n",
    "        super(BasketballNet, self).__init__()\n",
    "        torch.manual_seed(random_state)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=300,\n",
    "                            hidden_size=128,\n",
    "                            num_layers=1,\n",
    "                            bias=True)\n",
    "        self.to_out = nn.Linear(128, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Non-batched for a sequence with n words the\n",
    "        input needs to have shape (n, 1, 300).\n",
    "        We put in sequences of size (n, 300) which\n",
    "        we can change with a view.\n",
    "        \n",
    "        We will pass in the entire sequence at once,\n",
    "        but one can also iterate through one word at a\n",
    "        time. This would allow for a truncated backprop\n",
    "        through time since one could zero out the state\n",
    "        partway through. We won't be doing this. \"\"\"\n",
    "        \n",
    "        lstm_out, _ = self.lstm(x.view(x.shape[0], 1, -1))\n",
    "        raw_out = self.to_out(lstm_out[x.shape[0]-1])\n",
    "        out = torch.sigmoid(raw_out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show that this does in fact work by just feeding it some random input of the correct size\n",
    "and getting an output from it. That's it, that's the test. It's how I made sure it was working\n",
    "and fixed some typos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random output from random input: tensor([[0.5075]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "basketball_net = BasketballNet()\n",
    "random_input = torch.randn(5, 300)\n",
    "test_out = basketball_net(random_input)\n",
    "print('random output from random input:', test_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now let's train. We'll use ADAM to optimize and mean square error loss. We're not\n",
    "going too crazy here, ya know. I'll probably write this elsewhere in this post, but \n",
    "I don't think we're going to get great results. This is because fundamentally I don't\n",
    "believe that there's much signal in the training examples and that this is a hard\n",
    "problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "basketball_net = BasketballNet()\n",
    "optimizer = Adam(basketball_net.parameters())\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial training error rate: 0.655\n"
     ]
    }
   ],
   "source": [
    "num_wrong = 0.0\n",
    "for sample, label in training_data:\n",
    "    output = basketball_net(sample)\n",
    "    \n",
    "    if output[0][0] >= 0.5:\n",
    "        prediction = 1.0\n",
    "    else:\n",
    "        prediction = 0.0\n",
    "        \n",
    "    if np.abs(prediction - label.item()) > 0:\n",
    "        num_wrong += 1\n",
    "        \n",
    "print('initial training error rate: {:.3f}'.format(num_wrong / len(training_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 - loss: 0.251 - classification error: 0.217 - rate lakers: 0.497\n",
      "epoch: 2 - loss: 0.736 - classification error: 0.143 - rate lakers: 0.503\n",
      "epoch: 3 - loss: 0.000 - classification error: 0.111 - rate lakers: 0.498\n",
      "epoch: 4 - loss: 0.253 - classification error: 0.088 - rate lakers: 0.499\n",
      "epoch: 5 - loss: 0.001 - classification error: 0.076 - rate lakers: 0.498\n",
      "epoch: 6 - loss: 0.000 - classification error: 0.069 - rate lakers: 0.497\n",
      "epoch: 7 - loss: 0.000 - classification error: 0.064 - rate lakers: 0.500\n",
      "epoch: 8 - loss: 0.000 - classification error: 0.059 - rate lakers: 0.502\n",
      "epoch: 9 - loss: 0.009 - classification error: 0.056 - rate lakers: 0.500\n",
      "epoch: 10 - loss: 0.000 - classification error: 0.055 - rate lakers: 0.498\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10): \n",
    "\n",
    "    running_loss = 0.0\n",
    "    num_wrong = 0.0\n",
    "    num_lakers = 0.0\n",
    "    num_heat = 0.0\n",
    "    for sample, label in train_loader:\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        output = basketball_net(sample[0])\n",
    "        loss = criterion(output.float(), label.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if output[0][0] >= 0.5:\n",
    "            prediction = 1\n",
    "        else:\n",
    "            prediction = 0\n",
    "            \n",
    "        if int(label.item()) == 1:\n",
    "            num_lakers += 1\n",
    "        else:\n",
    "            num_heat += 1\n",
    "        \n",
    "        num_wrong += np.abs(prediction - label.item())\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    class_error = num_wrong / len(train_loader)\n",
    "    rate_lakers = num_lakers / (num_lakers + num_heat)\n",
    "    print('epoch: {} - loss: {:.3f} - classification error: {:.3f} - rate lakers: {:.3f}'.format(epoch+1,\n",
    "                                                                                                 loss,\n",
    "                                                                                                 class_error,\n",
    "                                                                                                 rate_lakers))\n",
    "    \n",
    "    if class_error == 0.0:\n",
    "        break\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This certainly looks like overtraining to me. The random samples do give weird things like\n",
    "the loss fluctuating so much.  Let's seee what the overall error rate is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final training error rate: 0.057\n"
     ]
    }
   ],
   "source": [
    "num_wrong = 0.0\n",
    "for sample, label in training_data:\n",
    "    output = basketball_net(sample)\n",
    "    \n",
    "    if output[0][0] >= 0.5:\n",
    "        prediction = 1.0\n",
    "    else:\n",
    "        prediction = 0.0\n",
    "        \n",
    "    if np.abs(prediction - label.item()) > 0:\n",
    "        num_wrong += 1\n",
    "        \n",
    "print('final training error rate: {:.3f}'.format(num_wrong / len(training_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least the error rate didn't shoot up, but the model has probably seen almost all the Lakers samples by this point.\n",
    "I've thought about re-training and stopping after a couple epochs, but I think I'll just roll with what we've got\n",
    "and see what the reuslts look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of neutral fans\n",
    "\n",
    "For the \"test set\" we're going to take the comments from neutral fans during the finals between the Lakers\n",
    "and Heat and do some analysis. Since these comments don't have labels, this isn't really a test set, it's\n",
    "just we're going to see what sort of (flawed!) analysis we can do with this overfit model.\n",
    " \n",
    "Before doing that, let's play around with some comments that we made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Lebron is the goat basketball player': Lakers (100.0%)\n",
      "\n",
      "'Lebron is terrible. He flops. I hate him. Jordan is better': Lakers (86.1%)\n",
      "\n",
      "'Jimmy Butler tries harder than everyone he deserves this': Heat (99.9%)\n",
      "\n",
      "'Duncan Robinson is a great shooter Lakers can't stop us': Heat (72.1%)\n",
      "\n",
      "'Jimmy and LeBron are the two best players out there': Heat (82.2%)\n",
      "\n",
      "'LeBron and Jimmy are the two best players out there': Heat (71.1%)\n",
      "\n",
      "'Wow, what a three pointer! On fire tonight': Lakers (98.5%)\n",
      "\n",
      "'Mark Jackson is the worst. Need to change commenters next year please': Heat (91.6%)\n",
      "\n",
      "'good good good': Lakers (83.2%)\n",
      "\n",
      "'bad bad bad': Heat (92.6%)\n",
      "\n",
      "'lakers heat lakers heat': Lakers (75.6%)\n",
      "\n",
      "'heat lakers heat lakers': Heat (94.3%)\n",
      "\n",
      "'neutral netural neutral': Heat (100.0%)\n",
      "\n",
      "'neutral comment without bias': Heat (57.9%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments = [\n",
    "    \"Lebron is the goat basketball player\",\n",
    "    \"Lebron is terrible. He flops. I hate him. Jordan is better\",\n",
    "    \"Jimmy Butler tries harder than everyone he deserves this\",\n",
    "    \"Duncan Robinson is a great shooter Lakers can't stop us\",\n",
    "    \"Jimmy and LeBron are the two best players out there\",\n",
    "    \"LeBron and Jimmy are the two best players out there\",\n",
    "    \"Wow, what a three pointer! On fire tonight\",\n",
    "    \"Mark Jackson is the worst. Need to change commenters next year please\",\n",
    "    \"good good good\",\n",
    "    \"bad bad bad\",\n",
    "    \"lakers heat lakers heat\",\n",
    "    \"heat lakers heat lakers\",\n",
    "    \"neutral netural neutral\",\n",
    "    \"neutral comment without bias\"\n",
    "]\n",
    "\n",
    "for comment in comments:\n",
    "    text_array = comment_to_numpy_array(comment)\n",
    "    predict_proba = basketball_net(torch.tensor(text_array)).item() * 100\n",
    "\n",
    "    if predict_proba >= 50:\n",
    "        prediction = \"Lakers\"\n",
    "    else:\n",
    "        prediction = \"Heat\"\n",
    "        predict_proba = 100 - predict_proba\n",
    "    \n",
    "    print(\"'\" + comment +\"': {} ({:.1f}%)\\n\".format(prediction, predict_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order obviously matters (see the last two comments). It mostly seems to be identifying \n",
    "players on each team, which is pretty cool! But also flawed, as you can see in the second\n",
    "comment which is critical of LeBron but still identifies the comment as being from a \n",
    "pro-Lakers fan. The percentages are also really high, which seems to indicate overfitting.\n",
    "It's funny to me that \"good good good\" is a Lakers comment while \"bad bad bad\" is predicted\n",
    "as a Heat comment. Maybe Heat fans were more negative overall this year? There certainly wasn't\n",
    "as much optimism surrounding them going into the season and Lakers fans are rather notoriously\n",
    "smug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get comments and predict\n",
    "\n",
    "That was fun, but let's now get the set of comments we want to analyze on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ····················\n"
     ]
    }
   ],
   "source": [
    "vault = Vault(getpass())\n",
    "with open('redditcreds.yml', 'r') as f:\n",
    "    reddit_creds = vault.load(f.read())\n",
    "\n",
    "reddit = praw.Reddit(username=reddit_creds['username'],\n",
    "                     user_agent=reddit_creds['user_agent'],\n",
    "                     client_id=reddit_creds['client_id'],\n",
    "                     client_secret=reddit_creds['client_secret'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAME THREAD: Los Angeles Lakers (52-19) @ Miami Heat (44-29) - (October 11, 2020)\n",
      "GAME THREAD: Miami Heat (44-29) @ Los Angeles Lakers (52-19) - (October 09, 2020)\n",
      "GAME THREAD: Miami Heat (44-29) @ Los Angeles Lakers (52-19) - (September 30, 2020)\n",
      "GAME THREAD: Los Angeles Lakers (52-19) @ Miami Heat (44-29) - (October 06, 2020)\n",
      "GAME THREAD: Los Angeles Lakers (52-19) @ Miami Heat (44-29) - (October 04, 2020)\n",
      "GAME THREAD: Miami Heat (44-29) @ Los Angeles Lakers (52-19) - (October 02, 2020)\n"
     ]
    }
   ],
   "source": [
    "subreddit = reddit.subreddit('nba')\n",
    "\n",
    "submissions = subreddit.search(query='title:\"GAME THREAD\"',\n",
    "                               time_filter=\"month\")\n",
    "game_threads = [\n",
    "    s for s in submissions \n",
    "    if\n",
    "        s.title[:11] == 'GAME THREAD' \n",
    "        and (\"Lakers\" in s.title and \"Heat\" in s.title)\n",
    "]\n",
    "\n",
    "for s in game_threads:\n",
    "    print(s.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was easy enough to get the comments with PRAW, so no need to rely on pushshift for this task.\n",
    "Now let's try to aggregate comments by user. First we have to realize more \n",
    "of the comments, though we won't do all of them. Recall that this takes awhile as we\n",
    "make repeated calls to the reddit API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "for game_thread in game_threads:\n",
    "    game_thread.comments.replace_more(limit=32, threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the comments that we care about, i.e., for users that are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAME THREAD: Los Angeles Lakers (52-19) @ Miami Heat (44-29) - (October 11, 2020)\n",
      "GAME THREAD: Miami Heat (44-29) @ Los Angeles Lakers (52-19) - (October 09, 2020)\n",
      "GAME THREAD: Miami Heat (44-29) @ Los Angeles Lakers (52-19) - (September 30, 2020)\n",
      "GAME THREAD: Los Angeles Lakers (52-19) @ Miami Heat (44-29) - (October 06, 2020)\n",
      "GAME THREAD: Los Angeles Lakers (52-19) @ Miami Heat (44-29) - (October 04, 2020)\n",
      "GAME THREAD: Miami Heat (44-29) @ Los Angeles Lakers (52-19) - (October 02, 2020)\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "def valid_user(flair):\n",
    "    \n",
    "    # want to ensure that user is flaired\n",
    "    if flair is None:\n",
    "        return False\n",
    "    \n",
    "    flair = flair.lower()\n",
    "    \n",
    "    if 'lal' in flair or 'mia' in flair \\\n",
    "        or 'lakers' in flair or 'heat' in flair:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "prediction_data = []\n",
    "\n",
    "for game in game_threads:\n",
    "    \n",
    "    print(game.title)\n",
    "    game_id = game.id\n",
    "    \n",
    "    for comment in game.comments:\n",
    "        \n",
    "        try:\n",
    "            author_id = comment.author.id\n",
    "            flair = comment.author_flair_text\n",
    "            comment_id = comment.id\n",
    "        except AttributeError:\n",
    "            continue\n",
    "            \n",
    "        if valid_user(flair):\n",
    "            text_array = comment_to_numpy_array(comment.body)\n",
    "\n",
    "            if text_array is not None and len(text_array) >= 3:\n",
    "                predict_proba = basketball_net(torch.tensor(text_array)).item()\n",
    "                \n",
    "                if predict_proba >= 0.5:\n",
    "                    prediction = 1\n",
    "                else:\n",
    "                    prediction = 0\n",
    "                \n",
    "                prediction_data += [(\n",
    "                    game_id,\n",
    "                    author_id,\n",
    "                    comment_id,\n",
    "                    predict_proba,\n",
    "                    prediction\n",
    "                )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/lakers_heat_prediction_data.pkl', 'wb') as f:\n",
    "    pickle.dump(prediction_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some easier analysis, we'll put this into a pandas `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cols = [\n",
    "    'game_id',\n",
    "    'author_id',\n",
    "    'comment_id',\n",
    "    'predict_proba',\n",
    "    'prediction'\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(prediction_data, columns=cols)\n",
    "\n",
    "# rename the prediction labels\n",
    "df['prediction_name'] = 'Lakers'\n",
    "df.loc[df['prediction']==0, 'prediction_name'] = 'Heat'\n",
    "\n",
    "# label the games by number (did some redacted printing of game_id's and titles)\n",
    "df['game'] = 0\n",
    "df.loc[df['game_id'] == 'j9eyw8', 'game'] = 6\n",
    "df.loc[df['game_id'] == 'j8ac6b', 'game'] = 5\n",
    "df.loc[df['game_id'] == 'j2xqua', 'game'] = 1\n",
    "df.loc[df['game_id'] == 'j6gk6h', 'game'] = 4\n",
    "df.loc[df['game_id'] == 'j57z5q', 'game'] = 3\n",
    "df.loc[df['game_id'] == 'j45t87', 'game'] = 2\n",
    "\n",
    "# record the winner\n",
    "df['winner'] = 'Lakers'\n",
    "df.loc[df['game']==3, 'winner'] = 'Heat'\n",
    "df.loc[df['game']==5, 'winner'] = 'Heat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our overtrained model thinks about the distribution of comments, whether it leans\n",
    "Lakers or Heat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of comments by affiliation:\n",
      "\n",
      "\tLakers: 65.8 %\n",
      "\tHeat: 34.2 %\n"
     ]
    }
   ],
   "source": [
    "comment_counts = df['prediction_name'].value_counts(True) * 100\n",
    "print('Percent of comments by affiliation:\\n')\n",
    "print('\\tLakers: {:.1f} %'.format(comment_counts['Lakers']))\n",
    "print('\\tHeat: {:.1f} %'.format(comment_counts['Heat']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least this result is in line with what we would have predicted. Let's plot a histogram of the\n",
    "predicted probabilities and see just how overtrained it is! (I'll admit that I wrote this line\n",
    "after I plotted the histogram.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "df['predict_proba'].hist(ax=ax)\n",
    "ax.set_title('Prediction Probabilities (1 = Lakers, 0 = Heat)')\n",
    "ax.set_ylabel('# comments')\n",
    "ax.set_xlabel('predict_proba')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So yeah, the model is way too confident in it's predictions. We might as well just look at the\n",
    "prediction labels and not the prediction probabilities. \n",
    "\n",
    "Let's see if we can figure out  what the opinions are by game and such. Do the comments change based on the winner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment classification in games won by the Heat:\n",
      "\n",
      "\tLakers: 67.7 %\n",
      "\tHeat: 32.3 %\n",
      "\n",
      "Comment classification in games won by the Lakers:\n",
      "\n",
      "\tLakers: 64.9 %\n",
      "\tHeat: 35.1 %\n"
     ]
    }
   ],
   "source": [
    "comment_counts = df[df['winner']=='Heat']['prediction_name'].value_counts(True) * 100\n",
    "print('Comment classification in games won by the Heat:\\n')\n",
    "print('\\tLakers: {:.1f} %'.format(comment_counts['Lakers']))\n",
    "print('\\tHeat: {:.1f} %'.format(comment_counts['Heat']))\n",
    "\n",
    "comment_counts = df[df['winner']=='Lakers']['prediction_name'].value_counts(True) * 100\n",
    "print('\\nComment classification in games won by the Lakers:\\n')\n",
    "print('\\tLakers: {:.1f} %'.format(comment_counts['Lakers']))\n",
    "print('\\tHeat: {:.1f} %'.format(comment_counts['Heat']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems backwards from what I would have predicted, but maybe the losing team does more complaining.\n",
    "Or maybe it's just random anyway.\n",
    "\n",
    "Let's dig into the most frequent commenters and see if we can deduce who they support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19v1ze      93\n",
       "xmlcpt2     54\n",
       "16xpvs      50\n",
       "2nu9liyg    48\n",
       "83t5t       44\n",
       "j7i90       41\n",
       "ylfg7       38\n",
       "7xx89       38\n",
       "tlosp       37\n",
       "j9wna       37\n",
       "Name: author_id, dtype: int64"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors = df['author_id'].value_counts()\n",
    "authors.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "prolific_commenters = authors[authors > 20]\n",
    "\n",
    "max_heat_pct = 0\n",
    "max_heat_author = ''\n",
    "max_lakers_pct = 0\n",
    "max_lakers_author = ''\n",
    "\n",
    "for author_id in prolific_commenters.index:\n",
    "    \n",
    "    counts = df[df['author_id']==author_id]['prediction_name'].value_counts()\n",
    "    \n",
    "    pct_lakers = 100.0 * counts['Lakers'] / authors[author_id]\n",
    "    pct_heat = 100 - pct_lakers\n",
    "    \n",
    "    if pct_lakers > max_lakers_pct:\n",
    "        max_lakers_pct = pct_lakers\n",
    "        max_lakers_author = author_id\n",
    "    \n",
    "    if pct_heat > max_heat_pct:\n",
    "        max_heat_pct = pct_heat\n",
    "        max_heat_author = author_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gitpage] *",
   "language": "python",
   "name": "conda-env-gitpage-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
