{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: code-post\n",
    "title: First look at PyTorch\n",
    "tags: [neural net]\n",
    "---\n",
    "\n",
    "So I've never played with PyTorch before, so I'm going to play around with \n",
    "some simple multilayer perceptrons to get started. Much of this code is adapted\n",
    "from PyTorchs' [60 minute bliz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) tutorial.\n",
    "This is a very simple walkthrough of creating a `DataSet` and then training a neural net on it. There's also\n",
    "an example of a class allowing for some variable architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a simple MLP\n",
    "\n",
    "In previous notebooks, we were playing around with neural nets that have the following\n",
    "architecture. The linear layers all have biases by default, but we an also set\n",
    "`bias=False` to remove those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, random_state=47):\n",
    "        super(Net, self).__init__()\n",
    "        torch.manual_seed(random_state)\n",
    "        self.fc1 = nn.Linear(2, 9)\n",
    "        self.fc2 = nn.Linear(9, 9)\n",
    "        self.fc3 = nn.Linear(9, 9)\n",
    "        self.fc4 = nn.Linear(9, 9)\n",
    "        self.fc5 = nn.Linear(9, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = torch.sigmoid(self.fc5(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see this `Net`'s parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.6321, -0.6365],\n",
       "         [-0.0457,  0.5313],\n",
       "         [ 0.0793,  0.4220],\n",
       "         [ 0.6728, -0.3561],\n",
       "         [-0.4993, -0.0926],\n",
       "         [ 0.2811,  0.5492],\n",
       "         [-0.3340, -0.3312],\n",
       "         [-0.5127, -0.0552],\n",
       "         [ 0.3450, -0.6575]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.5059, -0.1334,  0.0482,  0.1219, -0.4993, -0.2885, -0.3198,  0.3339,\n",
       "          0.5823], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.1811, -0.2940, -0.1910, -0.0098,  0.1716, -0.0090, -0.2765,  0.2010,\n",
       "           0.1643],\n",
       "         [-0.0962, -0.0630, -0.3145, -0.0620, -0.1136, -0.1233,  0.0276, -0.1000,\n",
       "          -0.1335],\n",
       "         [-0.0280, -0.1061, -0.0684, -0.1077, -0.0805, -0.2235, -0.3329,  0.0497,\n",
       "          -0.1480],\n",
       "         [-0.0531,  0.1734, -0.1789, -0.1540, -0.0209,  0.2476, -0.0416,  0.2050,\n",
       "          -0.2495],\n",
       "         [-0.1266,  0.1427,  0.2202, -0.1647,  0.1948, -0.3059, -0.0061, -0.1883,\n",
       "           0.2310],\n",
       "         [ 0.1373,  0.2318, -0.0628,  0.0181,  0.0913, -0.1387,  0.1729, -0.2129,\n",
       "          -0.3316],\n",
       "         [-0.2938, -0.1869,  0.0955,  0.1379, -0.2259, -0.1054, -0.0654,  0.2578,\n",
       "           0.2550],\n",
       "         [ 0.0694,  0.0563,  0.2023, -0.2261, -0.3202, -0.3294, -0.1565,  0.2440,\n",
       "           0.0995],\n",
       "         [-0.0856, -0.2808,  0.2253, -0.2386, -0.1659,  0.2512, -0.1394,  0.2062,\n",
       "           0.2192]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0364,  0.0177, -0.0671, -0.0629,  0.2630,  0.1507,  0.1259,  0.2261,\n",
       "          0.2479], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0164,  0.2709,  0.1827, -0.0299, -0.3086, -0.0359, -0.0419, -0.1654,\n",
       "           0.1594],\n",
       "         [-0.2598,  0.1595, -0.1115, -0.2831,  0.0947, -0.2015,  0.0895, -0.1406,\n",
       "          -0.1099],\n",
       "         [ 0.2737,  0.2374, -0.2414,  0.0338, -0.2378,  0.2125,  0.2424, -0.3081,\n",
       "          -0.1013],\n",
       "         [-0.1592,  0.3064, -0.3091,  0.0482, -0.1604, -0.1640,  0.0284, -0.1429,\n",
       "          -0.0736],\n",
       "         [-0.0347, -0.1987, -0.2923,  0.1233, -0.1856, -0.0249,  0.1784, -0.0934,\n",
       "          -0.2156],\n",
       "         [-0.2519, -0.1175,  0.2238, -0.0720,  0.2375, -0.2586, -0.3317, -0.0766,\n",
       "          -0.0132],\n",
       "         [-0.1333,  0.1775, -0.0158,  0.0148,  0.1501,  0.0394, -0.0820,  0.0427,\n",
       "          -0.2443],\n",
       "         [-0.1119, -0.2423, -0.1671,  0.1403, -0.1500,  0.3107,  0.2377, -0.1625,\n",
       "          -0.1476],\n",
       "         [ 0.0048, -0.2982, -0.1838, -0.0436,  0.2403,  0.3223, -0.1673,  0.3087,\n",
       "           0.2547]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2039,  0.1192,  0.2852, -0.0418,  0.1649,  0.1144,  0.0551, -0.1212,\n",
       "          0.2805], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.2038,  0.2893, -0.2738,  0.1934,  0.2750,  0.1062,  0.1668,  0.0617,\n",
       "          -0.2509],\n",
       "         [ 0.1988,  0.0586, -0.2811,  0.1281,  0.0124,  0.1549, -0.1912, -0.2525,\n",
       "           0.2383],\n",
       "         [ 0.2656,  0.0522,  0.3332, -0.0548, -0.0110,  0.0477, -0.0600,  0.0831,\n",
       "           0.2949],\n",
       "         [ 0.2794,  0.0648, -0.3260, -0.1705, -0.2997,  0.0134, -0.2370,  0.1158,\n",
       "          -0.2011],\n",
       "         [ 0.1341, -0.0625,  0.2552,  0.2959, -0.2620, -0.2471, -0.2127,  0.2490,\n",
       "          -0.2640],\n",
       "         [ 0.1694,  0.1014,  0.0625,  0.3167,  0.0342,  0.0496,  0.1624, -0.0332,\n",
       "           0.2074],\n",
       "         [-0.2492,  0.2484,  0.1048,  0.2532, -0.0296,  0.1778, -0.0860,  0.2848,\n",
       "           0.1285],\n",
       "         [-0.2977,  0.1723, -0.1206, -0.3330, -0.1580, -0.0370,  0.2655,  0.0722,\n",
       "           0.2115],\n",
       "         [-0.2625, -0.1298,  0.0753,  0.3242,  0.1949,  0.3222, -0.2359,  0.0084,\n",
       "           0.1131]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.3060,  0.2340,  0.2761,  0.0770, -0.1074,  0.2764,  0.0608,  0.1932,\n",
       "         -0.2089], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0924, -0.1799, -0.2314,  0.0416, -0.0254, -0.1123, -0.1199,  0.1387,\n",
       "           0.3063]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0768], requires_grad=True)]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4689],\n",
       "        [0.4699]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.tensor([[1., 2.], [2., 3.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "The cleanest way to use data with PyTorch -- I'm guessing here -- is to\n",
    "use the classes native to `torch.utils.data`, principally the `Dataset`\n",
    "(instead of say a numpy `Array` or a pandas `DataFrame`) and to load\n",
    "data from the dataset using a `DataLoader`. We find our data initially\n",
    "loaded into a pandas `DataFrame`, so we'll try to make a `Dataset` from\n",
    "it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_data(df, col_names=['x_1', 'x_2']):\n",
    "    \"\"\" return normalized x_1 and x_2 columns \"\"\"\n",
    "    return (df[col_names] - df[col_names].mean()) / df[col_names].std()\n",
    "\n",
    "def train_data(random_seed=3):\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    def rad_sq(array):\n",
    "        return array[:, 0]**2 + array[:, 1]**2\n",
    "\n",
    "    data_pos_ = np.random.normal(0, .75, size=(100, 2))\n",
    "    data_pos = data_pos_[rad_sq(data_pos_) < 4]\n",
    "\n",
    "    data_neg_ = np.random.uniform(-5, 5, size=(1000, 2))\n",
    "    data_neg = data_neg_[(rad_sq(data_neg_) > 6.25) & (rad_sq(data_neg_) < 16)]\n",
    "\n",
    "    data = np.concatenate((data_pos, data_neg), axis=0)\n",
    "    y = np.concatenate((np.ones(data_pos.shape[0]), np.zeros(data_neg.shape[0])), axis=0)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'x_1': data[:, 0]\n",
    "        ,'x_2': data[:, 1]\n",
    "        ,'const': 1.0\n",
    "        ,'y': y\n",
    "    })\n",
    "    \n",
    "    df[['x_1_norm', 'x_2_norm']] = normalize_data(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>const</th>\n",
       "      <th>y</th>\n",
       "      <th>x_1_norm</th>\n",
       "      <th>x_2_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.341471</td>\n",
       "      <td>0.327382</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.595074</td>\n",
       "      <td>0.205659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.072373</td>\n",
       "      <td>-1.397620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.001979</td>\n",
       "      <td>-0.650942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.208041</td>\n",
       "      <td>-0.266069</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.133901</td>\n",
       "      <td>-0.089037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.062056</td>\n",
       "      <td>-0.470251</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.065222</td>\n",
       "      <td>-0.190429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.032864</td>\n",
       "      <td>-0.357914</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.051488</td>\n",
       "      <td>-0.134645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        x_1       x_2  const    y  x_1_norm  x_2_norm\n",
       "0  1.341471  0.327382    1.0  1.0  0.595074  0.205659\n",
       "1  0.072373 -1.397620    1.0  1.0 -0.001979 -0.650942\n",
       "2 -0.208041 -0.266069    1.0  1.0 -0.133901 -0.089037\n",
       "3 -0.062056 -0.470251    1.0  1.0 -0.065222 -0.190429\n",
       "4 -0.032864 -0.357914    1.0  1.0 -0.051488 -0.134645"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_data()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TrainDataset` class will subclass `Dataset`. We are required to fill\n",
    "in the `__init__`, `__len__`, and `__getitem__` functions. We can then \n",
    "call `DataLoader` with this dataset, and it will know how to batch\n",
    "and shuffle through the dataset. We are relying on the fact that `DataFrame`\n",
    "can retrieve data by relying on numerical indexes. If we do not have indexes\n",
    "but instead only an iterable list, we could have used `IterableDataset` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        \"\"\" df contains the data that we want to use\n",
    "        it should have x_1_norm, x_2_norm, and y columns \"\"\"\n",
    "        self.df = df[['x_1_norm', 'x_2_norm', 'y']].copy()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        samples = self.df[['x_1_norm', 'x_2_norm']].iloc[idx].values\n",
    "        labels = self.df[['y']].iloc[idx].values\n",
    "        \n",
    "        return samples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TrainDataset(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out a random sample from this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset: 393\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.6871,  1.7872],\n",
       "         [ 1.0827, -0.3425],\n",
       "         [ 1.1661, -0.5120],\n",
       "         [ 0.6863, -1.4712]], dtype=torch.float64),\n",
       " tensor([[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]], dtype=torch.float64)]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader_4 = DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "trainiter = iter(trainloader)\n",
    "\n",
    "print('length of dataset:', len(trainset))\n",
    "trainiter.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Net\n",
    "\n",
    "With a neural net and training dataset ready to go, let's train the neural net\n",
    "using the Adam optimizer, which we've imported from `torch.optim`. We will\n",
    "use the Mean Squared Error loss. We'll replace the trainloader from above\n",
    "with one which ha batch size as the max size possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 - loss: 0.240 - classification error: 0.249\n",
      "epoch: 2 - loss: 0.231 - classification error: 0.249\n",
      "epoch: 3 - loss: 0.226 - classification error: 0.249\n",
      "epoch: 4 - loss: 0.219 - classification error: 0.249\n",
      "epoch: 5 - loss: 0.246 - classification error: 0.249\n",
      "epoch: 6 - loss: 0.214 - classification error: 0.249\n",
      "epoch: 7 - loss: 0.215 - classification error: 0.249\n",
      "epoch: 8 - loss: 0.205 - classification error: 0.249\n",
      "epoch: 9 - loss: 0.215 - classification error: 0.249\n",
      "epoch: 10 - loss: 0.238 - classification error: 0.249\n",
      "epoch: 11 - loss: 0.168 - classification error: 0.249\n",
      "epoch: 12 - loss: 0.184 - classification error: 0.249\n",
      "epoch: 13 - loss: 0.191 - classification error: 0.249\n",
      "epoch: 14 - loss: 0.169 - classification error: 0.249\n",
      "epoch: 15 - loss: 0.166 - classification error: 0.249\n",
      "epoch: 16 - loss: 0.187 - classification error: 0.249\n",
      "epoch: 17 - loss: 0.174 - classification error: 0.249\n",
      "epoch: 18 - loss: 0.139 - classification error: 0.249\n",
      "epoch: 19 - loss: 0.173 - classification error: 0.249\n",
      "epoch: 20 - loss: 0.161 - classification error: 0.249\n",
      "epoch: 21 - loss: 0.204 - classification error: 0.249\n",
      "epoch: 22 - loss: 0.166 - classification error: 0.249\n",
      "epoch: 23 - loss: 0.152 - classification error: 0.249\n",
      "epoch: 24 - loss: 0.176 - classification error: 0.249\n",
      "epoch: 25 - loss: 0.175 - classification error: 0.249\n",
      "epoch: 26 - loss: 0.207 - classification error: 0.249\n",
      "epoch: 27 - loss: 0.148 - classification error: 0.249\n",
      "epoch: 28 - loss: 0.166 - classification error: 0.249\n",
      "epoch: 29 - loss: 0.175 - classification error: 0.249\n",
      "epoch: 30 - loss: 0.163 - classification error: 0.249\n",
      "epoch: 31 - loss: 0.113 - classification error: 0.249\n",
      "epoch: 32 - loss: 0.098 - classification error: 0.249\n",
      "epoch: 33 - loss: 0.115 - classification error: 0.249\n",
      "epoch: 34 - loss: 0.205 - classification error: 0.249\n",
      "epoch: 35 - loss: 0.146 - classification error: 0.249\n",
      "epoch: 36 - loss: 0.134 - classification error: 0.249\n",
      "epoch: 37 - loss: 0.151 - classification error: 0.249\n",
      "epoch: 38 - loss: 0.114 - classification error: 0.249\n",
      "epoch: 39 - loss: 0.165 - classification error: 0.249\n",
      "epoch: 40 - loss: 0.132 - classification error: 0.249\n",
      "epoch: 41 - loss: 0.102 - classification error: 0.249\n",
      "epoch: 42 - loss: 0.092 - classification error: 0.249\n",
      "epoch: 43 - loss: 0.092 - classification error: 0.249\n",
      "epoch: 44 - loss: 0.114 - classification error: 0.249\n",
      "epoch: 45 - loss: 0.072 - classification error: 0.249\n",
      "epoch: 46 - loss: 0.088 - classification error: 0.249\n",
      "epoch: 47 - loss: 0.107 - classification error: 0.176\n",
      "epoch: 48 - loss: 0.067 - classification error: 0.132\n",
      "epoch: 49 - loss: 0.074 - classification error: 0.097\n",
      "epoch: 50 - loss: 0.082 - classification error: 0.069\n",
      "epoch: 51 - loss: 0.069 - classification error: 0.056\n",
      "epoch: 52 - loss: 0.060 - classification error: 0.046\n",
      "epoch: 53 - loss: 0.045 - classification error: 0.038\n",
      "epoch: 54 - loss: 0.065 - classification error: 0.025\n",
      "epoch: 55 - loss: 0.047 - classification error: 0.020\n",
      "epoch: 56 - loss: 0.034 - classification error: 0.020\n",
      "epoch: 57 - loss: 0.041 - classification error: 0.015\n",
      "epoch: 58 - loss: 0.057 - classification error: 0.015\n",
      "epoch: 59 - loss: 0.046 - classification error: 0.013\n",
      "epoch: 60 - loss: 0.041 - classification error: 0.010\n",
      "epoch: 61 - loss: 0.039 - classification error: 0.008\n",
      "epoch: 62 - loss: 0.036 - classification error: 0.005\n",
      "epoch: 63 - loss: 0.028 - classification error: 0.005\n",
      "epoch: 64 - loss: 0.030 - classification error: 0.003\n",
      "epoch: 65 - loss: 0.028 - classification error: 0.000\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "net = Net()\n",
    "trainloader = DataLoader(trainset, batch_size=50, shuffle=True, num_workers=0)\n",
    "optimizer = Adam(net.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(1000): \n",
    "\n",
    "    running_loss = 0.0\n",
    "    num_wrong = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        samples, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(samples.float())\n",
    "        loss = criterion(outputs.float(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predictions = 1 * (outputs >= 0.5) + 0 * (outputs < 0.5)\n",
    "        \n",
    "        num_wrong += torch.sum(torch.abs(predictions - labels))\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    class_error = num_wrong / len(trainset)\n",
    "    print('epoch: {} - loss: {:.3f} - classification error: {:.3f}'.format(epoch+1, loss, class_error))\n",
    "    \n",
    "    if class_error == 0.0:\n",
    "        break\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it, we finished training in 65 epochs when we achieved 0 classification error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable architecture Net\n",
    "\n",
    "Can we create a class that allows us to specify the number and width of\n",
    "layers upon creation? Apparently we have to use the `add_module` function \n",
    "to do this, as the layers are not connected explicitly to `self`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLuNet(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, layer_widths, random_state=47):\n",
    "        \"\"\" layer_widths should include the input and out layer widths.\"\"\"\n",
    "        torch.manual_seed(random_state)\n",
    "        super(ReLuNet, self).__init__()\n",
    "        self.layers = [\n",
    "            nn.Linear(layer_widths[i], layer_widths[i+1])\n",
    "            for i in range(len(layer_widths)-1)\n",
    "        ]\n",
    "        for i in range(len(self.layers)):\n",
    "            self.add_module(\"hidden layer \" + str(i), self.layers[i])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers)-1):\n",
    "            x = F.relu(self.layers[i](x))\n",
    "        x = torch.sigmoid(self.layers[-1](x))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_net = ReLuNet([2, 9, 9, 9, 9, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.6321, -0.6365],\n",
       "         [-0.0457,  0.5313],\n",
       "         [ 0.0793,  0.4220],\n",
       "         [ 0.6728, -0.3561],\n",
       "         [-0.4993, -0.0926],\n",
       "         [ 0.2811,  0.5492],\n",
       "         [-0.3340, -0.3312],\n",
       "         [-0.5127, -0.0552],\n",
       "         [ 0.3450, -0.6575]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.5059, -0.1334,  0.0482,  0.1219, -0.4993, -0.2885, -0.3198,  0.3339,\n",
       "          0.5823], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.1811, -0.2940, -0.1910, -0.0098,  0.1716, -0.0090, -0.2765,  0.2010,\n",
       "           0.1643],\n",
       "         [-0.0962, -0.0630, -0.3145, -0.0620, -0.1136, -0.1233,  0.0276, -0.1000,\n",
       "          -0.1335],\n",
       "         [-0.0280, -0.1061, -0.0684, -0.1077, -0.0805, -0.2235, -0.3329,  0.0497,\n",
       "          -0.1480],\n",
       "         [-0.0531,  0.1734, -0.1789, -0.1540, -0.0209,  0.2476, -0.0416,  0.2050,\n",
       "          -0.2495],\n",
       "         [-0.1266,  0.1427,  0.2202, -0.1647,  0.1948, -0.3059, -0.0061, -0.1883,\n",
       "           0.2310],\n",
       "         [ 0.1373,  0.2318, -0.0628,  0.0181,  0.0913, -0.1387,  0.1729, -0.2129,\n",
       "          -0.3316],\n",
       "         [-0.2938, -0.1869,  0.0955,  0.1379, -0.2259, -0.1054, -0.0654,  0.2578,\n",
       "           0.2550],\n",
       "         [ 0.0694,  0.0563,  0.2023, -0.2261, -0.3202, -0.3294, -0.1565,  0.2440,\n",
       "           0.0995],\n",
       "         [-0.0856, -0.2808,  0.2253, -0.2386, -0.1659,  0.2512, -0.1394,  0.2062,\n",
       "           0.2192]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0364,  0.0177, -0.0671, -0.0629,  0.2630,  0.1507,  0.1259,  0.2261,\n",
       "          0.2479], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0164,  0.2709,  0.1827, -0.0299, -0.3086, -0.0359, -0.0419, -0.1654,\n",
       "           0.1594],\n",
       "         [-0.2598,  0.1595, -0.1115, -0.2831,  0.0947, -0.2015,  0.0895, -0.1406,\n",
       "          -0.1099],\n",
       "         [ 0.2737,  0.2374, -0.2414,  0.0338, -0.2378,  0.2125,  0.2424, -0.3081,\n",
       "          -0.1013],\n",
       "         [-0.1592,  0.3064, -0.3091,  0.0482, -0.1604, -0.1640,  0.0284, -0.1429,\n",
       "          -0.0736],\n",
       "         [-0.0347, -0.1987, -0.2923,  0.1233, -0.1856, -0.0249,  0.1784, -0.0934,\n",
       "          -0.2156],\n",
       "         [-0.2519, -0.1175,  0.2238, -0.0720,  0.2375, -0.2586, -0.3317, -0.0766,\n",
       "          -0.0132],\n",
       "         [-0.1333,  0.1775, -0.0158,  0.0148,  0.1501,  0.0394, -0.0820,  0.0427,\n",
       "          -0.2443],\n",
       "         [-0.1119, -0.2423, -0.1671,  0.1403, -0.1500,  0.3107,  0.2377, -0.1625,\n",
       "          -0.1476],\n",
       "         [ 0.0048, -0.2982, -0.1838, -0.0436,  0.2403,  0.3223, -0.1673,  0.3087,\n",
       "           0.2547]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2039,  0.1192,  0.2852, -0.0418,  0.1649,  0.1144,  0.0551, -0.1212,\n",
       "          0.2805], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.2038,  0.2893, -0.2738,  0.1934,  0.2750,  0.1062,  0.1668,  0.0617,\n",
       "          -0.2509],\n",
       "         [ 0.1988,  0.0586, -0.2811,  0.1281,  0.0124,  0.1549, -0.1912, -0.2525,\n",
       "           0.2383],\n",
       "         [ 0.2656,  0.0522,  0.3332, -0.0548, -0.0110,  0.0477, -0.0600,  0.0831,\n",
       "           0.2949],\n",
       "         [ 0.2794,  0.0648, -0.3260, -0.1705, -0.2997,  0.0134, -0.2370,  0.1158,\n",
       "          -0.2011],\n",
       "         [ 0.1341, -0.0625,  0.2552,  0.2959, -0.2620, -0.2471, -0.2127,  0.2490,\n",
       "          -0.2640],\n",
       "         [ 0.1694,  0.1014,  0.0625,  0.3167,  0.0342,  0.0496,  0.1624, -0.0332,\n",
       "           0.2074],\n",
       "         [-0.2492,  0.2484,  0.1048,  0.2532, -0.0296,  0.1778, -0.0860,  0.2848,\n",
       "           0.1285],\n",
       "         [-0.2977,  0.1723, -0.1206, -0.3330, -0.1580, -0.0370,  0.2655,  0.0722,\n",
       "           0.2115],\n",
       "         [-0.2625, -0.1298,  0.0753,  0.3242,  0.1949,  0.3222, -0.2359,  0.0084,\n",
       "           0.1131]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.3060,  0.2340,  0.2761,  0.0770, -0.1074,  0.2764,  0.0608,  0.1932,\n",
       "         -0.2089], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0924, -0.1799, -0.2314,  0.0416, -0.0254, -0.1123, -0.1199,  0.1387,\n",
       "           0.3063]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0768], requires_grad=True)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(relu_net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And yeah, this works and we can create predictions. (The prediction is the \n",
    "same as the initial state of the `net` created above since we used\n",
    "the same random state.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4689], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_net(torch.tensor([1., 2.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the random state of course can be different and produces\n",
    "different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4216], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_net_2 = ReLuNet([2, 9, 9, 9, 9, 1], random_state=1)\n",
    "relu_net_2(torch.tensor([1., 2.]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gitpage] *",
   "language": "python",
   "name": "conda-env-gitpage-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
