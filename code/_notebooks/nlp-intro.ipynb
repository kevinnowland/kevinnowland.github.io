{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: code-post\n",
    "title: NLP Basic Processing\n",
    "description: This is just me playing around with basic NLP packages.\n",
    "tags: [misc]\n",
    "---\n",
    "\n",
    "Though I'm interested in NLP, I've never actually played around with NLP models\n",
    "or any of the packages and basic techniques that NLP relies on. So here I'm\n",
    "just going to play a bit with spaCy and some a basic algorithm or two.\n",
    "\n",
    "Outline:\n",
    "- spaCy\n",
    "- nltk\n",
    "- BPE and wordpiece\n",
    "\n",
    "## spaCy\n",
    "\n",
    "For my introduction to spaCy, I'm pretty much just running through the \n",
    "[spaCy introduction](https://spacy.io/usage/spacy-101). Truly there is\n",
    "not much reason to read this section, the actual spaCy documentation \n",
    "is much more interesting and likely to be correct! I will point out\n",
    "that I quickly ran into one of the limitations of using their pretrained\n",
    "models without fine-tuning them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see what we can do with a corpus made from the first chapter of Camus' \"La peste\".\n",
    "I physically copied the text from [here](https://www.ebooksgratuits.com/html/camus_la_peste.html#_Toc284769571),\n",
    "though I could've with some difficulty used `beautifulsoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/la_peste_1.txt\", \"r\") as f:\n",
    "    text = f.read().replace(\"\\n\", \" \")\n",
    "    \n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Les curieux événements qui font le sujet de cette chronique se sont produits en 194., à Oran. De l’avis général, ils n’y étaient pas à leur place, sortant un peu de l’ordinaire. À première vue, Oran est, en effet, une ville ordinaire et rien de plus qu’une préfecture française de la côte algérienne.    La cité elle-même, on doit l’avouer, est laide. D’aspect tranquille, il faut quelque temps pour apercevoir ce qui la rend différente de tant d’autres villes"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy does a lot of things including part of speech tagging and lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les DET le\n",
      "curieux ADJ curieux\n",
      "événements NOUN événement\n",
      "qui PRON qui\n",
      "font VERB faire\n",
      "le DET le\n",
      "sujet NOUN sujet\n",
      "de ADP de\n",
      "cette DET ce\n",
      "chronique NOUN chronique\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(doc[i].text, doc[i].pos_, doc[i].lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get named entities by called the `.ents` property of a document. The list of entity labels can be found [here](https://spacy.io/api/annotation#named-entities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oran - LOC - Non-GPE locations, mountain ranges, bodies of water\n",
      "Oran - LOC - Non-GPE locations, mountain ranges, bodies of water\n",
      "Oran - LOC - Non-GPE locations, mountain ranges, bodies of water\n",
      "Oran - LOC - Non-GPE locations, mountain ranges, bodies of water\n",
      "Oran - LOC - Non-GPE locations, mountain ranges, bodies of water\n",
      "Arrivé - LOC - Non-GPE locations, mountain ranges, bodies of water\n",
      "Bernard Rieux - PER - Named person or family.\n",
      "M. Michel - PER - Named person or family.\n",
      "M. Michel - PER - Named person or family.\n",
      "Bernard Rieux - PER - Named person or family.\n",
      "Rieux - LOC - Non-GPE locations, mountain ranges, bodies of water\n",
      "Dors si tu peux - MISC - Miscellaneous entities, e.g. events, nationalities, products or works of art\n"
     ]
    }
   ],
   "source": [
    "for entity in doc[:2000].ents:\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that last example shows that the named entities are not perfect. \"Dors si tu peux\" is just a \n",
    "normal phrase. A named entity will span multiple tokens, it's not a special tag of a single token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Le matin"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1524:1526]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the results are very different if you lowercase the results first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/la_peste_1.txt\", \"r\") as f:\n",
    "    text_lower = f.read().replace(\"\\n\", \" \").lower()\n",
    "    \n",
    "doc_lower = nlp(text_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oran - LOC - Non-GPE locations, mountain ranges, bodies of water\n",
      "oran - LOC - Non-GPE locations, mountain ranges, bodies of water\n",
      "laide - PER - Named person or family.\n",
      "le soleil incendie - MISC - Miscellaneous entities, e.g. events, nationalities, products or works of art\n",
      "les beaux jours - MISC - Miscellaneous entities, e.g. events, nationalities, products or works of art\n",
      "boulevard - PER - Named person or family.\n",
      "boulomanes - LOC - Non-GPE locations, mountain ranges, bodies of water\n",
      "amicales - LOC - Non-GPE locations, mountain ranges, bodies of water\n",
      "oran - PER - Named person or family.\n",
      "oran - LOC - Non-GPE locations, mountain ranges, bodies of water\n",
      "oran - LOC - Non-GPE locations, mountain ranges, bodies of water\n",
      "bernard - PER - Named person or family.\n",
      "m. michel - PER - Named person or family.\n",
      "m. michel - PER - Named person or family.\n",
      "bernard rieux - PER - Named person or family.\n",
      "dors si tu peux - MISC - Miscellaneous entities, e.g. events, nationalities, products or works of art\n",
      "au train de midi - MISC - Miscellaneous entities, e.g. events, nationalities, products or works of art\n"
     ]
    }
   ],
   "source": [
    "for entity in doc_lower[:2000].ents:\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one is going to rely on named entities, then it appears that lowercasing is a hindrance, which makes\n",
    "sense since in French, as in English, uppercasing is used to denote proper names. Let's go back to the\n",
    "upperacased version but use the full model and see if its predictions about named entities do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_large = spacy.load(\"fr_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_large = nlp_large(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oran - LOC - Non-GPE locations, mountain ranges, bodies of water\n",
      "Oran - LOC - Non-GPE locations, mountain ranges, bodies of water\n",
      "Oran - LOC - Non-GPE locations, mountain ranges, bodies of water\n",
      "Oran - LOC - Non-GPE locations, mountain ranges, bodies of water\n",
      "Oran - LOC - Non-GPE locations, mountain ranges, bodies of water\n",
      "Bernard Rieux - PER - Named person or family.\n",
      "M. Michel - PER - Named person or family.\n",
      "M. Michel - PER - Named person or family.\n",
      "Bernard Rieux - PER - Named person or family.\n",
      "Rieux - PER - Named person or family.\n",
      "Dors si tu peux - MISC - Miscellaneous entities, e.g. events, nationalities, products or works of art\n"
     ]
    }
   ],
   "source": [
    "for entity in doc_large[:2000].ents:\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still has \"Dors si tu peux\" (sleep if you can) but it did get rid of Arrivé which is a past participle.\n",
    "If we look up the line containing the incorrectly labelled phrase, we find the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dors si tu peux, dit-il."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_large[1956:1964]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which shows that this is clearly a phrase (\"Sleep if you can, he said\"). Apparently it is possible to train from scratch or fine-tune the models that spaCy uses to predict all these labels and is described in the [documentation](https://spacy.io/usage/training).\n",
    "\n",
    "Let's call the visualizer, since that's something that spaCy has built in apparently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"fr\" id=\"aba7cfb585ae4caf9eb68f00cde8f5b2-0\" class=\"displacy\" width=\"1100\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Dors</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">si</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">SCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">tu</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">peux,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">dit</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">-il.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aba7cfb585ae4caf9eb68f00cde8f5b2-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,2.0 750.0,2.0 750.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aba7cfb585ae4caf9eb68f00cde8f5b2-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aba7cfb585ae4caf9eb68f00cde8f5b2-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,177.0 565.0,177.0 565.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aba7cfb585ae4caf9eb68f00cde8f5b2-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aba7cfb585ae4caf9eb68f00cde8f5b2-0-2\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aba7cfb585ae4caf9eb68f00cde8f5b2-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aba7cfb585ae4caf9eb68f00cde8f5b2-0-3\" stroke-width=\"2px\" d=\"M70,352.0 C70,89.5 570.0,89.5 570.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aba7cfb585ae4caf9eb68f00cde8f5b2-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,354.0 L578.0,342.0 562.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aba7cfb585ae4caf9eb68f00cde8f5b2-0-4\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aba7cfb585ae4caf9eb68f00cde8f5b2-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M910.0,354.0 L918.0,342.0 902.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(nlp(\"Dors si tu peux, dit-il.\"), style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue seems to be that it is not recognizing \"dors\" as the imperative of \"dormir.\" If\n",
    "we change to the \"vous\" form from the \"tu\" form we get the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"fr\" id=\"1fa187103cf54477bf6058017097c845-0\" class=\"displacy\" width=\"1100\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Dormez</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">si</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">SCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">tu</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">peux,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">dit</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">-il</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1fa187103cf54477bf6058017097c845-0-0\" stroke-width=\"2px\" d=\"M245,352.0 C245,177.0 565.0,177.0 565.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1fa187103cf54477bf6058017097c845-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1fa187103cf54477bf6058017097c845-0-1\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1fa187103cf54477bf6058017097c845-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1fa187103cf54477bf6058017097c845-0-2\" stroke-width=\"2px\" d=\"M70,352.0 C70,89.5 570.0,89.5 570.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1fa187103cf54477bf6058017097c845-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,354.0 L578.0,342.0 562.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1fa187103cf54477bf6058017097c845-0-3\" stroke-width=\"2px\" d=\"M70,352.0 C70,2.0 750.0,2.0 750.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1fa187103cf54477bf6058017097c845-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,354.0 L758.0,342.0 742.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1fa187103cf54477bf6058017097c845-0-4\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1fa187103cf54477bf6058017097c845-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M910.0,354.0 L918.0,342.0 902.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(nlp(\"Dormez si tu peux, dit-il\"), style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although if we go to the entity visualizer it still does not understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Dors si tu peux\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       ", dit-il</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Dormez si vous pouvez\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       ", dit-il</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(nlp(\"Dors si tu peux, dit-il\"), style=\"ent\")\n",
    "displacy.render(nlp(\"Dormez si vous pouvez, dit-il\"), style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these are the two things that displaCy can do. Does make for some pretty output!\n",
    "\n",
    "The French models come with language vectors (probably produced by word2vec?) that we can see if we wanted by calling `token.vector` where `token` is a token from a  `doc` object. However, being simply a length 300 numpy array, it's not really interesting to look at\\. The tokens can also get similarity scores between its vector and another word's vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between roi and reine: 0.6281081\n",
      "Similarity between roi and renard: 0.2194891\n"
     ]
    }
   ],
   "source": [
    "small_doc = nlp(\"roi reine renard\")\n",
    "print(\"Similarity between roi and reine:\", small_doc[0].similarity(small_doc[1]))\n",
    "print(\"Similarity between roi and renard:\", small_doc[0].similarity(small_doc[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "\n",
    "From what I can surmise from googling, spaCy and NLTK take different approaches. An app developer is probably going to use spaCy as it provides out of the box ready algorithms for the problems at hand with not many choices. NLTK is more useful to researchers who are trying to build from the ground up, but it's trickier to adapt to well worn problems.\n",
    "\n",
    "After installing nltk with conda, you can't just immediately call functions such as `sent_tokenize` as you will be\n",
    "presented with a `LookupError` saying it can't find some resource that it is looking for.\n",
    "This is because nltk is much more customizable and must be customized before running. Data installation instructions\n",
    "can be found [here](https://www.nltk.org/data.html). I'll try to keep track of what I downloaded as I go. To\n",
    "download via GUI, run `nltk.download()` which will open up an interface in a new window and you an download\n",
    "everything or pieces. On my Mac it downloaded to `~/nltk_data`.\n",
    "\n",
    "For sentence level tokenization, we rely on punkt, which I installed from an interactive python shell I opened in the\n",
    "terminal via `nltk.download('punkt)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/french.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les curieux événements qui font le sujet de cette chronique se sont produits en 194., à Oran.\n",
      "De l’avis général, ils n’y étaient pas à leur place, sortant un peu de l’ordinaire.\n",
      "À première vue, Oran est, en effet, une ville ordinaire et rien de plus qu’une préfecture française de la côte algérienne.\n",
      "La cité elle-même, on doit l’avouer, est laide.\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'De l’avis général, ils n’y étaient pas à leur place, sortant un peu de l’ordinaire.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently punkt is just a sentence level tokenizer. But there are many options! Here's one that\n",
    "apparently was implemented / invented by someone from my alma mater of OSU. It seems to just be\n",
    "based on regexes from the [documentation](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.toktok).\n",
    "There was another regex based tokenizer called the Moses Tokenizer, but it was removed due to an\n",
    "incompatible license. It still comes up when you search on stack overflow (via google!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Les', 'curieux', 'événements', 'qui', 'font', 'le', 'sujet', 'de']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "toktok = ToktokTokenizer()\n",
    "print(toktok.tokenize(sentences[0])[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization seems much harder to find in nltk. There is the `WordNetLemmatizer` that exists in\n",
    "`nltk.stem`, but it is English only and even then the results seem dubious (based on googling).\n",
    "It is eassier to find Stemmers, as the snowball stemmer has a French version, but these\n",
    "do not really work in the desired way. One can download an external lemmatizer\n",
    "such as the [FrenchLefffLematizer](https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer) which\n",
    "already exists in the spaCy universe via [spacy-leff](https://spacy.io/universe/project/spacy-lefff).\n",
    "Here's an example of the French snowball stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les - le\n",
      "curieux - curieux\n",
      "événements - éven\n",
      "qui - qui\n",
      "font - font\n",
      "le - le\n",
      "sujet - sujet\n",
      "de - de\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"french\")\n",
    "words = toktok.tokenize(sentences[0])[:8]\n",
    "for word in words:\n",
    "    print(word, \"-\", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So stemming is obviously not great! This makes sense since it's based on characters and removing them and not really grammatical\n",
    "except where the grammar is reflected very simply at the character level. Even a semi-inaccurate lemmatizer\n",
    "would probably be better in a lot of cases, I would think, now that lemmatizers are probably decently\n",
    "cheap to train.\n",
    "\n",
    "It also seems that NLTK does not have a built in way to handle Named Entity Recognition for French, although\n",
    "it does for English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't deal with stopwords in spaCy, though we could have. Let's do it in nltk, showing again how it is language specific as well.\n",
    "This required to `nltk.download('stopwords')`. Then the stopwords appear to just be a list that onee can compare against\n",
    "using, e.g., list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('french')[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it's clear already that I'll probably be using spaCy if I want to work with French text.\n",
    "Given the different goals of the projects, it's understandable why ntlk makes the choices it does,\n",
    "it probably won't suit me as often. Or, perhaps more likley, everything will be custom, which\n",
    "seems to be likely with language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE and WordPiece\n",
    "\n",
    "Above we've been working at the sentence and word level, but the very large transformers that are currently en vogue are \n",
    "based on something more akin to the character level. It's not quite accurate, as the GPT family of models rely\n",
    "on Byte Pair Encodings (BPE) while BERT uses WordPiece, both of which we try to explain shortly.\n",
    "\n",
    "### BPE\n",
    "\n",
    "BPE is relatively simple to explain. Take a corpus and break it into distince words, recording\n",
    "their frequencies. Split each word into characters while also appending a stop word symbol to\n",
    "the last letter. At the beginning, each letter or letter+stop is considered as a byte.\n",
    "Iterate over the text doing the following. Look for the most frequent pair of consecutive\n",
    "bytes and combine them into a new byte. Do this until you are satisfied that the bytes\n",
    "are appropriately descriptive. Letting the tokenizer continue forever results in each\n",
    "word being its own byte, so BPE interpolates between letters and words. However, as stated\n",
    "previously, orthography and grammar are only somewhat related, so BPE does have some drawbacks.\n",
    "According to [HuggingFace](https://huggingface.co/transformers/tokenizer_summary.html), GPT-2 \n",
    "stopped increasing the vocabulary after 40,000 merges. BPE was presented in 1995 by \n",
    "[P. Gage](https://dl.acm.org/doi/10.5555/177910.177914), although the GPT-2 paper references \n",
    "[Sennrich et al](https://arxiv.org/pdf/1508.07909.pdf) which presents algorithm explicitly\n",
    "in python. Sennrich has also written the [subword-nmt package](https://github.com/rsennrich/subword-nmt)\n",
    "which implemeents BPE.\n",
    "\n",
    "\n",
    "### WordPiece\n",
    "\n",
    "WordPiece is a generalization of BPE. The difference lies in how you choose tokens to merge.\n",
    "At each step, you must have a model trained on the data. The chosen byte merge is the one\n",
    "which most improves the model. For example, say the model is a trigram which predicts the\n",
    "next byte from the preovious two bytes. Then you would merge the two bytes which most\n",
    "improve the model upon merging (once its retrained, I believe). This could clearly be\n",
    "much more computationally intensive depending on the model. Using WordPiece with a digram\n",
    "recovers BPE, I think.\n",
    "\n",
    "### BPE Example\n",
    "\n",
    "Let's see what vocab we get using the first chapter of La Peste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This corpus has 85190 words.\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/la_peste_1.txt\", \"r\") as f:\n",
    "    text = f.read().replace(\"\\n\", \" \")\n",
    "    \n",
    "print(\"This corpus has {} words.\".format(len(text.split(' '))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean the text by lowercasing and removing punctuation, keeping in mind of\n",
    "course that French has letters with accents. I believe we should leave the\n",
    "quote mark that comes with contractions as well as the dash in words\n",
    "such as \"elle-même.\" I think this means that we just want to remove\n",
    "periods, commas, and any punctionation that occurs in a word without\n",
    "other letters. We also have to end each word with the special end of word\n",
    "character \"\\</w>\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This corpus has 10318 distinct words.\n"
     ]
    }
   ],
   "source": [
    "text_ = text.lower()\n",
    "\n",
    "weird_chars = re.compile(r'[\\?;\\(\\)\\\\.,\\!:–»«]') # Removing En dash not Em dash\n",
    "text_ = weird_chars.sub(\" \", text_)\n",
    "\n",
    "def prepare_word(word):\n",
    "    \"\"\" take a word and add spaces between letters and add the\n",
    "    end of word symbol </w>\"\"\"\n",
    "    return \"\".join(l + \" \" for l in word) + \"</w>\"\n",
    "\n",
    "spaces = re.compile(\"\\s+\")\n",
    "words = [prepare_word(w) for w in spaces.split(text_)]\n",
    "\n",
    "vocab = dict(collections.Counter(words))\n",
    "\n",
    "print(\"This corpus has {} distinct words.\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we steal the code from the Sennrich paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's do some number of merges. Maybe 500?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "vocab_new = copy.deepcopy(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_merges = 500\n",
    "merge_pairs = []\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab_new)\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    merge_pairs += [best]\n",
    "    vocab_new = merge_vocab(best, vocab_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de</w> - 3370\t\tplus</w> - 464\t\test</w> - 231\t\tavaient</w> - 146\n",
      "la</w> - 2450\t\tlui</w> - 462\t\tcomme</w> - 230\t\ttoujours</w> - 145\n",
      "et</w> - 2212\t\tétait</w> - 442\t\tses</w> - 230\t\tdeux</w> - 145\n",
      "le</w> - 2098\t\tsur</w> - 407\t\tsi</w> - 220\t\taux</w> - 142\n",
      "les</w> - 1735\t\ton</w> - 398\t\trambert</w> - 206\t\trien</w> - 140\n",
      "à</w> - 1590\t\tje</w> - 388\t\toù</w> - 205\t\tmoment</w> - 140\n",
      "il</w> - 1314\t\tson</w> - 385\t\telle</w> - 192\t\tpouvait</w> - 138\n",
      "que</w> - 1182\t\ttout</w> - 356\t\tpeu</w> - 188\t\taprès</w> - 132\n",
      "des</w> - 1027\t\tavec</w> - 352\t\tencore</w> - 185\t\tvers</w> - 132\n",
      "dans</w> - 879\t\tdit</w> - 343\t\tcela</w> - 182\t\taussi</w> - 131\n",
      "qui</w> - 858\t\tils</w> - 340\t\tces</w> - 179\t\tfait</w> - 127\n",
      "un</w> - 843\t\tpar</w> - 336\t\ty</w> - 175\t\tquand</w> - 113\n",
      "en</w> - 820\t\ttarrou</w> - 307\t\ttous</w> - 168\t\tn’était</w> - 112\n",
      "pas</w> - 755\t\tvous</w> - 302\t\tgrand</w> - 168\t\tdont</w> - 111\n",
      "se</w> - 682\t\tc’est</w> - 298\t\tnous</w> - 167\t\têtre</w> - 106\n",
      "une</w> - 647\t\tsa</w> - 287\t\td’une</w> - 164\t\tseulement</w> - 105\n",
      "ce</w> - 637\t\tpeste</w> - 287\t\ta</w> - 164\t\tmoins</w> - 104\n",
      "mais</w> - 630\t\tcette</w> - 270\t\td’un</w> - 162\t\tdire</w> - 102\n",
      "ne</w> - 602\t\tdocteur</w> - 261\t\tétaient</w> - 160\t\toui</w> - 102\n",
      "du</w> - 557\t\tleur</w> - 253\t\ttemps</w> - 160\t\tfois</w> - 100\n",
      "avait</w> - 538\t\tbien</w> - 251\t\tqu’on</w> - 156\t\tpendant</w> - 94\n",
      "rieux</w> - 527\t\tville</w> - 238\t\tc’était</w> - 155\t\tleurs</w> - 94\n",
      "pour</w> - 524\t\tou</w> - 237\t\tfaire</w> - 148\t\tquelques</w> - 93\n",
      "au</w> - 523\t\tmême</w> - 236\t\tcottard</w> - 148\t\tcas</w> - 91\n",
      "qu’il</w> - 497\t\tsans</w> - 235\t\talors</w> - 146\t\tn’est</w> - 90\n"
     ]
    }
   ],
   "source": [
    "sorted_vocab_new = [(w, vocab_new[w]) for w in sorted(vocab_new, key=vocab_new.get, reverse=True)]\n",
    "for i in range(25):\n",
    "    print(sorted_vocab_new[i][0], \"-\", sorted_vocab_new[i][1], end=\"\\t\\t\")\n",
    "    print(sorted_vocab_new[i+25][0], \"-\", sorted_vocab_new[i+25][1], end=\"\\t\\t\")\n",
    "    print(sorted_vocab_new[i+50][0], \"-\", sorted_vocab_new[i+50][1], end=\"\\t\\t\")\n",
    "    print(sorted_vocab_new[i+75][0], \"-\", sorted_vocab_new[i+75][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, whould we have tried to remove named entitites before doing this? We can see that the names\n",
    "Rambert, Tarrou, and Rieux among others are all present in this list. Or maybe we should not, as these are all\n",
    "words that exist on their own mostly outside of other logic, so why not have them exist as 1 word? On the other\n",
    "hand, I never had thought about how many merges we chose to do. Of course all these common words end up\n",
    "as single units. After this many merges, we're essentially doing word level analysis.\n",
    "\n",
    "Let's see some of the early merges that we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e', '</w>'),\n",
       " ('s', '</w>'),\n",
       " ('t', '</w>'),\n",
       " ('a', 'i'),\n",
       " ('e', 'n'),\n",
       " ('e', 's</w>'),\n",
       " ('o', 'u'),\n",
       " ('o', 'n'),\n",
       " ('r', '</w>'),\n",
       " ('q', 'u'),\n",
       " ('a', '</w>'),\n",
       " ('ai', 't</w>'),\n",
       " ('a', 'n'),\n",
       " ('d', 'e</w>'),\n",
       " ('e', 'u')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_pairs[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gitpage] *",
   "language": "python",
   "name": "conda-env-gitpage-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
