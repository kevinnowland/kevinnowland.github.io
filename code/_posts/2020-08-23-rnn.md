---
layout: code-post
title: Recurrent neural networks
tags: [neural nets]
---

By now a classicial and outdated technique, I am going to try to implement a
recurrent neural network and see how well it fits some dummy data.
A good resource for an overview of RNNs is [this cheat sheet](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)
from a Stanford CS course.

Outline:
1. Architecture
2. Backpropagation through time (BTT)
3. Dummy training examples
4. LSTMs and other cell types

```python
import numpy as np
from copy import deepcopy
import matplotlib.pyplot as plt
import logging
```

```python
logger = logging.getLogger()
```

## 1. Architecture

RNNs are based on _cells_ and act on sequences of inputs. A single (simple) cell will take
two inputs: one element of the input sequence and either the output or the
_hidden state_ of the previous cell in the sequence. One can also take the two
inputs to be the output of the previous cell along with the hidden state of the 
previous cell if there is no longer an input sequence to rely upon. The many
variations can be found in the cheat sheet referenced above. A cell can also
take more than two inputs, as in the case of the more complicated 
_Long Short-Term Memory_ (LSTM) cells where
there is an additional _context_ that exists in addition to the hidden state
that is passed between cells.

Let $x\_t$ be the $t$th element of the input sequence and $h\_{t-1}$ the
hidden state from the $(t-1)$st cell. Then the new hidden state is given
by
$$
    h\_t = \sigma(W\_xx\_t + W\_xh\_{t-1} + b\_h)
$$
where $\sigma$ is the activation function (we will take this to be ReLU).
If the input sequence element $x\_t$ has dimension $n\_x$ and the hidden
state has size $n\_h$, then $W\_x$ has size $n\_h\times n\_x$
while $W\_h$ is a square matrix of size $n\_h\times n\_h$. This is equivalent
to
$$
    h\_t = \sigma([W\_x | W\_h](x\_t\oplus h\_{t-1} + b\_h)
$$
where $\oplus$ indicates vector concatenation and $[W\_{xh} | W\_{hh}]$ is a matrix
of size $n\_h \times(n\_x + n\_h)$. Many of the descriptions of RNNs, and in particular
when cells become more complicated as with LSTM cells, formulate
things in terms of concatenation, so I just wanted to quickly show that these were
equivalent. Without working it out, my
intuition -- which could be wrong! -- suggests that this means we can keep
formulating the initialization of the weights in terms of the input and output vector sizes.

The output is then given by
$$
    y\_t = \sigma(W\_yh\_t + b\_y).
$$

All of these words are why you will usually just see diagrams when discussing
varieties of RNNs.

The super simplified version of an RNN would actually just take the hidden 
state and the output to be the same, with no extra layer between the hidden
state and the output.

In the following code we implement a `Cell` class that initializes based on
input size, hidden state size, and output size. It has methods to take an
input and update the hidden state and update both the hidden state and its
output, which it stores. The updates can also return the inputs to the hidden
state neuron layer and the output neuron layers that are fed into the
ReLU and sigmoid activations, respectively.

```python
def relu(x):
    return x * (x>0) + 0 * (x<=0)

def relu_prime(x):
    return 1 * (x>0) + 0 * (x<=0)

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_prime(x):
    return sigmoid(x) * (1 - sigmoid(x))

class Cell():
    
    def __init__(self, input_size, hidden_size, output_size, random_seed=None):                 
        """ give the size of the input vectors, hidden state
        vectors, and output sizes. 
        
        If using the cells in such a way that the output is
        used in place of the hidden state, then hidden size and
        output size must be the same.
        
        If using in such a way that the output is used in place
        of the input at some point, then the output and input
        sizes must be the same.
        """
        np.random.seed(random_seed)
        
        self._input_size = input_size
        self._hidden_size = hidden_size
        self._output_size = output_size
        
        self._hidden_state = None
        self._y = None
        
        def initialize(nrows, ncols, init_type=None):
            # Kaiming He initialiation for weights
            # or just some arbitrary initialization for biases
            if ncols > 1:
                if init_type=='He':
                    std = np.sqrt(2.0 / ncols)
                elif init_type=='Xavier':
                    std = np.sqrt(2.0 / (ncols + nrows))
                else:
                    msg = 'init_type must be He or Xavier if initializing weights'
                    raise Exception(msg)
                    
                try_once = False
            else:
                # this is for biases
                std = np.sqrt(2.0 / nrows)
                try_once = nrows == 1
                
            if try_once:
                w = np.random.normal(0, std, size=(nrows, ncols))
            else:
                try_again = True
                while try_again:
                    w = np.random.normal(0, std, size=(nrows, ncols))
                    try_again = abs(np.std(w)-std) > .05 or abs(np.mean(w)) > .05
                
            return w
        
        wxh = initialize(hidden_size, input_size + hidden_size, 'He')
        
        self._wx = deepcopy(wxh[:, :input_size])
        self._wh = deepcopy(wxh[:, input_size:])
        self._bh = initialize(hidden_size, 1)
        
        self._wy = initialize(output_size, hidden_size, 'Xavier')
        self._by = initialize(output_size, 1)
        
    @property
    def input_size(self):
        return self._input_size
    
    @property
    def hidden_size(self):
        return self._hidden_size
    
    @property
    def output_size(self):
        return self._output_size
    
    @property
    def hidden_state(self):
        return self._hidden_state
    
    def _shape_assert(self, oldval, newval):
        assert oldval.shape == newval.shape, \
            "new value must have shape {}".format(oldval.shape)
    
    @hidden_state.setter
    def hidden_state(self, val):
        self._shape_assert(self.hidden_state, val)
        self._hidden_state = val
        
    def clear_hidden_state(self, num_samples):
        self._hidden_state = np.zeros((self.hidden_size, num_samples))
        
    @property
    def y(self):
        return self._y
    
    def clear_y(self, num_samples):
        self._y = np.zeros((self.output_size, num_samples))
    
    @y.setter
    def y(self, val):
        self._shape_assert(self.y, val)
        self._y = val
        
    @property
    def wx(self):
        return self._wx
    
    @wx.setter
    def wx(self, val):
        self._shape_assert(self.wx, val)
        self._wx = val
        
    @property
    def wh(self):
        return self._wh
    
    @wh.setter
    def wh(self, val):
        self._shape_assert(self.wh, val)
        self._wh = val
        
    @property
    def bh(self):
        return self._bh
    
    @bh.setter
    def bh(self, val):
        self._shape_assert(self.bh, val)
        self._bh = val
        
    @property
    def wy(self):
        return self._wy
    
    @wy.setter
    def wy(self, val):
        self._shape_assert(self.wy, val)
        self._wy = val
        
    @property
    def by(self):
        return self._by
    
    @by.setter
    def by(self, val):
        self._shape_assert(self.by, val)
        self._by = val
        
    def update_hidden_state(self, x, return_activation_input=False, clear_hidden_state=False):
        """ take a numpy vector of size (input_size, num_samples) and
        calculate the hidden state of size (hidden_size, num_samples).
        
        This returns nothing, updating in place. """
        
        assert type(x) == np.ndarray, "x must by numpy.ndarray"
        assert x.shape[0] == self.input_size, \
            "x must have first dim of length {}".format(self.input_size)
        
        # initialize hidden state
        if clear_hidden_state or self.hidden_state is None:
            self.clear_hidden_state(num_samples=x.shape[1])
        
        # get output
        activation_input = np.matmul(self.wx, x) \
                            + np.matmul(self.wh, self.hidden_state) \
                            + self.bh
        
        self.hidden_state = relu(activation_input)
        
        if return_activation_input:
            return activation_input
        else:
            return None
        
    def update_y(self, return_activation_input=False):
        """ takes the hidden state and runs through a neuron
        layer with sigmoid activation function to get the output y """
        self.clear_y(num_samples=self.hidden_state.shape[1])
        activation_input = np.matmul(self.wy, self.hidden_state) + self.by
        self.y = sigmoid(activation_input)
        
        if return_activation_input:
            return activation_input
        else:
            return None
        
    def update(self, x, return_activation_inputs=False, clear_hidden_state=False):
        hidden_layer_input = self.update_hidden_state(x, return_activation_inputs, clear_hidden_state)
        output_layer_input = self.update_y(return_activation_inputs)
        
        if return_activation_inputs:
            return hidden_layer_input, output_layer_input
        else:
            return None
        
    def update_get_output(self, x, clear_hidden_state=False):
        self.update(x, return_activation_inputs=False, clear_hidden_state=clear_hidden_state)
        return deepcopy(self.y)
```

We now have an updatable cell that can be used to transform a sequence into
another sequence! We'll take a sequence that is defined by four other
fuzzy sequences of valeus coming from sine and cosine functions.
We'll just see what happens, that's all. This is just random playing around.

```python
def func_sequence(func=np.sin, period_length=2*np.pi, periods=2, resolution=100, std=0.25, random_seed=None):
    """ returns a fuzzy periodic sequence using func """
    np.random.seed(random_seed)
    
    step = period_length / resolution
    x = np.arange(0, period_length * periods, step)
    y_exact = func(x)
    y_fuzzy = y_exact + np.random.normal(0, std, len(y_exact))
    
    return x, y_fuzzy
```

```python
_, seq_1 = func_sequence()
_, seq_2 = func_sequence(np.cos)
_, seq_3 = func_sequence(lambda x: np.sin(x) * np.sin(x))
_, seq_4 = func_sequence(lambda x: np.sin(x) * np.cos(x))

input_seq = np.vstack([seq_1, seq_2, seq_3, seq_4]).reshape(-1, 4)
```

```python
cell = Cell(4, 3, 1, random_seed=1)
```

```python
output_seq = np.array([cell.update_get_output(input_seq[i].reshape(4, -1))[0, 0] for i in range(len(input_seq))])
```

```python
fig, ax = plt.subplots(figsize=(10, 7))

x = range(len(input_seq))
ax.plot(seq_1, c='black', alpha=0.2)
ax.plot(seq_2, c='black', alpha=0.2)
ax.plot(seq_3, c='black', alpha=0.2)
ax.plot(seq_4, c='black', alpha=0.2, label='input')
ax.plot(output_seq, c='C1', label='predicted')
ax.legend()

plt.show()
```


![png](/assets/images/RNN_files/RNN_10_0.png)


All we've done is run a sequence of length 200 through the RNN. If we unrolled the RNN, it would
consist of a single layer of vanilla RNN cells of length 200, each of which gives an output
and feeds its hidden state into the next state. There's no learning here, I just wanted to 
demonstrate that the RNN code I've written works. Let's see what happens if we give a
constant input sequence for a few randomly initialized RNNs.

```python
input_seq = np.ones((400, 4))

cell_1 = Cell(4, 3, 1, random_seed=5)
output_seq_1 = np.array([cell_1.update_get_output(input_seq[i].reshape(4, -1))[0, 0] for i in range(len(input_seq))])

cell_2 = Cell(4, 3, 1, random_seed=9)
output_seq_2 = np.array([cell_2.update_get_output(input_seq[i].reshape(4, -1))[0, 0] for i in range(len(input_seq))])

cell_3 = Cell(4, 3, 1, random_seed=10)
output_seq_3 = np.array([cell_3.update_get_output(input_seq[i].reshape(4, -1))[0, 0] for i in range(len(input_seq))])

fig, ax = plt.subplots(figsize=(10, 7))

x = range(len(input_seq))
ax.plot(output_seq_1)
ax.plot(output_seq_2)
ax.plot(output_seq_3)

plt.show()
```


![png](/assets/images/RNN_files/RNN_12_0.png)


Playing around with the random state gave me the three patterns observed above:

1. Brief osciallations before a constant
2. Gradually heading toward a constant
3. Oscillations that dampen to a constant amplitude

With constant input, we essentially are just acting on the hidden state
which starts out as all zeros. At every step the hidden state is acted
upon by an affine linear transformation (with the constant input forming a
part of the bias terms) and then the negative values are zeroed out by the
ReLU functions. This then gets passed as the next hidden state. The output
is an affine linear transformation of the hidden state that never changes.
So all we have to do is examine the behavior of repeatedly applying
a random affine linear transformation followed by zeroing out the negative values
in the result. Probably something about eigvenvalues of these randomly initialized
matrices (which will usually have small coefficients) that I don't wish to spend
my time deriving right now.

Thinking about unfolding the RNN in time, we have a single layer of cells.
It also possible to have multiple cells at a single timestep. At timestep
$t$ the first cell takes the intput $x\_t$ and the hidden state $h\_{t-1, 1}$
from the previous timestep of the same cells. The second cell would take
the output $y\_{t, 1}$ and its previous hidden state $h\_{t-1, 2}$ as its
input and put out the output $y\_t$. Unrolling this structure in time would
lead to multiple layers of cells. It seems that only one or two layers of cells
are typical, as training is very expensive for RNNs.

## 2. Backpropagation through time (BTT)

### Discussion of theory

Backpropagation through time is backpropagation on an unrolled recurrent
neural network. Since an RNN can be used on an arbitrary number of 
steps (depending on the application), one would limit the BTT algorithm
to the last couple time steps.

For an output sequence with $k$ steps, the overall loss is usually taken to be the
sum of the losses at each of the $k$ steps. Thus we can take the derivatives of
these step-wise losses and add them up to the get the derviatives of the
overall loss. This is another reason to limit BTT to the most recent couple
time steps and hope for the best, since this requires backpropagating through
many layers many times, which can be very costly.

A technique that is often needed in training RNNs is [gradient clipping](http://proceedings.mlr.press/v28/pascanu13.pdf)
which replaces the rescales claculated gradients to have a specific norm
-- the authors of the linked paper use 1 as the 
threshold but say that one should look at the statistics of the gradient
to choose an appropriate threshold. Vanishing gradients are instead tackled
by changing the cell architecture from the vanilla RNN above to an
LSTM or Gated Reccurence Unit (GRU).

When calculating the gradient, note that there is a big difference
between calculating the gradient with respect to $W\_y$ and with respect
to $W\_x$ or $W\_h$. At timestep $k$, the matrix $W\_y$ only interacts one time
whereas $W\_x$ and $W\_h$ are involved in $k$ calculations, at least in the variant where
the hidden state and not the output is passed between RNN cells. Since I am
too lazy to upload a graphic myself, go look at [this blog post](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)
or the many others that try to explain BTT in more detail than I am.
The other thing to notice is that you can pretend that in the unrolled RNN
each of the $W\_x$ and $W\_h$ matrices at difference timesteps are independent
and calculate their
gradients separately. Then, to get the gradients for the real $W\_x$
and $W\_h$ you add up the corresponding gradients. This is because we are
using the product rule and
$$
    \frac{d}{dx}xf(x) = f(x) + xf'(x)
$$
while
$$
    \left[\frac{\partial}{\partial u} + \frac{\partial}{\partial v}\right]uf(v) = f(v) + uf'(v).
$$

### Implementation

We're now going to choose a particular implementation of an RNN so that
we know how to implement BTT. We'll create a dummy example that mimics
a sentiment analysis classifier that takes in a sequence of fixed length
and predicts a single output, either 0 or 1. The API for this is not
consistent with what I previously wrote, as now would should explicitly
use a forward pass to generate predictions, this is because the hidden
states have to update when running through samples.

```python
def add_weights(k1, w1, k2, w2):
    """ k1*w1 + k2*w2 for neural net weights w1 and w2 """
    return [k1*w1[i] + k2*w2[i] for i in range(len(w1))]

def power_weights(w, k):
    """ return the weights to the kth power (elementwise) """
    return [np.power(ww, k) for ww in w]

class RNN():
    """ implement a simple RNN that takes a length N sequence of
    inputs and returns a single output, although inputs and ouputs
    can be various dimensions. 
    
    Architecture:
    
                   ^
                   |
    ( ) -> ( ) -> ( )
     ^      ^      ^
     |      |      |
     
     
    We are only enabling training one sample at a time.
    """
    
    def __init__(self,
                 input_seq_length=10,
                 input_size=5,
                 hidden_size=3,
                 output_size=1,
                 random_seed=None):
        
        self._cell = Cell(input_size, hidden_size, output_size, random_seed)
        self._input_seq_length = input_seq_length
        self._hidden_states = None # change depending on input
        self._hidden_activations = None # change depending on input
        self._output_activation = np.zeros((output_size, 1))
        
    @property
    def cell(self):
        return self._cell
    
    @property
    def input_seq_length(self):
        return self._input_seq_length
    
    @property
    def hidden_size(self):
        return self.cell.hidden_size
    
    @property
    def input_size(self):
        return self.cell.input_size
    
    @property
    def output_size(self):
        return self.cell.output_size
    
    @property
    def output(self):
        return self.cell.y
    
    @property
    def predictions(self):
        return 1.0 * (self.output >= 0.5) + 0.0 * (self.output < 0.5)
    
    def forward_pass(self, x):
        """ computes the output and saves needed information into
        the RNN's state to compute the gradient with respect to the
        weights during the backward pass. """
        
        assert len(x) == self.input_seq_length, "input sequence wrong length"
        
        num_samples = x.shape[2]
        
        self.cell.clear_hidden_state(num_samples)
        
        self._hidden_states = \
            np.zeros((self.input_seq_length+1, self.hidden_size, num_samples))
        self._hidden_activations = \
            np.zeros((self.input_seq_length, self.hidden_size, num_samples))
        
        for i in range(self.input_seq_length):

            self._hidden_activations[i] = self.cell.update_hidden_state(x[i],
                                                return_activation_input=True)
            self._hidden_states[i+1] = deepcopy(self.cell.hidden_state)
            
            if i == self.input_seq_length - 1:
                self._output_activation = \
                    self.cell.update_y(return_activation_input=True)
    
    def backward_pass(self, x, truth):
        """ compute the gradient using the sequence and its truth"""
        num_samples = truth.shape[1]
        
        # calculate gradient for W_y first
        
        delta = self.cell.y - truth
        sigma_prime_delta = sigmoid_prime(self._output_activation) * delta
        grad_wy = np.matmul(sigma_prime_delta,
                            self._hidden_states[-1].transpose()) \
                    / num_samples
        
        # new delta 
        delta = np.matmul(self.cell.wy.transpose(), sigma_prime_delta)
        
        grad_wx = np.zeros(self.cell.wx.shape)
        grad_wh = np.zeros(self.cell.wh.shape)
        
        # Now caulcuate gradient for W_x and W_h one step at a time
        for i in range(1, self.input_seq_length+1):
            
            sigma_prime_delta = relu_prime(self._hidden_activations[-i]) * delta
            grad_wh += np.matmul(sigma_prime_delta,
                                 self._hidden_states[-(1+i)].transpose()) \
                            / num_samples
            grad_wx += np.matmul(sigma_prime_delta, x[-i].transpose()) \
                            / num_samples
            
            # going down a level only involves the hidden state, hence W_h is used.
            delta = np.matmul(self.cell.wh.transpose(), sigma_prime_delta)
            
        return grad_wx, grad_wh, grad_wy
    
    def fit(self, x, y, eta=0.1, max_error=0.1, 
            max_epochs=5, batch_size=100, max_iter=None,
            save_data=False, random_seed=None,
            alpha=.001, beta_1=0.9, beta_2=0.999, eps=1e-8):
        """ use ADAM with backpropagation
        to fit the network to the given training data x which
        should be of size (input_seq_length, input_size, n_samples),
        y should be of size (1, n_samples)"""
        
        np.random.seed(random_seed)
        
        num_samples = y.shape[1]
        
        # some samples might be left behind
        batches_per_epoch = np.floor(num_samples / batch_size)
        saved_data = []
        
        w = [
            deepcopy(self.cell.wx)
            ,deepcopy(self.cell.wh)
            ,deepcopy(self.cell.wy)
        ]
        
        t = 0
        m = [np.zeros(shape=ww.shape) for ww in w]
        v = [np.zeros(shape=ww.shape) for ww in w]
        
        def calculate_train_error():
            self.forward_pass(x)
            current_predictions = self.predictions
            wrong = np.sum(np.abs(y - current_predictions))
            return 1.0 * wrong / num_samples
                    
        curr_iter = 1
        curr_epoch = 1
        curr_batch_number = 0
        batch_indexes = np.arange(num_samples)
        train_error = calculate_train_error()
        
        def get_save_data():
            avg_loss = np.mean((self.output - y)**2)

            return {
                'epoch': curr_epoch
                ,'avg_loss': avg_loss
                ,'train_error': train_error
                ,'wx': deepcopy(cell.wx)
                ,'wh': deepcopy(cell.wh)
                ,'wy': deepcopy(cell.wy)
            }
            
        if save_data:
            saved_data = [get_save_data()]  
            
        keep_training = True   
        while keep_training:
            
            if curr_batch_number == 0:
                # re-shuffle indexes as neded
                logger.debug("NeuralNet.fit(): starting epoch {}".format(curr_epoch))
                np.random.shuffle(batch_indexes)
                
            batch_ind = batch_indexes[curr_batch_number * batch_size:(curr_batch_number + 1) * batch_size]
            
            x_batch = x[:, :, batch_ind]
            y_batch = y[:, batch_ind]
            
            # forward pass
            self.forward_pass(x_batch)
            
            # backward pass
            grad = list(self.backward_pass(x_batch, y_batch))
            
            t = t + 1

            grad_squared = power_weights(grad, 2)

            m = add_weights(beta_1, m, 1-beta_1, grad)
            v = add_weights(beta_2, v, 1-beta_2, grad_squared)

            m_hat = [mm / (1 - np.power(beta_1, t)) for mm in m]
            v_hat = [vv / (1 - np.power(beta_2, t)) for vv in v]

            w = [w[i] - alpha * m_hat[i] / (np.power(v_hat[i], 0.5) + eps) for i in range(len(w))]
            self.cell.wx = deepcopy(w[0])
            self.cell.wh = deepcopy(w[1])
            self.cell.wy = deepcopy(w[2])
            
            train_error = calculate_train_error()
            curr_iter += 1
            curr_batch_number = int((curr_batch_number + 1) % batches_per_epoch)
            
            if curr_batch_number == 0:
                curr_epoch += 1
                
                if save_data:
                    saved_data += [get_save_data()]
                    
            keep_training = train_error >= max_error and curr_epoch <= max_epochs
            if max_iter is not None:
                keep_training = keep_training and curr_iter <= max_iter
            
            
        if curr_epoch > max_epochs:
            logger.warning("NeuralNet.fit():no convergence, train_error above max_error")
        else:
            pass
            #logger.warning("NeuralNet.fit(): converged during epoch {}.".format(curr_epoch-1))
        
        if save_data:
            return saved_data
        else:
            return None

            
```

## Dummy training example

With the RNN code written, we'll run create a test problem. We'll take an RNN that works on
sequences of length 3 with each input having dimension 2. We'll initialize a many of these
sequences using a standard normal distribution for each entry. We mark the sequence as
positive if among the second entry of the second sequence element and the two entries of 
the final sequence element contain at least as many nonnegative entries as the other
entries in the sequence.

```python
rnn = RNN(input_seq_length=3,
          input_size=2,
          hidden_size=3,
          output_size=1,
          random_seed=None)
```

```python
x = np.random.normal(size=(3, 2, 1000))
```

```python
def get_sequence_labels(x):
    """ take a 3x2 and decide return 1 or 0 depnding on if there
    are more negatives in the (0, 0), (0, 1), and (1, 0) positions
    or in the remaining elements """
    pos = 1 * (x>0) + 0 * (x<=0)
    first_three = pos[0, 0] + pos[0, 1] + pos[1, 0]
    last_three = pos[1, 1] + pos[2, 0] + pos[2, 1]
    if first_three >= last_three:
        return 1.0
    else:
        return 0.0

num_samples = x.shape[2]
y = np.array([get_sequence_labels(x[:, :, i]) for i in range(num_samples)]).reshape(1, -1)
```

```python
data = rnn.fit(x, y, max_epochs=1000, save_data=True)
```

    NeuralNet.fit():no convergence, train_error above max_error


```python
fig, ax = plt.subplots(1, 2, figsize=(16, 7))

ax[0].plot([d['train_error'] for d in data])
ax[0].set_title('Train Error')
ax[0].set_xlabel('epoch')

ax[1].plot([d['avg_loss'] for d in data])
ax[1].set_title('Average Loss')
ax[1].set_xlabel('epoch')

plt.show()
```


![png](/assets/images/RNN_files/RNN_20_0.png)


And hey look at that, it learns! The chance that a randomly generated sequence as
described above will be labeled with a 1 is about 2/3, so naively predicting a 1 label
every time will give a training accuracy of about 67% and the observed training accuracy
is around 85%.

Here's what a single prediction looks like:

```python
# change the state by doing a forward pass through the RNN
rnn.forward_pass(np.array([[-1, 1], [1, 1], [-1, -1]]).reshape(3, 2, 1))

# see what the raw output is
rnn.output
```




    array([[0.96274349]])



## LSTMs and Other Architectures

One of the main issues with RNNs is that they do not hold onto their context for very long.
In particular, the hidden state changes at every step and often in a dramatic way. This makes
it hard to use an RNN on long sequences. The main advance that seems to still be in use
today is the Long Short-Term Memory cell ([Hochreiter and Schmidhuber](https://www.bioinf.jku.at/publications/older/2604.pdf), 1997). 
An actual explanatory blog post from Open AI's Christopher Olah 
unlike the code-focused more for my own benefit version can be found [here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). (Images
were taken during google, but I believe they originate from this post as well.) The main 
idea is that not only is their the hidden state / output being passed between cells, but
also a context that flows between cells and is only influenced slightly by the
hidden state. The main motiviating idea in the 1997 paper was to maintain constant
error through the cell and avoid gradient vanishing and blowup that tends to happen
in simple RNNs.

![png](/assets/images/RNN_files/lstm.png)

In an LSTM cell the context line flows through the top while the hidden state is in the
bottom. There are three steps. The first is to use a _forget gate_ that looks
at the combined hidden state and input and decides which parts of the context should
be maintined or forgotten. The output is controlled by a sigmoid function which goes from
0 to 1 and is multiplied by the current context. The next step is the _input gate_ which
takes the hidden state and input and decides what we want to add into the
current context. A tanh layer decides what values we want to add to the context
but are first multiplied by the output from a sigmoid layer to determine which
changes are allowable. Finally, there is the _output gate_ which takes the new 
context as determined by the previous two gates along with the input and
hidden state to determine the next hidden state / intermediate cell output.

There are many variations, as outlined in the linked blog post. The most common
recent variation is the Gated Recurrence Unit (GRU) ([Cho et al](https://arxiv.org/pdf/1406.1078v3.pdf), 2014)
which simplifies the LSTM architecture by merging the context and the hidden
state. The authors propose the GRU to be basis cell in an RNN encoder-decoder 
architecture which first uses an RNN to encode a single context and then 
the decoder takes that context to predict a first new word and then
uses the encoded context and all previously predicted words to predict a next
word. The GRU works with only two gates, a _reset gate_ and an _update gate_.

![png](/assets/images/RNN_files/gru.png)

To me, the motiviation for the GRU gating is not presented as clearly as for
the LSTM gating which makes an attempt at analyzing error propagation. A couple
of the authors of the original GRU paper and some others [compared GRUs to LSTMs](https://arxiv.org/pdf/1412.3555.pdf)
and found them to be similar, although GRU cells use less memory. Both outperformed
traditional RNNs.
