---
layout: code-post
title: NLP Basic Processing - spaCy and NLTK
description: This is just me playing around with basic NLP packages.
tags: [demo]
---

Though I'm interested in NLP, I've never actually played around with NLP models
or any of the packages and basic techniques that NLP relies on. So here I'm
just going to play a bit with spaCy and some a basic algorithm or two.

Outline:
- spaCy
- nltk
- BPE and wordpiece

## spaCy

For my introduction to spaCy, I'm pretty much just running through the 
[spaCy introduction](https://spacy.io/usage/spacy-101). Truly there is
not much reason to read this section, the actual spaCy documentation 
is much more interesting and likely to be correct! I will point out
that I quickly ran into one of the limitations of using their pretrained
models without fine-tuning them.

```python
import spacy
nlp = spacy.load("fr_core_news_md")
```

let's see what we can do with a corpus made from the first chapter of Camus' "La peste".
I physically copied the text from [here](https://www.ebooksgratuits.com/html/camus_la_peste.html#_Toc284769571),
though I could've with some difficulty used `beautifulsoup`.

```python
with open("data/la_peste_1.txt", "r") as f:
    text = f.read().replace("\n", " ")
    
doc = nlp(text)
```

```python
doc[:100]
```




    Les curieux événements qui font le sujet de cette chronique se sont produits en 194., à Oran. De l’avis général, ils n’y étaient pas à leur place, sortant un peu de l’ordinaire. À première vue, Oran est, en effet, une ville ordinaire et rien de plus qu’une préfecture française de la côte algérienne.    La cité elle-même, on doit l’avouer, est laide. D’aspect tranquille, il faut quelque temps pour apercevoir ce qui la rend différente de tant d’autres villes



spaCy does a lot of things including part of speech tagging and lemmatization:

```python
for i in range(10):
    print(doc[i].text, doc[i].pos_, doc[i].lemma_)
```

    Les DET le
    curieux ADJ curieux
    événements NOUN événement
    qui PRON qui
    font VERB faire
    le DET le
    sujet NOUN sujet
    de ADP de
    cette DET ce
    chronique NOUN chronique


We can get named entities by called the `.ents` property of a document. The list of entity labels can be found [here](https://spacy.io/api/annotation#named-entities).

```python
for entity in doc[:2000].ents:
    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))
```

    Oran - LOC - Non-GPE locations, mountain ranges, bodies of water
    Oran - LOC - Non-GPE locations, mountain ranges, bodies of water
    Oran - LOC - Non-GPE locations, mountain ranges, bodies of water
    Oran - LOC - Non-GPE locations, mountain ranges, bodies of water
    Oran - LOC - Non-GPE locations, mountain ranges, bodies of water
    Arrivé - LOC - Non-GPE locations, mountain ranges, bodies of water
    Bernard Rieux - PER - Named person or family.
    M. Michel - PER - Named person or family.
    M. Michel - PER - Named person or family.
    Bernard Rieux - PER - Named person or family.
    Rieux - LOC - Non-GPE locations, mountain ranges, bodies of water
    Dors si tu peux - MISC - Miscellaneous entities, e.g. events, nationalities, products or works of art


So that last example shows that the named entities are not perfect. "Dors si tu peux" is just a 
normal phrase. A named entity will span multiple tokens, it's not a special tag of a single token:

```python
doc[1524:1526]
```




    Le matin



Also, the results are very different if you lowercase the results first:

```python
with open("data/la_peste_1.txt", "r") as f:
    text_lower = f.read().replace("\n", " ").lower()
    
doc_lower = nlp(text_lower)
```

```python
for entity in doc_lower[:2000].ents:
    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))
```

    oran - LOC - Non-GPE locations, mountain ranges, bodies of water
    oran - LOC - Non-GPE locations, mountain ranges, bodies of water
    laide - PER - Named person or family.
    le soleil incendie - MISC - Miscellaneous entities, e.g. events, nationalities, products or works of art
    les beaux jours - MISC - Miscellaneous entities, e.g. events, nationalities, products or works of art
    boulevard - PER - Named person or family.
    boulomanes - LOC - Non-GPE locations, mountain ranges, bodies of water
    amicales - LOC - Non-GPE locations, mountain ranges, bodies of water
    oran - PER - Named person or family.
    oran - LOC - Non-GPE locations, mountain ranges, bodies of water
    oran - LOC - Non-GPE locations, mountain ranges, bodies of water
    bernard - PER - Named person or family.
    m. michel - PER - Named person or family.
    m. michel - PER - Named person or family.
    bernard rieux - PER - Named person or family.
    dors si tu peux - MISC - Miscellaneous entities, e.g. events, nationalities, products or works of art
    au train de midi - MISC - Miscellaneous entities, e.g. events, nationalities, products or works of art


If one is going to rely on named entities, then it appears that lowercasing is a hindrance, which makes
sense since in French, as in English, uppercasing is used to denote proper names. Let's go back to the
upperacased version but use the full model and see if its predictions about named entities do better.

```python
nlp_large = spacy.load("fr_core_news_lg")
```

```python
doc_large = nlp_large(text)
```

```python
for entity in doc_large[:2000].ents:
    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))
```

    Oran - LOC - Non-GPE locations, mountain ranges, bodies of water
    Oran - LOC - Non-GPE locations, mountain ranges, bodies of water
    Oran - LOC - Non-GPE locations, mountain ranges, bodies of water
    Oran - LOC - Non-GPE locations, mountain ranges, bodies of water
    Oran - LOC - Non-GPE locations, mountain ranges, bodies of water
    Bernard Rieux - PER - Named person or family.
    M. Michel - PER - Named person or family.
    M. Michel - PER - Named person or family.
    Bernard Rieux - PER - Named person or family.
    Rieux - PER - Named person or family.
    Dors si tu peux - MISC - Miscellaneous entities, e.g. events, nationalities, products or works of art


Still has "Dors si tu peux" (sleep if you can) but it did get rid of Arrivé which is a past participle.
If we look up the line containing the incorrectly labelled phrase, we find the following:

```python
doc_large[1956:1964]
```




    Dors si tu peux, dit-il.



Which shows that this is clearly a phrase ("Sleep if you can, he said"). Apparently it is possible to train from scratch or fine-tune the models that spaCy uses to predict all these labels and is described in the [documentation](https://spacy.io/usage/training).

Let's call the visualizer, since that's something that spaCy has built in apparently.

```python
from spacy import displacy

displacy.render(nlp("Dors si tu peux, dit-il."), style="dep")
```


<span class="tex2jax_ignore"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:lang="fr" id="3efc9ac27e144db0a91e7b11895815cf-0" class="displacy" width="1100" height="487.0" direction="ltr" style="max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr">
<text class="displacy-token" fill="currentColor" text-anchor="middle" y="397.0">
    <tspan class="displacy-word" fill="currentColor" x="50">Dors</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="50">NOUN</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="397.0">
    <tspan class="displacy-word" fill="currentColor" x="225">si</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="225">SCONJ</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="397.0">
    <tspan class="displacy-word" fill="currentColor" x="400">tu</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="400">PRON</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="397.0">
    <tspan class="displacy-word" fill="currentColor" x="575">peux,</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="575">VERB</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="397.0">
    <tspan class="displacy-word" fill="currentColor" x="750">dit</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="750">VERB</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="397.0">
    <tspan class="displacy-word" fill="currentColor" x="925">-il.</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="925">PRON</tspan>
</text>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-3efc9ac27e144db0a91e7b11895815cf-0-0" stroke-width="2px" d="M70,352.0 C70,2.0 750.0,2.0 750.0,352.0" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-3efc9ac27e144db0a91e7b11895815cf-0-0" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">advmod</textPath>
    </text>
    <path class="displacy-arrowhead" d="M70,354.0 L62,342.0 78,342.0" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-3efc9ac27e144db0a91e7b11895815cf-0-1" stroke-width="2px" d="M245,352.0 C245,177.0 565.0,177.0 565.0,352.0" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-3efc9ac27e144db0a91e7b11895815cf-0-1" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">mark</textPath>
    </text>
    <path class="displacy-arrowhead" d="M245,354.0 L237,342.0 253,342.0" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-3efc9ac27e144db0a91e7b11895815cf-0-2" stroke-width="2px" d="M420,352.0 C420,264.5 560.0,264.5 560.0,352.0" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-3efc9ac27e144db0a91e7b11895815cf-0-2" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">nsubj</textPath>
    </text>
    <path class="displacy-arrowhead" d="M420,354.0 L412,342.0 428,342.0" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-3efc9ac27e144db0a91e7b11895815cf-0-3" stroke-width="2px" d="M70,352.0 C70,89.5 570.0,89.5 570.0,352.0" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-3efc9ac27e144db0a91e7b11895815cf-0-3" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">dep</textPath>
    </text>
    <path class="displacy-arrowhead" d="M570.0,354.0 L578.0,342.0 562.0,342.0" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-3efc9ac27e144db0a91e7b11895815cf-0-4" stroke-width="2px" d="M770,352.0 C770,264.5 910.0,264.5 910.0,352.0" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-3efc9ac27e144db0a91e7b11895815cf-0-4" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">nsubj</textPath>
    </text>
    <path class="displacy-arrowhead" d="M910.0,354.0 L918.0,342.0 902.0,342.0" fill="currentColor"/>
</g>
</svg></span>


The issue seems to be that it is not recognizing "dors" as the imperative of "dormir." If
we change to the "vous" form from the "tu" form we get the following:

```python
displacy.render(nlp("Dormez si tu peux, dit-il"), style="dep")
```


<span class="tex2jax_ignore"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:lang="fr" id="c2eab3277e7d4634aac9921592b9f1c6-0" class="displacy" width="1100" height="487.0" direction="ltr" style="max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr">
<text class="displacy-token" fill="currentColor" text-anchor="middle" y="397.0">
    <tspan class="displacy-word" fill="currentColor" x="50">Dormez</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="50">VERB</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="397.0">
    <tspan class="displacy-word" fill="currentColor" x="225">si</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="225">SCONJ</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="397.0">
    <tspan class="displacy-word" fill="currentColor" x="400">tu</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="400">PRON</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="397.0">
    <tspan class="displacy-word" fill="currentColor" x="575">peux,</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="575">VERB</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="397.0">
    <tspan class="displacy-word" fill="currentColor" x="750">dit</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="750">VERB</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="397.0">
    <tspan class="displacy-word" fill="currentColor" x="925">-il</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="925">PRON</tspan>
</text>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-c2eab3277e7d4634aac9921592b9f1c6-0-0" stroke-width="2px" d="M245,352.0 C245,177.0 565.0,177.0 565.0,352.0" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-c2eab3277e7d4634aac9921592b9f1c6-0-0" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">mark</textPath>
    </text>
    <path class="displacy-arrowhead" d="M245,354.0 L237,342.0 253,342.0" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-c2eab3277e7d4634aac9921592b9f1c6-0-1" stroke-width="2px" d="M420,352.0 C420,264.5 560.0,264.5 560.0,352.0" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-c2eab3277e7d4634aac9921592b9f1c6-0-1" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">nsubj</textPath>
    </text>
    <path class="displacy-arrowhead" d="M420,354.0 L412,342.0 428,342.0" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-c2eab3277e7d4634aac9921592b9f1c6-0-2" stroke-width="2px" d="M70,352.0 C70,89.5 570.0,89.5 570.0,352.0" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-c2eab3277e7d4634aac9921592b9f1c6-0-2" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">ccomp</textPath>
    </text>
    <path class="displacy-arrowhead" d="M570.0,354.0 L578.0,342.0 562.0,342.0" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-c2eab3277e7d4634aac9921592b9f1c6-0-3" stroke-width="2px" d="M70,352.0 C70,2.0 750.0,2.0 750.0,352.0" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-c2eab3277e7d4634aac9921592b9f1c6-0-3" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">advcl</textPath>
    </text>
    <path class="displacy-arrowhead" d="M750.0,354.0 L758.0,342.0 742.0,342.0" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-c2eab3277e7d4634aac9921592b9f1c6-0-4" stroke-width="2px" d="M770,352.0 C770,264.5 910.0,264.5 910.0,352.0" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-c2eab3277e7d4634aac9921592b9f1c6-0-4" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">nsubj</textPath>
    </text>
    <path class="displacy-arrowhead" d="M910.0,354.0 L918.0,342.0 902.0,342.0" fill="currentColor"/>
</g>
</svg></span>


Although if we go to the entity visualizer it still does not understand.

```python
displacy.render(nlp("Dors si tu peux, dit-il"), style="ent")
displacy.render(nlp("Dormez si vous pouvez, dit-il"), style="ent")
```


    ---------------------------------------------------------------------------

    NameError                                 Traceback (most recent call last)

    <ipython-input-264-2819de732d5a> in <module>
    ----> 1 displacy.render(nlp("Dors si tu peux, dit-il"), style="ent")
          2 displacy.render(nlp("Dormez si vous pouvez, dit-il"), style="ent")


    NameError: name 'displacy' is not defined


And these are the two things that displaCy can do. Does make for some pretty output!

The French models come with language vectors (probably produced by word2vec?) that we can see if we wanted by calling `token.vector` where `token` is a token from a  `doc` object. However, being simply a length 300 numpy array, it's not really interesting to look at\. The tokens can also get similarity scores between its vector and another word's vector.

```python
small_doc = nlp("roi reine renard")
print("Similarity between roi and reine:", small_doc[0].similarity(small_doc[1]))
print("Similarity between roi and renard:", small_doc[0].similarity(small_doc[2]))
```

    Similarity between roi and reine: 0.6281081
    Similarity between roi and renard: 0.2194891


## NLTK

From what I can surmise from googling, spaCy and NLTK take different approaches. An app developer is probably going to use spaCy as it provides out of the box ready algorithms for the problems at hand with not many choices. NLTK is more useful to researchers who are trying to build from the ground up, but it's trickier to adapt to well worn problems.

After installing nltk with conda, you can't just immediately call functions such as `sent_tokenize` as you will be
presented with a `LookupError` saying it can't find some resource that it is looking for.
This is because nltk is much more customizable and must be customized before running. Data installation instructions
can be found [here](https://www.nltk.org/data.html). I'll try to keep track of what I downloaded as I go. To
download via GUI, run `nltk.download()` which will open up an interface in a new window and you an download
everything or pieces. On my Mac it downloaded to `~/nltk_data`.

For sentence level tokenization, we rely on punkt, which I installed from an interactive python shell I opened in the
terminal via `nltk.download('punkt)`.

```python
import nltk
tokenizer = nltk.data.load('tokenizers/punkt/PY3/french.pickle')
```

```python
sentences = tokenizer.tokenize(text)
```

```python
for i in range(4):
    print(sentences[i])
```

    Les curieux événements qui font le sujet de cette chronique se sont produits en 194., à Oran.
    De l’avis général, ils n’y étaient pas à leur place, sortant un peu de l’ordinaire.
    À première vue, Oran est, en effet, une ville ordinaire et rien de plus qu’une préfecture française de la côte algérienne.
    La cité elle-même, on doit l’avouer, est laide.


```python
sentences[1]
```




    'De l’avis général, ils n’y étaient pas à leur place, sortant un peu de l’ordinaire.'



Apparently punkt is just a sentence level tokenizer. But there are many options! Here's one that
apparently was implemented / invented by someone from my alma mater of OSU. It seems to just be
based on regexes from the [documentation](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.toktok).
There was another regex based tokenizer called the Moses Tokenizer, but it was removed due to an
incompatible license. It still comes up when you search on stack overflow (via google!).

```python
from nltk.tokenize.toktok import ToktokTokenizer
toktok = ToktokTokenizer()
print(toktok.tokenize(sentences[0])[:8])
```

    ['Les', 'curieux', 'événements', 'qui', 'font', 'le', 'sujet', 'de']


Lemmatization seems much harder to find in nltk. There is the `WordNetLemmatizer` that exists in
`nltk.stem`, but it is English only and even then the results seem dubious (based on googling).
It is eassier to find Stemmers, as the snowball stemmer has a French version, but these
do not really work in the desired way. One can download an external lemmatizer
such as the [FrenchLefffLematizer](https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer) which
already exists in the spaCy universe via [spacy-leff](https://spacy.io/universe/project/spacy-lefff).
Here's an example of the French snowball stemmer.

```python
from nltk.stem import SnowballStemmer

stemmer = SnowballStemmer("french")
words = toktok.tokenize(sentences[0])[:8]
for word in words:
    print(word, "-", stemmer.stem(word))
```

    Les - le
    curieux - curieux
    événements - éven
    qui - qui
    font - font
    le - le
    sujet - sujet
    de - de


So stemming is obviously not great! This makes sense since it's based on characters and removing them and not really grammatical
except where the grammar is reflected very simply at the character level. Even a semi-inaccurate lemmatizer
would probably be better in a lot of cases, I would think, now that lemmatizers are probably decently
cheap to train.

It also seems that NLTK does not have a built in way to handle Named Entity Recognition for French, although
it does for English.

We didn't deal with stopwords in spaCy, though we could have. Let's do it in nltk, showing again how it is language specific as well.
This required to `nltk.download('stopwords')`. Then the stopwords appear to just be a list that onee can compare against
using, e.g., list comprehension.

```python
from nltk.corpus import stopwords

stopwords.words('french')[0:10]
```




    ['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle']



I think it's clear already that I'll probably be using spaCy if I want to work with French text.
Given the different goals of the projects, it's understandable why ntlk makes the choices it does,
it probably won't suit me as often. Or, perhaps more likley, everything will be custom, which
seems to be likely with language data.

## BPE and WordPiece

Above we've been working at the sentence and word level, but the very large transformers that are currently en vogue are 
based on something more akin to the character level. It's not quite accurate, as the GPT family of models rely
on Byte Pair Encodings (BPE) while BERT uses WordPiece, both of which we try to explain shortly.

### BPE

BPE is relatively simple to explain. Take a corpus and break it into distince words, recording
their frequencies. Split each word into characters while also appending a stop word symbol to
the last letter. At the beginning, each letter or letter+stop is considered as a byte.
Iterate over the text doing the following. Look for the most frequent pair of consecutive
bytes and combine them into a new byte. Do this until you are satisfied that the bytes
are appropriately descriptive. Letting the tokenizer continue forever results in each
word being its own byte, so BPE interpolates between letters and words. However, as stated
previously, orthography and grammar are only somewhat related, so BPE does have some drawbacks.
According to [HuggingFace](https://huggingface.co/transformers/tokenizer_summary.html), GPT-2 
stopped increasing the vocabulary after 40,000 merges. BPE was presented in 1995 by 
[P. Gage](https://dl.acm.org/doi/10.5555/177910.177914), although the GPT-2 paper references 
[Sennrich et al](https://arxiv.org/pdf/1508.07909.pdf) which presents algorithm explicitly
in python. Sennrich has also written the [subword-nmt package](https://github.com/rsennrich/subword-nmt)
which implemeents BPE.


### WordPiece

WordPiece is a generalization of BPE. The difference lies in how you choose tokens to merge.
At each step, you must have a model trained on the data. The chosen byte merge is the one
which most improves the model. For example, say the model is a trigram which predicts the
next byte from the preovious two bytes. Then you would merge the two bytes which most
improve the model upon merging (once its retrained, I believe). This could clearly be
much more computationally intensive depending on the model. Using WordPiece with a digram
recovers BPE, I think.

### BPE Example

Let's see what vocab we get using the first chapter of La Peste.

```python
import re
import collections
```

```python
with open("data/la_peste_1.txt", "r") as f:
    text = f.read().replace("\n", " ")
    
print("This corpus has {} words.".format(len(text.split(' '))))
```

    This corpus has 85190 words.


Let's clean the text by lowercasing and removing punctuation, keeping in mind of
course that French has letters with accents. I believe we should leave the
quote mark that comes with contractions as well as the dash in words
such as "elle-même." I think this means that we just want to remove
periods, commas, and any punctionation that occurs in a word without
other letters. We also have to end each word with the special end of word
character "\</w>".

```python
text_ = text.lower()

weird_chars = re.compile(r'[\?;\(\)\\.,\!:–»«]') # Removing En dash not Em dash
text_ = weird_chars.sub(" ", text_)

def prepare_word(word):
    """ take a word and add spaces between letters and add the
    end of word symbol </w>"""
    return "".join(l + " " for l in word) + "</w>"

spaces = re.compile("\s+")
words = [prepare_word(w) for w in spaces.split(text_)]

vocab = dict(collections.Counter(words))

print("This corpus has {} distinct words.".format(len(vocab)))
```

    This corpus has 10318 distinct words.


Now we steal the code from the Sennrich paper:

```python
def get_stats(vocab):
    pairs = collections.defaultdict(int)
    for word, freq in vocab.items():
        symbols = word.split()
        for i in range(len(symbols)-1):
            pairs[symbols[i],symbols[i+1]] += freq
    return pairs


def merge_vocab(pair, v_in):
    v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in v_in:
        w_out = p.sub(''.join(pair), word)
        v_out[w_out] = v_in[word]
    return v_out
```

And now let's do some number of merges. Maybe 500?

```python
import copy
vocab_new = copy.deepcopy(vocab)
```

```python
num_merges = 500
merge_pairs = []
for i in range(num_merges):
    pairs = get_stats(vocab_new)
    best = max(pairs, key=pairs.get)
    merge_pairs += [best]
    vocab_new = merge_vocab(best, vocab_new)
```

```python
sorted_vocab_new = [(w, vocab_new[w]) for w in sorted(vocab_new, key=vocab_new.get, reverse=True)]
for i in range(25):
    print(sorted_vocab_new[i][0], "-", sorted_vocab_new[i][1], end="\t\t")
    print(sorted_vocab_new[i+25][0], "-", sorted_vocab_new[i+25][1], end="\t\t")
    print(sorted_vocab_new[i+50][0], "-", sorted_vocab_new[i+50][1], end="\t\t")
    print(sorted_vocab_new[i+75][0], "-", sorted_vocab_new[i+75][1])
```

    de</w> - 3370		plus</w> - 464		est</w> - 231		avaient</w> - 146
    la</w> - 2450		lui</w> - 462		comme</w> - 230		toujours</w> - 145
    et</w> - 2212		était</w> - 442		ses</w> - 230		deux</w> - 145
    le</w> - 2098		sur</w> - 407		si</w> - 220		aux</w> - 142
    les</w> - 1735		on</w> - 398		rambert</w> - 206		rien</w> - 140
    à</w> - 1590		je</w> - 388		où</w> - 205		moment</w> - 140
    il</w> - 1314		son</w> - 385		elle</w> - 192		pouvait</w> - 138
    que</w> - 1182		tout</w> - 356		peu</w> - 188		après</w> - 132
    des</w> - 1027		avec</w> - 352		encore</w> - 185		vers</w> - 132
    dans</w> - 879		dit</w> - 343		cela</w> - 182		aussi</w> - 131
    qui</w> - 858		ils</w> - 340		ces</w> - 179		fait</w> - 127
    un</w> - 843		par</w> - 336		y</w> - 175		quand</w> - 113
    en</w> - 820		tarrou</w> - 307		tous</w> - 168		n’était</w> - 112
    pas</w> - 755		vous</w> - 302		grand</w> - 168		dont</w> - 111
    se</w> - 682		c’est</w> - 298		nous</w> - 167		être</w> - 106
    une</w> - 647		sa</w> - 287		d’une</w> - 164		seulement</w> - 105
    ce</w> - 637		peste</w> - 287		a</w> - 164		moins</w> - 104
    mais</w> - 630		cette</w> - 270		d’un</w> - 162		dire</w> - 102
    ne</w> - 602		docteur</w> - 261		étaient</w> - 160		oui</w> - 102
    du</w> - 557		leur</w> - 253		temps</w> - 160		fois</w> - 100
    avait</w> - 538		bien</w> - 251		qu’on</w> - 156		pendant</w> - 94
    rieux</w> - 527		ville</w> - 238		c’était</w> - 155		leurs</w> - 94
    pour</w> - 524		ou</w> - 237		faire</w> - 148		quelques</w> - 93
    au</w> - 523		même</w> - 236		cottard</w> - 148		cas</w> - 91
    qu’il</w> - 497		sans</w> - 235		alors</w> - 146		n’est</w> - 90


So, whould we have tried to remove named entitites before doing this? We can see that the names
Rambert, Tarrou, and Rieux among others are all present in this list. Or maybe we should not, as these are all
words that exist on their own mostly outside of other logic, so why not have them exist as 1 word? On the other
hand, I never had thought about how many merges we chose to do. Of course all these common words end up
as single units. After this many merges, we're essentially doing word level analysis.

Let's see some of the early merges that we do:

```python
merge_pairs[:15]
```




    [('e', '</w>'),
     ('s', '</w>'),
     ('t', '</w>'),
     ('a', 'i'),
     ('e', 'n'),
     ('e', 's</w>'),
     ('o', 'u'),
     ('o', 'n'),
     ('r', '</w>'),
     ('q', 'u'),
     ('a', '</w>'),
     ('ai', 't</w>'),
     ('a', 'n'),
     ('d', 'e</w>'),
     ('e', 'u')]



```python

```

```python

```

```python

```
